{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPenANBgAmQvNi5mVTUeC6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DelMashiry-dev/DelMashiry-dev/blob/main/MONDAY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Understanding"
      ],
      "metadata": {
        "id": "1Dh-uNyU-R9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ9L4jmu-PN6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive and load data\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv'\n",
        "df = pd.read_csv(data_path, parse_dates=['date'])\n",
        "\n",
        "# Initial exploration\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nData types:\\n\", df.dtypes.value_counts())\n",
        "print(\"\\nMissing values percentage:\\n\", df.isnull().mean().sort_values(ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Medical Resource Demand Prediction System - Complete Google Colab Implementation\n",
        "# ============================================================================\n",
        "\n",
        "# PART 1: INSTALLATION AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "# Install required packages\n",
        "!pip install plotly dash jupyter-dash scikit-learn xgboost tensorflow pandas numpy matplotlib seaborn requests\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output, dash_table\n",
        "import jupyter_dash\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import os\n",
        "import requests\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Setup Google Colab environment\n",
        "def setup_google_colab():\n",
        "    \"\"\"Setup function specifically for Google Colab users\"\"\"\n",
        "    print(\"üîß Setting up Google Colab environment...\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"‚úÖ Google Drive mounted successfully\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  Not running in Google Colab environment\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Drive mounting failed: {e}\")\n",
        "\n",
        "    # Create necessary directories\n",
        "    os.makedirs('/content/drive/MyDrive/Medical_Resource_Prediction', exist_ok=True)\n",
        "    print(\"‚úÖ Project directories created\")\n",
        "\n",
        "    # Download sample data\n",
        "    data_url = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\"\n",
        "    data_path = '/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv'\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(\"üì• Downloading COVID-19 dataset...\")\n",
        "        try:\n",
        "            response = requests.get(data_url)\n",
        "            response.raise_for_status()\n",
        "            with open(data_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(\"‚úÖ Dataset downloaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to download dataset: {e}\")\n",
        "            # Try alternative path\n",
        "            data_path = '/content/owid-covid-data.csv'\n",
        "            if not os.path.exists(data_path):\n",
        "                response = requests.get(data_url)\n",
        "                with open(data_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"‚úÖ Dataset saved to {data_path}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Dataset already exists\")\n",
        "\n",
        "    return data_path\n",
        "\n",
        "# PART 2: MEDICAL RESOURCE PREDICTOR CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class MedicalResourcePredictor:\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.data = None\n",
        "        self.processed_data = None\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.feature_importance = {}\n",
        "        self.training_results = {}\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and initial examination of data\"\"\"\n",
        "        print(\"üìä Loading COVID-19 data...\")\n",
        "        try:\n",
        "            self.data = pd.read_csv(self.data_path)\n",
        "            print(f\"‚úÖ Data loaded successfully. Shape: {self.data.shape}\")\n",
        "            print(f\"üìã Columns: {list(self.data.columns[:10])}...\")  # Show first 10 columns\n",
        "            return self.data\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        \"\"\"Comprehensive data preprocessing and feature engineering\"\"\"\n",
        "        print(\"üîÑ Starting data preprocessing...\")\n",
        "\n",
        "        if self.data is None:\n",
        "            print(\"‚ùå No data available for preprocessing\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Convert date column\n",
        "            self.data['date'] = pd.to_datetime(self.data['date'])\n",
        "\n",
        "            # Select relevant columns for medical resource prediction\n",
        "            resource_columns = [\n",
        "                'date', 'location', 'total_cases', 'new_cases', 'total_deaths',\n",
        "                'new_deaths', 'total_tests', 'new_tests', 'population',\n",
        "                'population_density', 'median_age', 'aged_65_older',\n",
        "                'aged_70_older', 'gdp_per_capita', 'hospital_beds_per_thousand',\n",
        "                'icu_patients', 'hosp_patients', 'weekly_icu_admissions',\n",
        "                'weekly_hosp_admissions', 'total_vaccinations', 'people_vaccinated',\n",
        "                'people_fully_vaccinated', 'total_boosters', 'stringency_index'\n",
        "            ]\n",
        "\n",
        "            # Filter columns that exist in the dataset\n",
        "            available_columns = [col for col in resource_columns if col in self.data.columns]\n",
        "            df = self.data[available_columns].copy()\n",
        "\n",
        "            print(f\"üìã Available columns: {len(available_columns)}/{len(resource_columns)}\")\n",
        "\n",
        "            # Focus on countries with sufficient data (at least 100 records)\n",
        "            country_data_counts = df.groupby('location').size()\n",
        "            countries_with_data = country_data_counts[country_data_counts >= 100].index\n",
        "            df = df[df['location'].isin(countries_with_data)].copy()\n",
        "\n",
        "            print(f\"üåç Countries with sufficient data: {len(countries_with_data)}\")\n",
        "\n",
        "            # Feature Engineering\n",
        "            print(\"‚öôÔ∏è Engineering features...\")\n",
        "\n",
        "            # Time-based features\n",
        "            df['year'] = df['date'].dt.year\n",
        "            df['month'] = df['date'].dt.month\n",
        "            df['day_of_year'] = df['date'].dt.dayofyear\n",
        "            df['week_of_year'] = df['date'].dt.isocalendar().week\n",
        "\n",
        "            # Sort data for rolling calculations\n",
        "            df = df.sort_values(['location', 'date'])\n",
        "\n",
        "            # Rolling averages for trend analysis\n",
        "            for col in ['new_cases', 'new_deaths', 'new_tests']:\n",
        "                if col in df.columns:\n",
        "                    df[f'{col}_7day_avg'] = df.groupby('location')[col].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\n",
        "                    df[f'{col}_14day_avg'] = df.groupby('location')[col].rolling(14, min_periods=1).mean().reset_index(0, drop=True)\n",
        "\n",
        "            # Growth rates\n",
        "            for col in ['total_cases', 'total_deaths']:\n",
        "                if col in df.columns:\n",
        "                    df[f'{col}_growth_rate'] = df.groupby('location')[col].pct_change().fillna(0)\n",
        "                    # Cap extreme growth rates\n",
        "                    df[f'{col}_growth_rate'] = df[f'{col}_growth_rate'].clip(-1, 5)\n",
        "\n",
        "            # Case fatality rate\n",
        "            if 'total_cases' in df.columns and 'total_deaths' in df.columns:\n",
        "                df['case_fatality_rate'] = np.where(df['total_cases'] > 0,\n",
        "                                                   df['total_deaths'] / df['total_cases'], 0)\n",
        "                df['case_fatality_rate'] = df['case_fatality_rate'].clip(0, 0.2)  # Cap at 20%\n",
        "\n",
        "            # Testing rate\n",
        "            if 'total_tests' in df.columns and 'population' in df.columns:\n",
        "                df['tests_per_capita'] = np.where(df['population'] > 0,\n",
        "                                                 df['total_tests'] / df['population'], 0)\n",
        "                df['tests_per_capita'] = df['tests_per_capita'].clip(0, 10)  # Cap at reasonable level\n",
        "\n",
        "            # Vaccination rate\n",
        "            if 'people_fully_vaccinated' in df.columns and 'population' in df.columns:\n",
        "                df['vaccination_rate'] = np.where(df['population'] > 0,\n",
        "                                                df['people_fully_vaccinated'] / df['population'], 0)\n",
        "                df['vaccination_rate'] = df['vaccination_rate'].clip(0, 1.2)  # Cap at 120%\n",
        "\n",
        "            # Create target variables (medical resource demands)\n",
        "            print(\"üéØ Creating target variables...\")\n",
        "\n",
        "            # ICU demand prediction\n",
        "            if 'icu_patients' in df.columns:\n",
        "                df['icu_demand'] = df['icu_patients'].fillna(0)\n",
        "            else:\n",
        "                # Estimate ICU demand based on severe cases (5% of active cases)\n",
        "                if 'total_cases' in df.columns and 'total_deaths' in df.columns:\n",
        "                    # Estimate recovered cases (rough approximation)\n",
        "                    df['estimated_recovered'] = df.groupby('location')['total_cases'].shift(14).fillna(0) * 0.95\n",
        "                    df['estimated_active_cases'] = (df['total_cases'] - df['total_deaths'] - df['estimated_recovered']).clip(0)\n",
        "                    df['icu_demand'] = df['estimated_active_cases'] * 0.05\n",
        "                else:\n",
        "                    df['icu_demand'] = df['new_cases'] * 0.1 if 'new_cases' in df.columns else 0\n",
        "\n",
        "            # Hospital bed demand\n",
        "            if 'hosp_patients' in df.columns:\n",
        "                df['hospital_demand'] = df['hosp_patients'].fillna(0)\n",
        "            else:\n",
        "                # Estimate hospital demand (15% of active cases)\n",
        "                if 'estimated_active_cases' in df.columns:\n",
        "                    df['hospital_demand'] = df['estimated_active_cases'] * 0.15\n",
        "                else:\n",
        "                    df['hospital_demand'] = df['new_cases'] * 0.2 if 'new_cases' in df.columns else 0\n",
        "\n",
        "            # Ventilator demand (80% of ICU patients)\n",
        "            df['ventilator_demand'] = df['icu_demand'] * 0.8\n",
        "\n",
        "            # PPE demand (based on healthcare workers and case load)\n",
        "            if 'new_cases' in df.columns:\n",
        "                df['ppe_demand'] = df['new_cases'] * 10  # 10 PPE units per case\n",
        "            else:\n",
        "                df['ppe_demand'] = 100  # Default minimal demand\n",
        "\n",
        "            # Handle missing values\n",
        "            numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "            for col in numeric_columns:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "            # Encode categorical variables\n",
        "            if 'location' in df.columns:\n",
        "                le = LabelEncoder()\n",
        "                df['location_encoded'] = le.fit_transform(df['location'])\n",
        "                self.location_encoder = le\n",
        "\n",
        "            # Remove extreme outliers using IQR method for target variables\n",
        "            for col in ['icu_demand', 'hospital_demand', 'ventilator_demand', 'ppe_demand']:\n",
        "                Q1 = df[col].quantile(0.05)  # Use 5th and 95th percentiles instead of IQR\n",
        "                Q3 = df[col].quantile(0.95)\n",
        "                df[col] = df[col].clip(Q1, Q3)\n",
        "\n",
        "            self.processed_data = df\n",
        "            print(f\"‚úÖ Preprocessing complete. Final data shape: {df.shape}\")\n",
        "            print(f\"üìä Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during preprocessing: {e}\")\n",
        "            return None\n",
        "\n",
        "    def prepare_features_targets(self):\n",
        "        \"\"\"Prepare feature matrices and target variables\"\"\"\n",
        "        if self.processed_data is None:\n",
        "            print(\"‚ùå No processed data available\")\n",
        "            return None, None, None\n",
        "\n",
        "        df = self.processed_data.copy()\n",
        "\n",
        "        # Define feature columns\n",
        "        feature_cols = [\n",
        "            'location_encoded', 'year', 'month', 'day_of_year', 'week_of_year',\n",
        "            'total_cases', 'new_cases', 'total_deaths', 'new_deaths',\n",
        "            'population', 'population_density', 'median_age', 'aged_65_older',\n",
        "            'gdp_per_capita', 'hospital_beds_per_thousand', 'stringency_index'\n",
        "        ]\n",
        "\n",
        "        # Add engineered features if they exist\n",
        "        engineered_features = [\n",
        "            'new_cases_7day_avg', 'new_cases_14day_avg', 'new_deaths_7day_avg',\n",
        "            'total_cases_growth_rate', 'case_fatality_rate', 'tests_per_capita',\n",
        "            'vaccination_rate'\n",
        "        ]\n",
        "\n",
        "        for feat in engineered_features:\n",
        "            if feat in df.columns:\n",
        "                feature_cols.append(feat)\n",
        "\n",
        "        # Filter features that exist in the data\n",
        "        available_features = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "        X = df[available_features].copy()\n",
        "\n",
        "        # Target variables\n",
        "        targets = {\n",
        "            'icu_demand': df['icu_demand'],\n",
        "            'hospital_demand': df['hospital_demand'],\n",
        "            'ventilator_demand': df['ventilator_demand'],\n",
        "            'ppe_demand': df['ppe_demand']\n",
        "        }\n",
        "\n",
        "        print(f\"üìä Features prepared: {len(available_features)} features\")\n",
        "        print(f\"üéØ Targets prepared: {len(targets)} target variables\")\n",
        "\n",
        "        return X, targets, available_features\n",
        "\n",
        "    def train_random_forest(self, X_train, y_train, target_name):\n",
        "        \"\"\"Train Random Forest model\"\"\"\n",
        "        print(f\"üå≤ Training Random Forest for {target_name}...\")\n",
        "\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        rf.fit(X_train, y_train)\n",
        "        self.models[f'rf_{target_name}'] = rf\n",
        "\n",
        "        # Feature importance\n",
        "        self.feature_importance[f'rf_{target_name}'] = dict(\n",
        "            zip(X_train.columns, rf.feature_importances_)\n",
        "        )\n",
        "\n",
        "        return rf\n",
        "\n",
        "    def train_xgboost(self, X_train, y_train, target_name):\n",
        "        \"\"\"Train XGBoost model\"\"\"\n",
        "        print(f\"üöÄ Training XGBoost for {target_name}...\")\n",
        "\n",
        "        xgb_model = xgb.XGBRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            verbosity=0\n",
        "        )\n",
        "\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "        self.models[f'xgb_{target_name}'] = xgb_model\n",
        "\n",
        "        # Feature importance\n",
        "        self.feature_importance[f'xgb_{target_name}'] = dict(\n",
        "            zip(X_train.columns, xgb_model.feature_importances_)\n",
        "        )\n",
        "\n",
        "        return xgb_model\n",
        "\n",
        "    def train_gradient_boosting(self, X_train, y_train, target_name):\n",
        "        \"\"\"Train Gradient Boosting model\"\"\"\n",
        "        print(f\"üìà Training Gradient Boosting for {target_name}...\")\n",
        "\n",
        "        gb = GradientBoostingRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        gb.fit(X_train, y_train)\n",
        "        self.models[f'gb_{target_name}'] = gb\n",
        "\n",
        "        # Feature importance\n",
        "        self.feature_importance[f'gb_{target_name}'] = dict(\n",
        "            zip(X_train.columns, gb.feature_importances_)\n",
        "        )\n",
        "\n",
        "        return gb\n",
        "\n",
        "    def train_ensemble(self, X_train, y_train, target_name):\n",
        "        \"\"\"Train ensemble model combining RF, XGB, and GB\"\"\"\n",
        "        print(f\"üé≠ Training Ensemble for {target_name}...\")\n",
        "\n",
        "        # Individual models\n",
        "        rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)\n",
        "        xgb_model = xgb.XGBRegressor(n_estimators=50, random_state=42, verbosity=0)\n",
        "        gb = GradientBoostingRegressor(n_estimators=50, random_state=42)\n",
        "\n",
        "        # Ensemble\n",
        "        ensemble = VotingRegressor([\n",
        "            ('rf', rf),\n",
        "            ('xgb', xgb_model),\n",
        "            ('gb', gb)\n",
        "        ])\n",
        "\n",
        "        ensemble.fit(X_train, y_train)\n",
        "        self.models[f'ensemble_{target_name}'] = ensemble\n",
        "\n",
        "        return ensemble\n",
        "\n",
        "    def evaluate_model(self, model, X_test, y_test):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        try:\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            return {\n",
        "                'MSE': mse,\n",
        "                'MAE': mae,\n",
        "                'R2': r2,\n",
        "                'RMSE': np.sqrt(mse)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error evaluating model: {e}\")\n",
        "            return {'MSE': 0, 'MAE': 0, 'R2': 0, 'RMSE': 0}\n",
        "\n",
        "    def train_all_models(self):\n",
        "        \"\"\"Train all models for all target variables\"\"\"\n",
        "        print(\"üöÄ Starting comprehensive model training...\")\n",
        "\n",
        "        X, targets, feature_names = self.prepare_features_targets()\n",
        "\n",
        "        if X is None:\n",
        "            print(\"‚ùå Cannot proceed without features\")\n",
        "            return {}\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = pd.DataFrame(\n",
        "            scaler.fit_transform(X),\n",
        "            columns=X.columns,\n",
        "            index=X.index\n",
        "        )\n",
        "        self.scalers['feature_scaler'] = scaler\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for target_name, y in targets.items():\n",
        "            print(f\"\\nüéØ Training models for {target_name}...\")\n",
        "\n",
        "            # Train-test split\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X_scaled, y, test_size=0.2, random_state=42\n",
        "            )\n",
        "\n",
        "            target_results = {}\n",
        "\n",
        "            try:\n",
        "                # Train Random Forest\n",
        "                rf_model = self.train_random_forest(X_train, y_train, target_name)\n",
        "                target_results['Random Forest'] = self.evaluate_model(rf_model, X_test, y_test)\n",
        "\n",
        "                # Train XGBoost\n",
        "                xgb_model = self.train_xgboost(X_train, y_train, target_name)\n",
        "                target_results['XGBoost'] = self.evaluate_model(xgb_model, X_test, y_test)\n",
        "\n",
        "                # Train Gradient Boosting\n",
        "                gb_model = self.train_gradient_boosting(X_train, y_train, target_name)\n",
        "                target_results['Gradient Boosting'] = self.evaluate_model(gb_model, X_test, y_test)\n",
        "\n",
        "                # Train Ensemble\n",
        "                ensemble_model = self.train_ensemble(X_train, y_train, target_name)\n",
        "                target_results['Ensemble'] = self.evaluate_model(ensemble_model, X_test, y_test)\n",
        "\n",
        "                results[target_name] = target_results\n",
        "                print(f\"‚úÖ Completed training for {target_name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error training models for {target_name}: {e}\")\n",
        "                results[target_name] = {}\n",
        "\n",
        "        self.training_results = results\n",
        "        print(\"üéâ Model training completed!\")\n",
        "        return results\n",
        "\n",
        "# PART 3: VISUALIZATION AND ANALYSIS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def analyze_feature_importance(predictor):\n",
        "    \"\"\"Analyze and visualize feature importance across all models\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üìä FEATURE IMPORTANCE ANALYSIS\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if not hasattr(predictor, 'feature_importance') or not predictor.feature_importance:\n",
        "        print(\"‚ùå Feature importance data not available. Please train models first.\")\n",
        "        return\n",
        "\n",
        "    # Aggregate feature importance across all models\n",
        "    all_features = {}\n",
        "\n",
        "    for model_target, importance_dict in predictor.feature_importance.items():\n",
        "        for feature, importance in importance_dict.items():\n",
        "            if feature not in all_features:\n",
        "                all_features[feature] = []\n",
        "            all_features[feature].append(importance)\n",
        "\n",
        "    # Calculate average importance\n",
        "    avg_importance = {\n",
        "        feature: np.mean(importances)\n",
        "        for feature, importances in all_features.items()\n",
        "    }\n",
        "\n",
        "    # Sort by importance\n",
        "    sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    print(\"\\nüèÜ TOP 15 MOST IMPORTANT FEATURES (Average across all models):\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"{'Rank':<5} {'Feature':<30} {'Importance':<12} {'Bar'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    max_importance = sorted_features[0][1] if sorted_features else 1\n",
        "\n",
        "    for i, (feature, importance) in enumerate(sorted_features[:15], 1):\n",
        "        bar_length = int((importance / max_importance) * 20)\n",
        "        bar = \"‚ñà\" * bar_length\n",
        "        print(f\"{i:<5} {feature:<30} {importance:<12.4f} {bar}\")\n",
        "\n",
        "def generate_predictions_report(predictor, country='United States', days_ahead=30):\n",
        "    \"\"\"Generate detailed predictions report for a specific country\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìã MEDICAL RESOURCE PREDICTION REPORT - {country}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Get country data\n",
        "        if predictor.processed_data is None:\n",
        "            print(\"‚ùå No processed data available\")\n",
        "            return\n",
        "\n",
        "        country_data = predictor.processed_data[\n",
        "            predictor.processed_data['location'] == country\n",
        "        ].copy()\n",
        "\n",
        "        if len(country_data) == 0:\n",
        "            available_countries = predictor.processed_data['location'].unique()[:10]\n",
        "            print(f\"‚ùå No data available for {country}\")\n",
        "            print(f\"üìç Available countries (sample): {list(available_countries)}\")\n",
        "            return\n",
        "\n",
        "        latest_data = country_data.iloc[-1]\n",
        "\n",
        "        print(f\"üìÖ Report Date: {latest_data['date'].strftime('%Y-%m-%d')}\")\n",
        "        print(f\"üë• Population: {latest_data['population']:,.0f}\")\n",
        "\n",
        "        # Current resource demands\n",
        "        resources = {\n",
        "            'ICU Beds': latest_data['icu_demand'],\n",
        "            'Hospital Beds': latest_data['hospital_demand'],\n",
        "            'Ventilators': latest_data['ventilator_demand'],\n",
        "            'PPE Units': latest_data['ppe_demand']\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüìä CURRENT RESOURCE DEMANDS:\")\n",
        "        print(\"-\" * 40)\n",
        "        for resource, demand in resources.items():\n",
        "            print(f\"{resource:<15}: {demand:>10,.0f}\")\n",
        "\n",
        "        # Simple predictions\n",
        "        print(f\"\\nüîÆ PREDICTED DEMANDS (+{days_ahead} days):\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        growth_rates = {'ICU Beds': 1.15, 'Hospital Beds': 1.10, 'Ventilators': 1.12, 'PPE Units': 1.20}\n",
        "\n",
        "        for resource, current_demand in resources.items():\n",
        "            predicted = current_demand * growth_rates[resource]\n",
        "            change = ((predicted - current_demand) / current_demand) * 100 if current_demand > 0 else 0\n",
        "            print(f\"{resource:<15}: {predicted:>10,.0f} ({change:+.1f}%)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating report: {str(e)}\")\n",
        "\n",
        "def create_visualizations(predictor):\n",
        "    \"\"\"Create comprehensive visualizations\"\"\"\n",
        "\n",
        "    print(f\"\\nüìä Creating visualizations...\")\n",
        "\n",
        "    if predictor.processed_data is None:\n",
        "        print(\"‚ùå No data available for visualization\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = predictor.processed_data\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Medical Resource Demand Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Resource demand trends over time\n",
        "        ax1 = axes[0, 0]\n",
        "        resources_to_plot = ['icu_demand', 'hospital_demand', 'ventilator_demand']\n",
        "        for resource in resources_to_plot:\n",
        "            if resource in df.columns:\n",
        "                monthly_avg = df.groupby(df['date'].dt.to_period('M'))[resource].mean()\n",
        "                ax1.plot(range(len(monthly_avg)), monthly_avg.values,\n",
        "                        label=resource.replace('_', ' ').title(), linewidth=2)\n",
        "        ax1.set_title('Resource Demand Trends Over Time')\n",
        "        ax1.set_xlabel('Time Period')\n",
        "        ax1.set_ylabel('Average Demand')\n",
        "        ax1.legend()\n",
        "\n",
        "        # 2. ICU demand distribution\n",
        "        ax2 = axes[0, 1]\n",
        "        if 'icu_demand' in df.columns:\n",
        "            ax2.hist(df['icu_demand'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            ax2.set_title('ICU Demand Distribution')\n",
        "            ax2.set_xlabel('ICU Demand')\n",
        "            ax2.set_ylabel('Frequency')\n",
        "\n",
        "        # 3. Top countries by ICU demand\n",
        "        ax3 = axes[1, 0]\n",
        "        if 'location' in df.columns and 'icu_demand' in df.columns:\n",
        "            top_countries = df.groupby('location')['icu_demand'].max().nlargest(10)\n",
        "            ax3.barh(range(len(top_countries)), top_countries.values, color='lightcoral')\n",
        "            ax3.set_yticks(range(len(top_countries)))\n",
        "            ax3.set_yticklabels([str(country)[:15] + '...' if len(str(country)) > 15 else str(country)\n",
        "                                for country in top_countries.index])\n",
        "            ax3.set_title('Top 10 Countries by Peak ICU Demand')\n",
        "            ax3.set_xlabel('Peak ICU Demand')\n",
        "\n",
        "        # 4. Model performance comparison\n",
        "        ax4 = axes[1, 1]\n",
        "        if hasattr(predictor, 'training_results') and predictor.training_results:\n",
        "            models = []\n",
        "            r2_scores = []\n",
        "\n",
        "            for target, model_results in predictor.training_results.items():\n",
        "                if target == 'icu_demand':\n",
        "                    for model_name, metrics in model_results.items():\n",
        "                        models.append(model_name[:10])  # Truncate long names\n",
        "                        r2_scores.append(metrics.get('R2', 0))\n",
        "\n",
        "            if models:\n",
        "                colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99']\n",
        "                bars = ax4.bar(models, r2_scores, color=colors[:len(models)])\n",
        "                ax4.set_title('Model Performance (R¬≤ Score)')\n",
        "                ax4.set_ylabel('R¬≤ Score')\n",
        "                ax4.tick_params(axis='x', rotation=45)\n",
        "\n",
        "                # Add value labels on bars\n",
        "                for bar, score in zip(bars, r2_scores):\n",
        "                    height = bar.get_height()\n",
        "                    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                            f'{score:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('medical_resource_analysis.png', dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"‚úÖ Visualizations created and saved as 'medical_resource_analysis.png'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating visualizations: {e}\")\n",
        "\n",
        "# PART 4: MAIN EXECUTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"üè• MEDICAL RESOURCE DEMAND PREDICTION SYSTEM\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"üîß Setting up environment...\")\n",
        "\n",
        "    # Setup Google Colab\n",
        "    data_path = setup_google_colab()\n",
        "\n",
        "    # Initialize predictor\n",
        "    predictor = MedicalResourcePredictor(data_path)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"\\nüìä Loading and preprocessing data...\")\n",
        "    if predictor.load_data() is None:\n",
        "        print(\"‚ùå Failed to load data. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    if predictor.preprocess_data() is None:\n",
        "        print(\"‚ùå Failed to preprocess data. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Train all models\n",
        "    print(\"\\nüöÄ Training machine learning models...\")\n",
        "    results = predictor.train_all_models()\n",
        "\n",
        "    # Display results\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"üìä TRAINING RESULTS SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for target, models in results.items():\n",
        "            print(f\"\\nüéØ {target.upper().replace('_', ' ')}:\")\n",
        "            for model_name, metrics in models.items():\n",
        "                print(f\"  üìà {model_name}:\")\n",
        "                print(f\"    MAE: {metrics['MAE']:.2f}\")\n",
        "                print(f\"    RMSE: {metrics['RMSE']:.2f}\")\n",
        "                print(f\"    R¬≤: {metrics['R2']:.3f}\")\n",
        "\n",
        "    # Generate additional analyses\n",
        "    print(\"\\nüîç Generating additional analyses...\")\n",
        "\n",
        "    # Feature importance analysis\n",
        "    analyze_feature_importance(predictor)\n",
        "\n",
        "    # Generate prediction report for a sample country\n",
        "    sample_countries = ['United States', 'United Kingdom', 'Germany', 'France', 'Italy']\n",
        "    available_countries = predictor.processed_data['location'].unique()\n",
        "\n",
        "    for country in sample_countries:\n",
        "        if country in available_countries:\n",
        "            generate_predictions_report(predictor, country=country)\n",
        "            break\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  Sample countries not found in dataset. Using first available country.\")\n",
        "        generate_predictions_report(predictor, country=available_countries[0])\n",
        "\n",
        "    # Create visualizations\n",
        "    create_visualizations(predictor)\n",
        "\n",
        "    print(\"\\nüéâ Medical Resource Demand Prediction System completed successfully!\")\n",
        "    return predictor\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    predictor = main()"
      ],
      "metadata": {
        "id": "r9F857c37lBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Addressing Data Quality"
      ],
      "metadata": {
        "id": "0Ln8TsGO-4nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing data - we'll focus on countries with substantial data\n",
        "country_coverage = df.groupby('location').apply(lambda x: x.isnull().mean())\n",
        "good_coverage_countries = country_coverage[country_coverage['icu_patients_per_million'] < 0.7].index\n",
        "df = df[df['location'].isin(good_coverage_countries)]\n",
        "\n",
        "# Focus on key resource demand indicators\n",
        "resource_metrics = [\n",
        "    'icu_patients_per_million',\n",
        "    'hosp_patients_per_million',\n",
        "    'weekly_icu_admissions_per_million',\n",
        "    'weekly_hosp_admissions_per_million'\n",
        "]\n",
        "\n",
        "# Plot resource metric trends\n",
        "plt.figure(figsize=(12, 8))\n",
        "for metric in resource_metrics:\n",
        "    if metric in df.columns:\n",
        "        sns.lineplot(data=df, x='date', y=metric, label=metric)\n",
        "plt.title('Medical Resource Demand Metrics Over Time')\n",
        "plt.ylabel('Per Million People')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NULl9hqCAE7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation\n",
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "oiAYivzFAdDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporal features\n",
        "df['day_of_week'] = df['date'].dt.dayofweek\n",
        "df['month'] = df['date'].dt.month\n",
        "df['day_of_year'] = df['date'].dt.dayofyear\n",
        "\n",
        "# Calculate rolling averages for key metrics\n",
        "window_size = 7\n",
        "for metric in resource_metrics:\n",
        "    if metric in df.columns:\n",
        "        df[f'{metric}_rolling_avg'] = df.groupby('location')[metric].transform(\n",
        "            lambda x: x.rolling(window_size, min_periods=1).mean()\n",
        "        )\n",
        "\n",
        "# Create target variable - ICU demand 2 weeks ahead\n",
        "df['future_icu_demand'] = df.groupby('location')['icu_patients_per_million'].shift(-14)\n",
        "\n",
        "# Select relevant features\n",
        "features = [\n",
        "    'new_cases_per_million', 'new_deaths_per_million',\n",
        "    'positive_rate', 'tests_per_case',\n",
        "    'people_vaccinated_per_hundred', 'population_density',\n",
        "    'median_age', 'aged_65_older', 'gdp_per_capita',\n",
        "    'hospital_beds_per_thousand', 'day_of_week', 'month',\n",
        "    'icu_patients_per_million_rolling_avg',\n",
        "    'hosp_patients_per_million_rolling_avg'\n",
        "]\n",
        "\n",
        "# Filter dataframe\n",
        "model_df = df[['location', 'date', 'future_icu_demand'] + features].dropna()\n",
        "\n",
        "# Encode categorical variables\n",
        "model_df = pd.get_dummies(model_df, columns=['location'], drop_first=True)"
      ],
      "metadata": {
        "id": "RUAhnNrJBFmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "9icKozvJBokM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = model_df.drop(['future_icu_demand', 'date'], axis=1)\n",
        "y = model_df['future_icu_demand']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, shuffle=False)"
      ],
      "metadata": {
        "id": "NNb1aW7-Bpcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation of Top Models"
      ],
      "metadata": {
        "id": "FNcqexeXB9fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# XGBoost Model\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=8,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    preds = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    rmse = sqrt(mean_squared_error(y_test, preds))\n",
        "    print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "    return preds\n",
        "\n",
        "print(\"XGBoost Performance:\")\n",
        "xgb_preds = evaluate_model(xgb_model, X_test, y_test)\n",
        "\n",
        "print(\"\\nRandom Forest Performance:\")\n",
        "rf_preds = evaluate_model(rf_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "Md_oHrpjB8qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance Analysis"
      ],
      "metadata": {
        "id": "tDq5EA_oCZ0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Feature Importance\n",
        "xgb_importance = xgb_model.feature_importances_\n",
        "sorted_idx = np.argsort(xgb_importance)[-10:]  # Top 10 features\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(sorted_idx)), xgb_importance[sorted_idx], align='center')\n",
        "plt.yticks(range(len(sorted_idx)), np.array(X.columns)[sorted_idx])\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDfA2-JPCa6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Evaluation\n",
        "# Model Comparison"
      ],
      "metadata": {
        "id": "Kb2ao__DCluy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Time-series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Cross-validation for XGBoost\n",
        "xgb_scores = cross_val_score(\n",
        "    xgb_model, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
        "print(f\"XGBoost CV MAE: {-xgb_scores.mean():.2f} (¬± {xgb_scores.std():.2f})\")\n",
        "\n",
        "# Cross-validation for Random Forest\n",
        "rf_scores = cross_val_score(\n",
        "    rf_model, X, y, cv=tscv, scoring='neg_mean_absolute_error')\n",
        "print(f\"Random Forest CV MAE: {-rf_scores.mean():.2f} (¬± {rf_scores.std():.2f})\")"
      ],
      "metadata": {
        "id": "NibsoOmMCkvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of saving model for deployment\n",
        "import joblib\n",
        "\n",
        "joblib.dump(xgb_model, 'icu_demand_predictor.pkl')\n",
        "\n",
        "# Example inference function\n",
        "def predict_icu_demand(country, current_metrics):\n",
        "    \"\"\"Predict ICU demand 2 weeks ahead for a country\"\"\"\n",
        "    # Preprocess input metrics\n",
        "    input_df = preprocess_input(current_metrics)\n",
        "\n",
        "    # Load model\n",
        "    model = joblib.load('icu_demand_predictor.pkl')\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(input_df)\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "Z7o3Lw79Dbeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ox4tREXYYm0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive and load data\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv'\n",
        "df = pd.read_csv(data_path, parse_dates=['date'])\n",
        "\n",
        "# Initial data inspection\n",
        "print(f\"Dataset dimensions: {df.shape}\")\n",
        "print(\"\\nData types:\\n\", df.dtypes.value_counts())\n",
        "print(\"\\nMissing values percentage:\\n\", df.isnull().mean().sort_values(ascending=False).head(15))\n",
        "\n",
        "# Temporal coverage analysis\n",
        "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Country-level data completeness\n",
        "country_coverage = df.groupby('location').apply(lambda x: x[['hosp_patients_per_million',\n",
        "                                                           'icu_patients_per_million']].notnull().mean())\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(country_coverage.T, cmap='viridis')\n",
        "plt.title('Hospitalization Data Coverage by Country')\n",
        "plt.show()\n",
        "\n",
        "# Resource metric distributions\n",
        "resource_cols = ['hosp_patients_per_million', 'icu_patients_per_million',\n",
        "                'weekly_hosp_admissions_per_million', 'weekly_icu_admissions_per_million']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "for col, ax in zip(resource_cols, axes.flatten()):\n",
        "    if col in df.columns:\n",
        "        sns.histplot(df[col].dropna(), kde=True, ax=ax)\n",
        "        ax.set_title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Temporal trends for key metrics\n",
        "plt.figure(figsize=(14, 8))\n",
        "for metric in resource_cols:\n",
        "    if metric in df.columns:\n",
        "        sns.lineplot(data=df.groupby('date')[metric].mean().reset_index(),\n",
        "                    x='date', y=metric, label=metric)\n",
        "plt.title('Global Average Medical Resource Demand Over Time')\n",
        "plt.ylabel('Patients per Million')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Correlation analysis\n",
        "corr_matrix = df[resource_cols + ['new_cases_smoothed_per_million',\n",
        "                                'people_fully_vaccinated_per_hundred',\n",
        "                                'stringency_index',\n",
        "                                'hospital_beds_per_thousand']].corr()\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Matrix of Key Variables')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PlCBg8mjX9LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "iewwN9-jYpls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "def prepare_features(df):\n",
        "    # Create temporal features\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day_of_year'] = df['date'].dt.dayofyear\n",
        "\n",
        "    # Calculate growth rates\n",
        "    df['case_growth_7day'] = df.groupby('location')['new_cases_smoothed_per_million'].pct_change(7)\n",
        "    df['hosp_growth_7day'] = df.groupby('location')['hosp_patients_per_million'].pct_change(7)\n",
        "\n",
        "    # Vaccination effectiveness metric\n",
        "    df['vax_effectiveness'] = (df['people_fully_vaccinated_per_hundred'] * 0.7 +\n",
        "                              df['total_boosters_per_hundred'] * 0.3)\n",
        "\n",
        "    # Healthcare strain index\n",
        "    df['strain_index'] = (df['hosp_patients_per_million'] /\n",
        "                         df['hospital_beds_per_thousand']) * 1000\n",
        "\n",
        "    # Wave detection\n",
        "    df['pandemic_wave'] = (df['new_cases_smoothed_per_million'].rolling(14).mean() > 100).astype(int)\n",
        "\n",
        "    # Target variables - 14-day ahead forecast\n",
        "    df['future_hosp_demand'] = df.groupby('location')['hosp_patients_per_million'].shift(-14)\n",
        "    df['future_icu_demand'] = df.groupby('location')['icu_patients_per_million'].shift(-14)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = prepare_features(df)\n",
        "\n",
        "# Select complete cases\n",
        "model_df = df[['location', 'date', 'future_hosp_demand', 'future_icu_demand',\n",
        "              'new_cases_smoothed_per_million', 'case_growth_7day',\n",
        "              'positive_rate', 'reproduction_rate',\n",
        "              'vax_effectiveness', 'strain_index',\n",
        "              'hospital_beds_per_thousand', 'median_age',\n",
        "              'aged_65_older', 'stringency_index',\n",
        "              'pandemic_wave']].dropna()\n",
        "\n",
        "# Encode categorical variables\n",
        "model_df = pd.get_dummies(model_df, columns=['location'], drop_first=True)\n",
        "\n",
        "# Temporal train-test split\n",
        "train = model_df[model_df['date'] < '2022-01-01']\n",
        "test = model_df[model_df['date'] >= '2022-01-01']\n",
        "\n",
        "X_train = train.drop(['date', 'future_hosp_demand', 'future_icu_demand'], axis=1)\n",
        "y_train_hosp = train['future_hosp_demand']\n",
        "y_train_icu = train['future_icu_demand']\n",
        "\n",
        "X_test = test.drop(['date', 'future_hosp_demand', 'future_icu_demand'], axis=1)\n",
        "y_test_hosp = test['future_hosp_demand']\n",
        "y_test_icu = test['future_icu_demand']"
      ],
      "metadata": {
        "id": "rn4yBQfZYv_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List columns related to healthcare resources and potential predictors\n",
        "healthcare_cols = [col for col in df.columns if any(term in col.lower() for term in\n",
        "                   ['hospital', 'icu', 'ventilator', 'bed', 'patient', 'medical',\n",
        "                    'healthcare', 'resource', 'admission'])]\n",
        "\n",
        "case_cols = [col for col in df.columns if any(term in col.lower() for term in\n",
        "             ['case', 'death', 'positive', 'test', 'confirmed'])]\n",
        "\n",
        "demographic_cols = [col for col in df.columns if any(term in col.lower() for term in\n",
        "                   ['population', 'density', 'age', 'gdp', 'income', 'poverty'])]\n",
        "\n",
        "# Display identified columns\n",
        "print(\"Healthcare resource columns:\", healthcare_cols)\n",
        "print(\"Case/infection columns:\", case_cols)\n",
        "print(\"Demographic columns:\", demographic_cols)"
      ],
      "metadata": {
        "id": "6G4-cpUl8lSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a few representative countries\n",
        "countries = ['United States', 'United Kingdom', 'India', 'Brazil', 'South Africa']\n",
        "\n",
        "# Plot hospital admissions or ICU patients over time (if available)\n",
        "plt.figure(figsize=(14, 8))\n",
        "for country in countries:\n",
        "    country_data = df[df['location'] == country]\n",
        "    if 'hosp_patients' in df.columns:\n",
        "        plt.plot(pd.to_datetime(country_data['date']),\n",
        "                 country_data['hosp_patients'], label=country)\n",
        "\n",
        "plt.title('COVID-19 Hospital Patients Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot new cases vs. hospital admissions (if available)\n",
        "if 'new_cases' in df.columns and 'weekly_hosp_admissions' in df.columns:\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    for country in countries:\n",
        "        country_data = df[df['location'] == country].dropna(subset=['new_cases', 'weekly_hosp_admissions'])\n",
        "        plt.scatter(country_data['new_cases'], country_data['weekly_hosp_admissions'],\n",
        "                    alpha=0.6, label=country)\n",
        "\n",
        "    plt.title('New Cases vs. Hospital Admissions')\n",
        "    plt.xlabel('New Cases')\n",
        "    plt.ylabel('Weekly Hospital Admissions')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lIo2Ycv59dKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Analysis"
      ],
      "metadata": {
        "id": "YEKqQYeV_Mxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select relevant numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "relevant_cols = [col for col in numeric_cols if col not in ['iso_code']]\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df[relevant_cols].corr()\n",
        "\n",
        "# Visualize correlations with hospitalization data\n",
        "plt.figure(figsize=(16, 12))\n",
        "hosp_cols = [col for col in relevant_cols if 'hosp' in col or 'icu' in col]\n",
        "if hosp_cols:\n",
        "    hosp_corr = corr_matrix[hosp_cols].sort_values(by=hosp_cols[0], ascending=False)\n",
        "    sns.heatmap(hosp_corr.head(15), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Correlation with Hospitalization Data')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "t0pmBI46_LXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning"
      ],
      "metadata": {
        "id": "hPPOL7lBBeBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date column to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Identify key target variables for prediction\n",
        "target_variables = [col for col in df.columns if col in\n",
        "                   ['hosp_patients', 'icu_patients', 'weekly_hosp_admissions',\n",
        "                    'weekly_icu_admissions', 'hospital_beds_per_thousand']]\n",
        "\n",
        "# Handle missing values in target variables\n",
        "# For time series data, forward-fill might be appropriate\n",
        "for col in target_variables:\n",
        "    if col in df.columns:\n",
        "        # Group by location and forward fill\n",
        "        df[col] = df.groupby('location')[col].transform(\n",
        "            lambda x: x.ffill().bfill()\n",
        "        )\n",
        "\n",
        "# Filter countries with sufficient healthcare data\n",
        "if target_variables:\n",
        "    min_data_points = 30  # Minimum required data points\n",
        "    valid_countries = df.groupby('location').apply(\n",
        "        lambda x: x[target_variables].notna().sum().min() >= min_data_points\n",
        "    )\n",
        "    valid_countries = valid_countries[valid_countries].index.tolist()\n",
        "\n",
        "    print(f\"Number of countries with sufficient healthcare data: {len(valid_countries)}\")\n",
        "    df_filtered = df[df['location'].isin(valid_countries)]\n",
        "else:\n",
        "    print(\"No target healthcare variables found, using complete dataset\")\n",
        "    df_filtered = df.copy()\n",
        "\n",
        "# Handle outliers in key variables\n",
        "def handle_outliers(df, column, lower_quantile=0.001, upper_quantile=0.999):\n",
        "    if column in df.columns:\n",
        "        q_low = df[column].quantile(lower_quantile)\n",
        "        q_high = df[column].quantile(upper_quantile)\n",
        "        df[column] = df[column].clip(lower=q_low, upper=q_high)\n",
        "    return df\n",
        "\n",
        "# Apply outlier handling to case, death and hospitalization data\n",
        "for col in ['new_cases', 'new_deaths'] + target_variables:\n",
        "    if col in df.columns:\n",
        "        df_filtered = handle_outliers(df_filtered, col)\n",
        "\n",
        "# Remove unnecessary columns\n",
        "cols_to_drop = ['iso_code', 'tests_units']  # Add more as needed\n",
        "df_filtered = df_filtered.drop(columns=[col for col in cols_to_drop if col in df_filtered.columns])"
      ],
      "metadata": {
        "id": "jsYCa8U1BhtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "# Creating new features that might help predict resource demand"
      ],
      "metadata": {
        "id": "sx7pFng4DVFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add time-based features\n",
        "df_filtered['year'] = df_filtered['date'].dt.year\n",
        "df_filtered['month'] = df_filtered['date'].dt.month\n",
        "df_filtered['week'] = df_filtered['date'].dt.isocalendar().week\n",
        "df_filtered['day_of_week'] = df_filtered['date'].dt.dayofweek\n",
        "\n",
        "# Calculate rolling metrics for cases and deaths\n",
        "for window in [7, 14, 30]:\n",
        "    # Group by location and calculate rolling averages\n",
        "    df_filtered[f'new_cases_rolling_{window}d'] = df_filtered.groupby('location')['new_cases'].transform(\n",
        "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "    df_filtered[f'new_deaths_rolling_{window}d'] = df_filtered.groupby('location')['new_deaths'].transform(\n",
        "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
        "    )\n",
        "\n",
        "# Calculate rate of change (acceleration/deceleration)\n",
        "df_filtered['case_growth_rate'] = df_filtered.groupby('location')['new_cases_rolling_7d'].transform(\n",
        "    lambda x: x.pct_change()\n",
        ")\n",
        "\n",
        "# Calculate case fatality rate (CFR)\n",
        "df_filtered['case_fatality_rate'] = df_filtered['total_deaths'] / df_filtered['total_cases']\n",
        "\n",
        "# Create lagged features (cases tend to precede hospitalizations)\n",
        "for lag in [7, 14, 21]:\n",
        "    df_filtered[f'new_cases_lag_{lag}d'] = df_filtered.groupby('location')['new_cases'].transform(\n",
        "        lambda x: x.shift(lag)\n",
        "    )\n",
        "\n",
        "    df_filtered[f'new_deaths_lag_{lag}d'] = df_filtered.groupby('location')['new_deaths'].transform(\n",
        "        lambda x: x.shift(lag)\n",
        "    )\n",
        "\n",
        "# Create features representing healthcare capacity\n",
        "if 'hospital_beds_per_thousand' in df_filtered.columns and 'population' in df_filtered.columns:\n",
        "    # Estimate total bed capacity\n",
        "    df_filtered['estimated_total_beds'] = (\n",
        "        df_filtered['hospital_beds_per_thousand'] * df_filtered['population'] / 1000\n",
        "    )\n",
        "\n",
        "# Population demographic features\n",
        "age_cols = [col for col in df_filtered.columns if 'aged' in col]\n",
        "if age_cols:\n",
        "    # Create composite risk score based on age demographics\n",
        "    # Higher weights for older population groups which typically require more resources\n",
        "    weights = {\n",
        "        'aged_65_older': 1.0,\n",
        "        'aged_70_older': 1.5,\n",
        "        'population_80plus': 2.0  # If this column exists\n",
        "    }\n",
        "\n",
        "    # Calculate age risk score\n",
        "    for col, weight in weights.items():\n",
        "        if col in df_filtered.columns:\n",
        "            if 'age_risk_score' not in df_filtered.columns:\n",
        "                df_filtered['age_risk_score'] = 0\n",
        "            df_filtered['age_risk_score'] += df_filtered[col] * weight"
      ],
      "metadata": {
        "id": "G4dlCGX7DbQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n",
        "# Selecting the most relevant features for modeling:"
      ],
      "metadata": {
        "id": "rNNyl_CpEFqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify potential predictor variables\n",
        "predictors = [\n",
        "    # Case and death metrics\n",
        "    'new_cases', 'new_deaths', 'total_cases', 'total_deaths',\n",
        "    'new_cases_per_million', 'new_deaths_per_million',\n",
        "    'case_fatality_rate',\n",
        "\n",
        "    # Rolling averages and lagged features\n",
        "    'new_cases_rolling_7d', 'new_cases_rolling_14d', 'new_cases_rolling_30d',\n",
        "    'new_deaths_rolling_7d', 'new_deaths_rolling_14d', 'new_deaths_rolling_30d',\n",
        "    'new_cases_lag_7d', 'new_cases_lag_14d', 'new_cases_lag_21d',\n",
        "    'case_growth_rate',\n",
        "\n",
        "    # Healthcare capacity\n",
        "    'hospital_beds_per_thousand', 'estimated_total_beds',\n",
        "\n",
        "    # Demographics\n",
        "    'population', 'population_density', 'median_age', 'aged_65_older', 'aged_70_older',\n",
        "    'age_risk_score',\n",
        "\n",
        "    # GDP and healthcare indicators\n",
        "    'gdp_per_capita', 'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
        "    'female_smokers', 'male_smokers', 'handwashing_facilities',\n",
        "\n",
        "    # Time-based features\n",
        "    'month', 'week', 'day_of_week'\n",
        "]\n",
        "\n",
        "# Filter to include only columns that actually exist in the dataset\n",
        "predictors = [col for col in predictors if col in df_filtered.columns]\n",
        "\n",
        "# Check feature correlations\n",
        "correlation_matrix = df_filtered[predictors].corr()\n",
        "\n",
        "# Identify highly correlated features (absolute correlation > 0.9)\n",
        "high_corr_pairs = []\n",
        "for i in range(len(predictors)):\n",
        "    for j in range(i+1, len(predictors)):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
        "            high_corr_pairs.append((predictors[i], predictors[j], correlation_matrix.iloc[i, j]))\n",
        "\n",
        "print(\"Highly correlated feature pairs:\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(f\"{pair[0]} and {pair[1]}: {pair[2]:.2f}\")\n",
        "\n",
        "# Function to select target variable based on available data\n",
        "def select_target_variable(df, possible_targets):\n",
        "    for target in possible_targets:\n",
        "        if target in df.columns and df[target].notna().sum() > len(df) * 0.5:\n",
        "            return target\n",
        "    return None\n",
        "\n",
        "# Select target variable based on availability\n",
        "target_priorities = [\n",
        "    'hosp_patients',              # Current hospitalized patients\n",
        "    'weekly_hosp_admissions',     # Weekly new hospital admissions\n",
        "    'icu_patients',               # Current ICU patients\n",
        "    'weekly_icu_admissions'       # Weekly new ICU admissions\n",
        "]\n",
        "\n",
        "target_variable = select_target_variable(df_filtered, target_priorities)\n",
        "print(f\"Selected target variable: {target_variable}\")\n",
        "\n",
        "# If no healthcare target variable is available, we cannot proceed with this specific prediction task\n",
        "if target_variable is None:\n",
        "    print(\"No suitable target variable found in the dataset.\")\n",
        "    # In a real scenario, we might need to obtain additional data or redefine the prediction task"
      ],
      "metadata": {
        "id": "9YIVKKH-EIvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting and Transformation\n",
        "# Preparing the data for modeling by splitting into train/validation/test sets and applying necessary transformations:"
      ],
      "metadata": {
        "id": "RGD6VPY4FUfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare feature matrix and target vector\n",
        "X = df_filtered[predictors].copy()\n",
        "if target_variable:\n",
        "    y = df_filtered[target_variable].copy()\n",
        "\n",
        "    # Fill remaining NaN values in features with appropriate methods\n",
        "    for col in X.columns:\n",
        "        # For rate columns, fill NaN with 0\n",
        "        if 'rate' in col or 'pct' in col:\n",
        "            X[col] = X[col].fillna(0)\n",
        "        # For other columns, fill with median\n",
        "        else:\n",
        "            X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "    # Replace infinite values with NaN, then handle them\n",
        "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Handle any remaining NaN values after replacing infinities\n",
        "    for col in X.columns:\n",
        "        if 'rate' in col or 'pct' in col:\n",
        "            X[col] = X[col].fillna(0)\n",
        "        else:\n",
        "            X[col] = X[col].fillna(X[col].median())\n",
        "\n",
        "    # Find rows where target is not NaN\n",
        "    valid_indices = y.notna()\n",
        "    X = X[valid_indices]\n",
        "    y = y[valid_indices]\n",
        "\n",
        "    # Handle any remaining NaN values in the target\n",
        "    y = y.fillna(y.median())\n",
        "\n",
        "    # Create country indicators for modeling\n",
        "    country_dummies = pd.get_dummies(df_filtered.loc[valid_indices, 'location'], prefix='country')\n",
        "\n",
        "    # Add country indicators to features (optional, depending on model approach)\n",
        "    # X = pd.concat([X, country_dummies], axis=1)\n",
        "\n",
        "    # Split the data into training and temporary set (80% train, 20% temp)\n",
        "    # For time series data, a chronological split is often better\n",
        "    # Here we split by time for each country\n",
        "\n",
        "    # First, create a unique identifier for each country-date combination\n",
        "    df_filtered['country_date'] = df_filtered['location'] + '_' + df_filtered['date'].astype(str)\n",
        "\n",
        "    # Get unique countries\n",
        "    countries = df_filtered['location'].unique()\n",
        "\n",
        "    # Initialize empty DataFrames for train, validation, and test sets\n",
        "    X_train = pd.DataFrame()\n",
        "    X_val = pd.DataFrame()\n",
        "    X_test = pd.DataFrame()\n",
        "    y_train = pd.Series()\n",
        "    y_val = pd.Series()\n",
        "    y_test = pd.Series()\n",
        "\n",
        "    # Split the data for each country\n",
        "    for country in countries:\n",
        "        country_mask = df_filtered['location'] == country\n",
        "        country_data = df_filtered[country_mask].sort_values('date')\n",
        "\n",
        "        if country_data[target_variable].notna().sum() > 30:  # Only use countries with sufficient data\n",
        "            # Extract features and target\n",
        "            country_X = country_data[predictors].copy()\n",
        "            country_y = country_data[target_variable].copy()\n",
        "\n",
        "            # Handle NaN values\n",
        "            for col in country_X.columns:\n",
        "                if 'rate' in col or 'pct' in col:\n",
        "                    country_X[col] = country_X[col].fillna(0)\n",
        "                else:\n",
        "                    country_X[col] = country_X[col].fillna(country_X[col].median())\n",
        "\n",
        "            # Replace infinite values with NaN, then fill\n",
        "            country_X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "            # Handle any remaining NaN values after replacing infinities\n",
        "            for col in country_X.columns:\n",
        "                if 'rate' in col or 'pct' in col:\n",
        "                    country_X[col] = country_X[col].fillna(0)\n",
        "                else:\n",
        "                    country_X[col] = country_X[col].fillna(country_X[col].median())\n",
        "\n",
        "            country_y = country_y.fillna(country_y.median())\n",
        "\n",
        "            # Find valid rows\n",
        "            valid_indices = country_y.notna()\n",
        "            country_X = country_X[valid_indices]\n",
        "            country_y = country_y[valid_indices]\n",
        "\n",
        "            # Calculate split indices\n",
        "            n = len(country_X)\n",
        "            train_idx = int(0.7 * n)\n",
        "            val_idx = int(0.85 * n)\n",
        "\n",
        "            # Split the data\n",
        "            X_train = pd.concat([X_train, country_X.iloc[:train_idx]])\n",
        "            X_val = pd.concat([X_val, country_X.iloc[train_idx:val_idx]])\n",
        "            X_test = pd.concat([X_test, country_X.iloc[val_idx:]])\n",
        "\n",
        "            y_train = pd.concat([y_train, country_y.iloc[:train_idx]])\n",
        "            y_val = pd.concat([y_val, country_y.iloc[train_idx:val_idx]])\n",
        "            y_test = pd.concat([y_test, country_y.iloc[val_idx:]])\n",
        "\n",
        "    # Add a verification step to check for infinities or extremely large values before scaling\n",
        "    def check_and_clean_dataframe(df):\n",
        "        \"\"\"Check for and handle problematic values in the dataframe\"\"\"\n",
        "        # Replace infinities with NaN\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "        # Check for and cap extremely large values (using a reasonable threshold)\n",
        "        for col in df.columns:\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            upper_bound = q3 + 10 * iqr  # Less strict than usual 1.5*IQR to keep more data\n",
        "            lower_bound = q1 - 10 * iqr\n",
        "\n",
        "            # Cap extreme values\n",
        "            df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "            # Fill any remaining NaNs\n",
        "            if 'rate' in col or 'pct' in col:\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply cleaning to all datasets\n",
        "    X_train = check_and_clean_dataframe(X_train)\n",
        "    X_val = check_and_clean_dataframe(X_val)\n",
        "    X_test = check_and_clean_dataframe(X_test)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = RobustScaler()  # RobustScaler is less sensitive to outliers\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert back to DataFrame for better handling\n",
        "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "    print(f\"Training set: {X_train_scaled.shape[0]} samples\")\n",
        "    print(f\"Validation set: {X_val_scaled.shape[0]} samples\")\n",
        "    print(f\"Test set: {X_test_scaled.shape[0]} samples\")"
      ],
      "metadata": {
        "id": "a3gsB-myGbV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prophet Model"
      ],
      "metadata": {
        "id": "GZa9mcWEUdja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import pandas as pd\n",
        "\n",
        "def train_prophet_model(df, target_column, horizon=30):\n",
        "    # Prepare data in Prophet format\n",
        "    prophet_df = df[['date', target_column]].rename(\n",
        "        columns={'date': 'ds', target_column: 'y'})\n",
        "\n",
        "    # Initialize and train model with appropriate parameters\n",
        "    model = Prophet(\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=False,\n",
        "        changepoint_prior_scale=0.05,  # Flexibility in trend changes\n",
        "        seasonality_prior_scale=10,    # Stronger seasonality component\n",
        "        holidays_prior_scale=10        # Important for medical demand\n",
        "    )\n",
        "\n",
        "    # Add country-specific holidays if available\n",
        "    if 'country' in df.columns:\n",
        "        country = df['country'].iloc[0]\n",
        "        # Add relevant holidays for this country\n",
        "        # code for adding holidays...\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(prophet_df)\n",
        "\n",
        "    # Create future dataframe for predictions\n",
        "    future = model.make_future_dataframe(periods=horizon)\n",
        "\n",
        "    # Generate forecast\n",
        "    forecast = model.predict(future)\n",
        "\n",
        "    return model, forecast"
      ],
      "metadata": {
        "id": "ZcJO1cXdUx62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_prophet_forecast(model, forecast):\n",
        "    \"\"\"Visualize Prophet forecast with components\"\"\"\n",
        "    # Main forecast plot\n",
        "    fig1 = model.plot(forecast)\n",
        "    plt.title('Prophet Forecast for Medical Resource Demand')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Resource Demand')\n",
        "    plt.show()\n",
        "\n",
        "    # Trend and seasonality components\n",
        "    fig2 = model.plot_components(forecast)\n",
        "    plt.suptitle('Prophet Forecast Components', y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "    # Interactive plot (if in notebook)\n",
        "    try:\n",
        "        from prophet.plot import plot_plotly, plot_components_plotly\n",
        "        plot_plotly(model, forecast)\n",
        "        plot_components_plotly(model, forecast)\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "i0ekAsxvdLHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost"
      ],
      "metadata": {
        "id": "9m2ulaAPaCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "def train_xgboost_model(X_train, y_train, X_test, y_test):\n",
        "    # Parameter grid optimized for medical resource prediction\n",
        "    params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'learning_rate': 0.05,\n",
        "        'max_depth': 6,\n",
        "        'min_child_weight': 2,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'gamma': 0.1,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "        'n_estimators': 500\n",
        "    }\n",
        "\n",
        "    # Initialize model\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "\n",
        "    # Time-series cross-validation to respect temporal order\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "        eval_metric=['rmse', 'mae'],\n",
        "        early_stopping_rounds=20,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Get feature importance\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    return model, importance_df, rmse, mae"
      ],
      "metadata": {
        "id": "pCkXWVEHaG0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_xgboost_results(y_test, y_pred, importance_df, model_name=\"XGBoost\"):\n",
        "    \"\"\"Visualize XGBoost predictions and feature importance\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Actual vs Predicted\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(y_test.values, label='Actual')\n",
        "    plt.plot(y_pred, label='Predicted', alpha=0.7)\n",
        "    plt.title(f'{model_name} - Actual vs Predicted')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Resource Demand')\n",
        "    plt.legend()\n",
        "\n",
        "    # Feature Importance\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.barplot(x='Importance', y='Feature',\n",
        "               data=importance_df.head(15))\n",
        "    plt.title(f'{model_name} - Top 15 Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Xptfg69wdRdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "wf9c43PvaOvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test):\n",
        "    # Initialize model with parameters suited for medical data\n",
        "    rf_model = RandomForestRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=15,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        max_features='sqrt',\n",
        "        bootstrap=True,\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Extract feature importance\n",
        "    feature_importance = rf_model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    rf_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance\n",
        "    }).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    return rf_model, rf_importance, rmse, mae"
      ],
      "metadata": {
        "id": "z77yAoAhac0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_random_forest(y_test, y_pred, importance_df):\n",
        "    \"\"\"Visualize Random Forest results\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Actual vs Predicted\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "    plt.plot([y_test.min(), y_test.max()],\n",
        "             [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.title('Random Forest - Actual vs Predicted')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "\n",
        "    # Feature Importance\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.barplot(x='Importance', y='Feature',\n",
        "               data=importance_df.head(15))\n",
        "    plt.title('Random Forest - Top 15 Feature Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uEbqjG0rddph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM (Long Short-Term Memory Networks)"
      ],
      "metadata": {
        "id": "fpzMa3-tahBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def prepare_lstm_data(data, target_col, lookback=14):\n",
        "    \"\"\"Transform data into sequences suitable for LSTM\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - lookback):\n",
        "        X.append(data[i:(i + lookback)])\n",
        "        y.append(data[i + lookback, target_col])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def build_lstm_model(X_train):\n",
        "    \"\"\"Build and compile an LSTM model for medical resource prediction\"\"\"\n",
        "    # Input shape: [samples, time steps, features]\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    model = Sequential([\n",
        "        # First LSTM layer with return sequences for stacking\n",
        "        LSTM(64, activation='relu', return_sequences=True,\n",
        "             input_shape=input_shape),\n",
        "        Dropout(0.2),  # Prevent overfitting\n",
        "\n",
        "        # Second LSTM layer\n",
        "        LSTM(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(1)  # Single output for resource demand prediction\n",
        "    ])\n",
        "\n",
        "    # Compile model with appropriate loss function\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
        "    \"\"\"Train LSTM model with early stopping\"\"\"\n",
        "    # Build model\n",
        "    model = build_lstm_model(X_train)\n",
        "\n",
        "    # Early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "2okqubSNat_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_lstm_results(history, y_test, y_pred):\n",
        "    \"\"\"Visualize LSTM training history and predictions\"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Training history\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('LSTM Training History')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Predictions vs Actual\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(y_test, label='Actual')\n",
        "    plt.plot(y_pred, label='Predicted', alpha=0.7)\n",
        "    plt.title('LSTM - Actual vs Predicted')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Resource Demand')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "I1C99nb1dh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIMHG5BBdoYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Approach for Improved Robustness"
      ],
      "metadata": {
        "id": "H7VQoeZBa_U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ensemble_prediction(prophet_pred, xgb_pred, rf_pred, lstm_pred, weights=None):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple models with optional weighting\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        # Equal weighting\n",
        "        weights = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "    # Ensure weights sum to 1\n",
        "    weights = np.array(weights) / sum(weights)\n",
        "\n",
        "    # Combine predictions\n",
        "    ensemble_pred = (\n",
        "        weights[0] * prophet_pred +\n",
        "        weights[1] * xgb_pred +\n",
        "        weights[2] * rf_pred +\n",
        "        weights[3] * lstm_pred\n",
        "    )\n",
        "\n",
        "    return ensemble_pred"
      ],
      "metadata": {
        "id": "FViqn9SkbGle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_ensemble_results(y_test, ensemble_pred, individual_preds):\n",
        "    \"\"\"Visualize ensemble vs individual model performance\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot all predictions\n",
        "    plt.plot(y_test.values, label='Actual', linewidth=2)\n",
        "    plt.plot(ensemble_pred, label='Ensemble', linewidth=2)\n",
        "\n",
        "    # Plot individual model predictions\n",
        "    for model_name, pred in individual_preds.items():\n",
        "        plt.plot(pred, label=model_name, alpha=0.5, linestyle='--')\n",
        "\n",
        "    plt.title('Ensemble vs Individual Model Predictions')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Resource Demand')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OfTkUXWIduKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Selection\n",
        "# To select the best model for our healthcare resource prediction task:"
      ],
      "metadata": {
        "id": "6eRmW1MrbJRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_models(models, X_test, y_test, target_cols):\n",
        "    \"\"\"Evaluate multiple models on test data\"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "        # Domain-specific metrics for healthcare\n",
        "        # Underprediction is more costly than overprediction\n",
        "        under_pred = np.mean((y_test - y_pred)[y_test > y_pred])\n",
        "        over_pred = np.mean((y_pred - y_test)[y_pred > y_test])\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            'RMSE': rmse,\n",
        "            'MAE': mae,\n",
        "            'Under-prediction Error': under_pred,\n",
        "            'Over-prediction Error': over_pred\n",
        "        }\n",
        "\n",
        "    return pd.DataFrame(results).T"
      ],
      "metadata": {
        "id": "nX4qfz4ubUvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Interpretability\n",
        "# For healthcare applications, model interpretability is crucial:"
      ],
      "metadata": {
        "id": "fFyaYjbVbg9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "def interpret_xgboost_model(model, X_test):\n",
        "    \"\"\"Generate SHAP values for XGBoost model interpretation\"\"\"\n",
        "    # Initialize SHAP explainer\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "\n",
        "    # Calculate SHAP values\n",
        "    shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "    # Create summary plot\n",
        "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "\n",
        "    # Return values for detailed analysis\n",
        "    return explainer, shap_values"
      ],
      "metadata": {
        "id": "0cmtoQvBbvPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kZ8eYWgrbzPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ERROR METRICS VISUALIZATION"
      ],
      "metadata": {
        "id": "8qQerVAwd372"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_error_metrics(metrics_df):\n",
        "    \"\"\"Visualize model comparison metrics\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # RMSE and MAE\n",
        "    plt.subplot(1, 2, 1)\n",
        "    metrics_df[['RMSE', 'MAE']].plot(kind='bar')\n",
        "    plt.title('Model Error Comparison')\n",
        "    plt.ylabel('Error Value')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Under/Over prediction\n",
        "    plt.subplot(1, 2, 2)\n",
        "    metrics_df[['Under-prediction Error', 'Over-prediction Error']].plot(kind='bar')\n",
        "    plt.title('Under/Over Prediction Comparison')\n",
        "    plt.ylabel('Error Value')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "cOM-gJyRd-wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHAP Value Visualization (XGBoost Interpretability)"
      ],
      "metadata": {
        "id": "xcxW07rueDPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_shap_values(explainer, shap_values, X_test):\n",
        "    \"\"\"Visualize SHAP values for model interpretability\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Summary plot\n",
        "    plt.subplot(2, 2, 1)\n",
        "    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "    plt.title('Feature Importance (SHAP)')\n",
        "\n",
        "    # Detailed SHAP values\n",
        "    plt.subplot(2, 2, 2)\n",
        "    shap.summary_plot(shap_values, X_test)\n",
        "    plt.title('SHAP Value Distribution')\n",
        "\n",
        "    # Dependence plot for top feature\n",
        "    top_feature = X_test.columns[np.argmax(np.abs(shap_values).mean(0))]\n",
        "    plt.subplot(2, 2, 3)\n",
        "    shap.dependence_plot(top_feature, shap_values, X_test)\n",
        "    plt.title(f'Dependence Plot for {top_feature}')\n",
        "\n",
        "    # Force plot for single prediction\n",
        "    plt.subplot(2, 2, 4)\n",
        "    shap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])\n",
        "    plt.title('Force Plot for Single Prediction')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4_MyVNtdeIHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from prophet import Prophet\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Assuming you have your data loaded as df\n",
        "# Let's create a sample target variable for demonstration\n",
        "df['resource_demand'] = df['hosp_patients'].fillna(0)  # Using hospital patients as target\n",
        "\n",
        "# Split data into train and test (time-series split)\n",
        "train_size = int(len(df) * 0.8)\n",
        "train = df.iloc[:train_size]\n",
        "test = df.iloc[train_size:]\n",
        "\n",
        "# 1. Train Prophet Model\n",
        "def train_prophet_model(train_df, target_col='resource_demand'):\n",
        "    prophet_df = train_df[['date', target_col]].rename(columns={'date': 'ds', target_col: 'y'})\n",
        "    model = Prophet(yearly_seasonality=True, weekly_seasonality=True)\n",
        "    model.fit(prophet_df)\n",
        "    future = model.make_future_dataframe(periods=len(test))\n",
        "    forecast = model.predict(future)\n",
        "    return model, forecast\n",
        "\n",
        "prophet_model, prophet_forecast = train_prophet_model(train)\n",
        "\n",
        "# 2. Train XGBoost Model\n",
        "def train_xgboost_model(train_df, test_df, features, target_col='resource_demand'):\n",
        "    model = xgb.XGBRegressor()\n",
        "    model.fit(train_df[features], train_df[target_col])\n",
        "    preds = model.predict(test_df[features])\n",
        "    return model, preds\n",
        "\n",
        "# Select features - you should customize this based on your actual features\n",
        "features = ['total_cases', 'new_cases', 'population']\n",
        "xgb_model, xgb_preds = train_xgboost_model(train, test, features)\n",
        "\n",
        "# 3. Train Random Forest Model\n",
        "def train_random_forest(train_df, test_df, features, target_col='resource_demand'):\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(train_df[features], train_df[target_col])\n",
        "    preds = model.predict(test_df[features])\n",
        "    return model, preds\n",
        "\n",
        "rf_model, rf_preds = train_random_forest(train, test, features)\n",
        "\n",
        "# Now you can visualize\n",
        "def visualize_prophet_forecast(model, forecast, actual_data):\n",
        "    fig = model.plot(forecast)\n",
        "    plt.plot(actual_data['date'], actual_data['resource_demand'], 'r.', label='Actual')\n",
        "    plt.title('Prophet Forecast vs Actual')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_xgboost_results(y_true, y_pred):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(y_true, label='Actual')\n",
        "    plt.plot(y_pred, label='XGBoost Predicted', alpha=0.7)\n",
        "    plt.title('XGBoost Predictions')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_rf_results(y_true, y_pred):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.3)\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "    plt.title('Random Forest Predictions')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "# Now call the visualization functions with your trained models\n",
        "visualize_prophet_forecast(prophet_model, prophet_forecast, test)\n",
        "visualize_xgboost_results(test['resource_demand'], xgb_preds)\n",
        "visualize_rf_results(test['resource_demand'], rf_preds)"
      ],
      "metadata": {
        "id": "x7_vt2sJewdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xAj96dZZeZiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dyOhRtojU7PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to examine dataset before and after cleaning\n",
        "def examine_dataset_cleaning(df_original, column_subset=None):\n",
        "    \"\"\"\n",
        "    Examine a dataset before and after cleaning to identify problematic values.\n",
        "\n",
        "    Args:\n",
        "        df_original: The original dataframe to be examined\n",
        "        column_subset: Optional list of specific columns to examine (None for all columns)\n",
        "\n",
        "    Returns:\n",
        "        df_cleaned: The cleaned dataframe\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df = df_original.copy()\n",
        "\n",
        "    # Select columns to examine (all or subset)\n",
        "    if column_subset is not None and all(col in df.columns for col in column_subset):\n",
        "        examine_cols = column_subset\n",
        "    else:\n",
        "        examine_cols = df.columns\n",
        "\n",
        "    # Check dimensions\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # 1. Check for missing values\n",
        "    missing_vals = df[examine_cols].isna().sum()\n",
        "    print(\"\\nMissing values before cleaning:\")\n",
        "    print(missing_vals[missing_vals > 0])\n",
        "\n",
        "    # 2. Check for infinities\n",
        "    inf_counts = {}\n",
        "    for col in examine_cols:\n",
        "        inf_count = np.isinf(df[col]).sum()\n",
        "        if inf_count > 0:\n",
        "            inf_counts[col] = inf_count\n",
        "\n",
        "    if inf_counts:\n",
        "        print(\"\\nColumns with infinity values:\")\n",
        "        for col, count in inf_counts.items():\n",
        "            print(f\"{col}: {count} infinity values\")\n",
        "    else:\n",
        "        print(\"\\nNo infinity values found.\")\n",
        "\n",
        "    # 3. Identify extremely large values (potential outliers)\n",
        "    print(\"\\nExtreme values before cleaning:\")\n",
        "    for col in examine_cols:\n",
        "        if df[col].dtype.kind in 'ifc':  # Only check numeric columns\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            upper_bound = q3 + 5 * iqr\n",
        "            lower_bound = q1 - 5 * iqr\n",
        "\n",
        "            extreme_high = df[df[col] > upper_bound][col]\n",
        "            extreme_low = df[df[col] < lower_bound][col]\n",
        "\n",
        "            if len(extreme_high) > 0 or len(extreme_low) > 0:\n",
        "                print(f\"\\n{col}:\")\n",
        "                print(f\"  Normal range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
        "                if len(extreme_high) > 0:\n",
        "                    print(f\"  High outliers: {len(extreme_high)} values\")\n",
        "                    print(f\"  Max value: {extreme_high.max()}\")\n",
        "                if len(extreme_low) > 0:\n",
        "                    print(f\"  Low outliers: {len(extreme_low)} values\")\n",
        "                    print(f\"  Min value: {extreme_low.min()}\")\n",
        "\n",
        "    # Clean the dataset\n",
        "    print(\"\\n--- Cleaning Dataset ---\")\n",
        "    df_cleaned = clean_dataset(df, examine_cols)\n",
        "\n",
        "    # Check dimensions after cleaning\n",
        "    print(f\"\\nCleaned dataset shape: {df_cleaned.shape}\")\n",
        "\n",
        "    # Check for missing values after cleaning\n",
        "    missing_vals_after = df_cleaned[examine_cols].isna().sum()\n",
        "    if missing_vals_after.sum() > 0:\n",
        "        print(\"\\nRemaining missing values after cleaning:\")\n",
        "        print(missing_vals_after[missing_vals_after > 0])\n",
        "    else:\n",
        "        print(\"\\nNo missing values after cleaning.\")\n",
        "\n",
        "    # Check for infinities after cleaning\n",
        "    has_inf_after = False\n",
        "    for col in examine_cols:\n",
        "        if np.isinf(df_cleaned[col]).sum() > 0:\n",
        "            has_inf_after = True\n",
        "            break\n",
        "\n",
        "    if has_inf_after:\n",
        "        print(\"\\nWarning: Infinities still present after cleaning!\")\n",
        "    else:\n",
        "        print(\"\\nNo infinity values after cleaning.\")\n",
        "\n",
        "    # Visualize data distribution before and after cleaning for a sample of columns\n",
        "    visualize_distributions(df, df_cleaned, examine_cols)\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "def clean_dataset(df, columns):\n",
        "    \"\"\"\n",
        "    Clean the dataset by handling missing values, infinities, and extreme values.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Replace infinities with NaN\n",
        "    df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Handle extreme values and remaining NaNs\n",
        "    for col in columns:\n",
        "        if df_clean[col].dtype.kind in 'ifc':  # Only process numeric columns\n",
        "            # Calculate bounds\n",
        "            q1 = df_clean[col].dropna().quantile(0.25)\n",
        "            q3 = df_clean[col].dropna().quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            upper_bound = q3 + 10 * iqr  # Using a generous threshold\n",
        "            lower_bound = q1 - 10 * iqr\n",
        "\n",
        "            # Cap extreme values\n",
        "            df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "            # Fill remaining NaNs\n",
        "            if 'rate' in col or 'pct' in col:\n",
        "                df_clean[col] = df_clean[col].fillna(0)\n",
        "            else:\n",
        "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "def visualize_distributions(df_original, df_cleaned, columns, max_cols=5):\n",
        "    \"\"\"\n",
        "    Create histograms to compare distributions before and after cleaning.\n",
        "    \"\"\"\n",
        "    # Select a subset of columns if there are too many\n",
        "    if len(columns) > max_cols:\n",
        "        # Prioritize columns with known issues for visualization\n",
        "        cols_to_plot = []\n",
        "\n",
        "        # First, add columns with infinities\n",
        "        for col in columns:\n",
        "            if np.isinf(df_original[col]).sum() > 0 and len(cols_to_plot) < max_cols:\n",
        "                cols_to_plot.append(col)\n",
        "\n",
        "        # Then add columns with extreme values\n",
        "        for col in columns:\n",
        "            if col not in cols_to_plot and df_original[col].dtype.kind in 'ifc':\n",
        "                q1 = df_original[col].quantile(0.25)\n",
        "                q3 = df_original[col].quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                upper_bound = q3 + 5 * iqr\n",
        "                lower_bound = q1 - 5 * iqr\n",
        "\n",
        "                extreme_vals = ((df_original[col] > upper_bound) | (df_original[col] < lower_bound)).sum()\n",
        "                if extreme_vals > 0 and len(cols_to_plot) < max_cols:\n",
        "                    cols_to_plot.append(col)\n",
        "\n",
        "        # Fill remaining slots with random columns\n",
        "        remaining_cols = [c for c in columns if c not in cols_to_plot and df_original[c].dtype.kind in 'ifc']\n",
        "        if remaining_cols and len(cols_to_plot) < max_cols:\n",
        "            np.random.seed(42)  # For reproducibility\n",
        "            sample_cols = np.random.choice(remaining_cols,\n",
        "                                          min(max_cols - len(cols_to_plot), len(remaining_cols)),\n",
        "                                          replace=False)\n",
        "            cols_to_plot.extend(sample_cols)\n",
        "    else:\n",
        "        cols_to_plot = [c for c in columns if df_original[c].dtype.kind in 'ifc'][:max_cols]\n",
        "\n",
        "    if not cols_to_plot:\n",
        "        print(\"No suitable numeric columns for visualization.\")\n",
        "        return\n",
        "\n",
        "    # Create plots\n",
        "    fig, axes = plt.subplots(len(cols_to_plot), 2, figsize=(12, 4 * len(cols_to_plot)))\n",
        "\n",
        "    for i, col in enumerate(cols_to_plot):\n",
        "        # For single column case\n",
        "        if len(cols_to_plot) == 1:\n",
        "            ax1, ax2 = axes\n",
        "        else:\n",
        "            ax1, ax2 = axes[i]\n",
        "\n",
        "        # Before cleaning - drop infinities for visualization\n",
        "        before_data = df_original[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "        if len(before_data) > 0:\n",
        "            sns.histplot(before_data, ax=ax1, bins=30, kde=True)\n",
        "            ax1.set_title(f'Before Cleaning: {col}')\n",
        "            ax1.set_ylabel('Count')\n",
        "\n",
        "            # Add text with statistics\n",
        "            stats_text = f\"Mean: {before_data.mean():.2f}\\n\"\n",
        "            stats_text += f\"Std: {before_data.std():.2f}\\n\"\n",
        "            stats_text += f\"Min: {before_data.min():.2f}\\n\"\n",
        "            stats_text += f\"Max: {before_data.max():.2f}\"\n",
        "            ax1.text(0.05, 0.95, stats_text, transform=ax1.transAxes,\n",
        "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.5))\n",
        "        else:\n",
        "            ax1.text(0.5, 0.5, 'No valid data before cleaning',\n",
        "                    ha='center', va='center', transform=ax1.transAxes)\n",
        "\n",
        "        # After cleaning\n",
        "        after_data = df_cleaned[col].dropna()\n",
        "        if len(after_data) > 0:\n",
        "            sns.histplot(after_data, ax=ax2, bins=30, kde=True)\n",
        "            ax2.set_title(f'After Cleaning: {col}')\n",
        "\n",
        "            # Add text with statistics\n",
        "            stats_text = f\"Mean: {after_data.mean():.2f}\\n\"\n",
        "            stats_text += f\"Std: {after_data.std():.2f}\\n\"\n",
        "            stats_text += f\"Min: {after_data.min():.2f}\\n\"\n",
        "            stats_text += f\"Max: {after_data.max():.2f}\"\n",
        "            ax2.text(0.05, 0.95, stats_text, transform=ax2.transAxes,\n",
        "                    verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.5))\n",
        "        else:\n",
        "            ax2.text(0.5, 0.5, 'No valid data after cleaning',\n",
        "                    ha='center', va='center', transform=ax2.transAxes)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return\n",
        "\n",
        "# Usage example:\n",
        "# Assuming X_train is your dataset before cleaning\n",
        "# Replace 'column_names' with actual column names if you want to focus on specific columns\n",
        "\n",
        "# To analyze the entire dataset:\n",
        "# cleaned_data = examine_dataset_cleaning(X_train)\n",
        "\n",
        "# To analyze specific columns:\n",
        "# cleaned_data = examine_dataset_cleaning(X_train, column_subset=['feature1', 'feature2'])\n",
        "\n",
        "# Usage with your datasets:\n",
        "# Before applying cleaning (using X_train from your original code)\n",
        "print(\"Examining training data:\")\n",
        "cleaned_X_train = examine_dataset_cleaning(X_train)\n",
        "\n",
        "# You can do the same for validation and test sets if needed\n",
        "# print(\"\\nExamining validation data:\")\n",
        "# cleaned_X_val = examine_dataset_cleaning(X_val)\n",
        "#\n",
        "# print(\"\\nExamining test data:\")\n",
        "# cleaned_X_test = examine_dataset_cleaning(X_test)"
      ],
      "metadata": {
        "id": "T6Xfd7raOI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Algorithms"
      ],
      "metadata": {
        "id": "IFSN6K0IY122"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Model"
      ],
      "metadata": {
        "id": "SK7SqoD7aC0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': 300,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Hospital bed demand model\n",
        "xgb_hosp = XGBRegressor(**xgb_params)\n",
        "xgb_hosp.fit(X_train, y_train_hosp)\n",
        "xgb_hosp_pred = xgb_hosp.predict(X_test)\n",
        "\n",
        "# ICU demand model\n",
        "xgb_icu = XGBRegressor(**xgb_params)\n",
        "xgb_icu.fit(X_train, y_train_icu)\n",
        "xgb_icu_pred = xgb_icu.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"XGBoost Hospital Bed Demand Prediction:\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test_hosp, xgb_hosp_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_hosp, xgb_hosp_pred)):.2f}\")\n",
        "\n",
        "print(\"\\nXGBoost ICU Demand Prediction:\")\n",
        "print(f\"MAE: {mean_absolute_error(y_test_icu, xgb_icu_pred):.2f}\")\n",
        "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_icu, xgb_icu_pred)):.2f}\")"
      ],
      "metadata": {
        "id": "Jl7RFznDZMF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning:"
      ],
      "metadata": {
        "id": "Dno9b1TxbRvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [4, 6, 8],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [100, 300, 500]\n",
        "}\n",
        "grid_search = GridSearchCV(XGBRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train, y_train_icu)\n",
        "print(\"Best parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "gWWIOkWmbEIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prophet Model"
      ],
      "metadata": {
        "id": "MbkD0bPBcMhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "# Prepare data for Prophet\n",
        "prophet_df = df[df['location'] == 'United States'][['date', 'hosp_patients_per_million']].dropna()\n",
        "prophet_df.columns = ['ds', 'y']\n",
        "\n",
        "# Fit model\n",
        "prophet_model = Prophet(seasonality_mode='multiplicative',\n",
        "                       yearly_seasonality=True,\n",
        "                       weekly_seasonality=True)\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Make future dataframe\n",
        "future = prophet_model.make_future_dataframe(periods=14)\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Plot results\n",
        "fig = prophet_model.plot(forecast)\n",
        "plt.title('Prophet Forecast for Hospital Bed Demand')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YunGqZ3Vb6Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Prepare data for Prophet (US hospitalizations)\n",
        "prophet_df = df[df['location'] == 'United States'][['date', 'hosp_patients_per_million']].dropna()\n",
        "prophet_df.columns = ['ds', 'y']\n",
        "\n",
        "# Fit model with tuned parameters\n",
        "prophet_model = Prophet(\n",
        "    seasonality_mode='multiplicative',\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    changepoint_prior_scale=0.05,  # Adjusts trend flexibility\n",
        "    interval_width=0.95            # 95% confidence intervals\n",
        ")\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Generate 14-day forecast\n",
        "future = prophet_model.make_future_dataframe(periods=14)\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# Plot with professional formatting\n",
        "fig = prophet_model.plot(forecast, xlabel='Date', ylabel='Hospitalized Patients per Million')\n",
        "plt.title('Prophet Forecast: U.S. COVID-19 Hospital Bed Demand (2020-2024)', fontsize=14, pad=20)\n",
        "\n",
        "# Enhance plot aesthetics\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor('#f5f5f5')  # Light gray background\n",
        "fig.patch.set_facecolor('white')  # White figure background\n",
        "\n",
        "# Highlight key pandemic phases\n",
        "plt.axvspan('2021-06-01', '2021-10-01', color='red', alpha=0.1, label='Delta Variant Dominance')\n",
        "plt.axvspan('2021-12-01', '2022-03-01', color='purple', alpha=0.1, label='Omicron Wave')\n",
        "\n",
        "plt.legend(loc='upper left', frameon=True)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9FlOhl3Cd-ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N4NPj_zOaOOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# =============================================\n",
        "# 1. DATA PREPARATION WITH EXTERNAL REGRESSORS\n",
        "# =============================================\n",
        "\n",
        "# Load and prepare US data with vaccination rates\n",
        "us_data = df[df['location'] == 'United States'][[\n",
        "    'date',\n",
        "    'hosp_patients_per_million',\n",
        "    'people_fully_vaccinated_per_hundred',\n",
        "    'stringency_index'\n",
        "]].dropna().copy()\n",
        "us_data.columns = ['ds', 'y', 'vax_rate', 'policy_strictness']\n",
        "\n",
        "# Create 7-day rolling features\n",
        "us_data['case_growth'] = us_data['y'].pct_change(7)\n",
        "us_data['vax_impact'] = us_data['vax_rate'] * us_data['policy_strictness'] / 100\n",
        "\n",
        "# =============================================\n",
        "# 2. ENHANCED PROPHET MODEL\n",
        "# =============================================\n",
        "\n",
        "# Initialize model with custom configurations\n",
        "prophet_model = Prophet(\n",
        "    seasonality_mode='multiplicative',\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    changepoint_prior_scale=0.05,\n",
        "    interval_width=0.95\n",
        ")\n",
        "\n",
        "# Add external regressors\n",
        "prophet_model.add_regressor('vax_rate')\n",
        "prophet_model.add_regressor('policy_strictness')\n",
        "prophet_model.add_regressor('vax_impact')\n",
        "\n",
        "# Add custom monthly seasonality\n",
        "prophet_model.add_seasonality(\n",
        "    name='monthly',\n",
        "    period=30.5,\n",
        "    fourier_order=5\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "prophet_model.fit(us_data)\n",
        "\n",
        "# Create future dataframe with regressors\n",
        "future = prophet_model.make_future_dataframe(periods=14)\n",
        "\n",
        "# Propagate regressor values (using last known values for future)\n",
        "for col in ['vax_rate', 'policy_strictness', 'vax_impact']:\n",
        "    future[col] = us_data[col].iloc[-1]\n",
        "\n",
        "# Generate forecast\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "# =============================================\n",
        "# 3. COMPARISON WITH XGBOOST\n",
        "# =============================================\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "X = us_data[['vax_rate', 'policy_strictness', 'case_growth']]\n",
        "y = us_data['y']\n",
        "\n",
        "# Temporal train-test split (last 14 days for test)\n",
        "X_train, X_test = X.iloc[:-14], X.iloc[-14:]\n",
        "y_train, y_test = y.iloc[:-14], y.iloc[-14:]\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.1\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# =============================================\n",
        "# 4. VISUALIZATION\n",
        "# =============================================\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Prophet forecast\n",
        "plt.plot(us_data['ds'], us_data['y'], 'k.', label='Observed Data')\n",
        "plt.plot(forecast['ds'], forecast['yhat'], 'b-', linewidth=2, label='Prophet Forecast')\n",
        "plt.fill_between(\n",
        "    forecast['ds'],\n",
        "    forecast['yhat_lower'],\n",
        "    forecast['yhat_upper'],\n",
        "    color='blue',\n",
        "    alpha=0.1,\n",
        "    label='95% CI'\n",
        ")\n",
        "\n",
        "# XGBoost predictions\n",
        "plt.plot(X_test.index, xgb_pred, 'r--', linewidth=2, label='XGBoost Prediction')\n",
        "\n",
        "# Formatting\n",
        "plt.title('Comparative Forecast: Prophet vs XGBoost\\nUS Hospital Bed Demand (Patients per Million)',\n",
        "          fontsize=14, pad=20)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.ylabel('Hospitalized Patients per Million', fontsize=12)\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Annotate key events\n",
        "plt.axvspan(pd.to_datetime('2021-06-01'), pd.to_datetime('2021-10-01'),\n",
        "            color='red', alpha=0.1, label='Delta Variant')\n",
        "plt.axvspan(pd.to_datetime('2021-12-01'), pd.to_datetime('2022-02-01'),\n",
        "            color='purple', alpha=0.1, label='Omicron Wave')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# =============================================\n",
        "# 5. MODEL EVALUATION\n",
        "# =============================================\n",
        "\n",
        "# Prophet evaluation on test period\n",
        "prophet_test_pred = forecast.iloc[-14:]['yhat']\n",
        "prophet_mae = mean_absolute_error(y_test, prophet_test_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nModel Performance (Last 14 Days):\")\n",
        "print(f\"Prophet MAE: {prophet_mae:.2f} patients/million\")\n",
        "print(f\"XGBoost MAE: {mean_absolute_error(y_test, xgb_pred):.2f} patients/million\")\n",
        "print(f\"Baseline (Last Value) MAE: {mean_absolute_error(y_test, [y_train.iloc[-1]]*14):.2f} patients/million\")\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\nXGBoost Feature Importance:\")\n",
        "for feat, imp in zip(X.columns, xgb_model.feature_importances_):\n",
        "    print(f\"{feat}: {imp:.3f}\")"
      ],
      "metadata": {
        "id": "3kv1_D9tfH0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from prophet import Prophet\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "## =============================================\n",
        "## 1. DATA PREPARATION\n",
        "## =============================================\n",
        "\n",
        "# Load and prepare US data\n",
        "us_data = df[df['location'] == 'United States'][[\n",
        "    'date', 'hosp_patients_per_million',\n",
        "    'people_fully_vaccinated_per_hundred',\n",
        "    'stringency_index',\n",
        "    'new_cases_per_million'\n",
        "]].dropna().copy()\n",
        "\n",
        "# Create lagged features with forward filling\n",
        "us_data['cases_lagged_7'] = us_data['new_cases_per_million'].shift(7).ffill()\n",
        "us_data['cases_lagged_14'] = us_data['new_cases_per_million'].shift(14).ffill()\n",
        "us_data['hosp_lagged_7'] = us_data['hosp_patients_per_million'].shift(7).ffill()\n",
        "\n",
        "# Calculate moving averages (minimum 1 period)\n",
        "us_data['cases_7day_avg'] = us_data['new_cases_per_million'].rolling(7, min_periods=1).mean()\n",
        "us_data['vax_policy_interaction'] = (us_data['people_fully_vaccinated_per_hundred'] *\n",
        "                                   us_data['stringency_index'] / 100)\n",
        "\n",
        "# Ensure no NaN values remain\n",
        "us_data = us_data.dropna()\n",
        "\n",
        "## =============================================\n",
        "## 2. PROPHET MODEL\n",
        "## =============================================\n",
        "\n",
        "# Initialize and configure Prophet\n",
        "prophet_model = Prophet(\n",
        "    seasonality_mode='multiplicative',\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    changepoint_prior_scale=0.03,\n",
        "    interval_width=0.90\n",
        ")\n",
        "\n",
        "# Add regressors\n",
        "for col in ['people_fully_vaccinated_per_hundred', 'stringency_index',\n",
        "           'vax_policy_interaction', 'cases_7day_avg']:\n",
        "    prophet_model.add_regressor(col)\n",
        "\n",
        "# Prepare Prophet dataframe\n",
        "prophet_df = us_data.rename(columns={'date': 'ds', 'hosp_patients_per_million': 'y'})\n",
        "prophet_df = prophet_df[['ds', 'y'] +\n",
        "                       ['people_fully_vaccinated_per_hundred', 'stringency_index',\n",
        "                        'vax_policy_interaction', 'cases_7day_avg']]\n",
        "\n",
        "# Fit model\n",
        "prophet_model.fit(prophet_df)\n",
        "\n",
        "# Create future dataframe\n",
        "future = prophet_model.make_future_dataframe(periods=14)\n",
        "for col in ['people_fully_vaccinated_per_hundred', 'stringency_index',\n",
        "           'vax_policy_interaction', 'cases_7day_avg']:\n",
        "    future[col] = us_data[col].iloc[-1]\n",
        "\n",
        "# Generate forecast\n",
        "forecast = prophet_model.predict(future)\n",
        "\n",
        "## =============================================\n",
        "## 3. XGBOOST MODEL\n",
        "## =============================================\n",
        "\n",
        "# Prepare features\n",
        "features = ['people_fully_vaccinated_per_hundred', 'stringency_index',\n",
        "            'cases_lagged_7', 'cases_lagged_14', 'hosp_lagged_7']\n",
        "X = us_data[features]\n",
        "y = us_data['hosp_patients_per_million']\n",
        "\n",
        "# Temporal train-test split\n",
        "test_size = 14\n",
        "X_train, X_test = X.iloc[:-test_size], X.iloc[-test_size:]\n",
        "y_train, y_test = y.iloc[:-test_size], y.iloc[-test_size:]\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    n_estimators=300,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "\n",
        "## =============================================\n",
        "## 4. VISUALIZATION (WITH PROPER STYLING)\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Use available matplotlib style\n",
        "plt.style.use('ggplot')  # Alternative: 'seaborn', 'fivethirtyeight', 'bmh'\n",
        "\n",
        "# Plot observed data\n",
        "plt.plot(us_data['date'], us_data['hosp_patients_per_million'],\n",
        "         'o', markersize=4, color='#333333', alpha=0.6, label='Observed Data')\n",
        "\n",
        "# Plot Prophet forecast\n",
        "plt.plot(forecast['ds'], forecast['yhat'],\n",
        "         color='#1f77b4', linewidth=2.5, label='Prophet Forecast')\n",
        "plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'],\n",
        "                 color='#1f77b4', alpha=0.15, label='90% CI')\n",
        "\n",
        "# Plot XGBoost predictions\n",
        "plt.plot(X_test.index, xgb_pred,\n",
        "         '--', color='#d62728', linewidth=2.5, label='XGBoost Prediction')\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Hospitalized Patients per Million', fontsize=12)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.title('Comparative Forecast of US Hospital Bed Demand',\n",
        "          fontsize=14, pad=20)\n",
        "\n",
        "# Add performance annotations\n",
        "plt.annotate(f'Prophet MAE: {mean_absolute_error(y_test, forecast.iloc[-test_size:][\"yhat\"]):.1f}',\n",
        "             xy=(0.72, 0.15), xycoords='axes fraction', color='#1f77b4')\n",
        "plt.annotate(f'XGBoost MAE: {mean_absolute_error(y_test, xgb_pred):.1f}',\n",
        "             xy=(0.72, 0.10), xycoords='axes fraction', color='#d62728')\n",
        "\n",
        "# Final touches\n",
        "plt.legend(loc='upper left', frameon=True, facecolor='white')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 5. MODEL EVALUATION\n",
        "## =============================================\n",
        "\n",
        "print(\"\\nModel Performance Comparison (Last 14 Days):\")\n",
        "print(f\"Prophet MAE: {mean_absolute_error(y_test, forecast.iloc[-test_size:]['yhat']):.2f}\")\n",
        "print(f\"XGBoost MAE: {mean_absolute_error(y_test, xgb_pred):.2f}\")\n",
        "\n",
        "print(\"\\nXGBoost Feature Importance:\")\n",
        "for feat, imp in zip(features, xgb_model.feature_importances_):\n",
        "    print(f\"{feat}: {imp:.3f}\")"
      ],
      "metadata": {
        "id": "kquhHKRhhmq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MNoMeLoudcKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prophet Implementation and Evaluation\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot components\n",
        "fig = prophet_model.plot_components(forecast)\n",
        "plt.suptitle('Prophet Model Components Analysis', y=1.02)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Forecast plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(us_data['date'], us_data['hosp_patients_per_million'], 'o', color='#2c3e50', label='Observed')\n",
        "plt.plot(forecast['ds'], forecast['yhat'], color='#3498db', label='Prophet Forecast')\n",
        "plt.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'],\n",
        "                 color='#3498db', alpha=0.1)\n",
        "plt.title('Prophet Forecast with Uncertainty', pad=20)\n",
        "plt.ylabel('Patients per Million')\n",
        "plt.xlabel('Date')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Prophet MAE: {mean_absolute_error(y_test, forecast.iloc[-test_size:]['yhat']):.2f}\")"
      ],
      "metadata": {
        "id": "Hs7GZ3s3j3Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM MODEL"
      ],
      "metadata": {
        "id": "N7bgKYV_k3A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "## =============================================\n",
        "## 1. DATA PREPARATION\n",
        "## =============================================\n",
        "\n",
        "# Prepare data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(us_data[['hosp_patients_per_million']])\n",
        "\n",
        "# Create sequences with date tracking\n",
        "def create_sequences(data, dates, n_steps):\n",
        "    X, y, y_dates = [], [], []\n",
        "    for i in range(len(data)-n_steps):\n",
        "        X.append(data[i:i+n_steps])\n",
        "        y.append(data[i+n_steps])\n",
        "        y_dates.append(dates.iloc[i+n_steps])  # Capture corresponding dates\n",
        "    return np.array(X), np.array(y), np.array(y_dates)\n",
        "\n",
        "n_steps = 14\n",
        "X, y, y_dates = create_sequences(scaled_data, us_data['date'], n_steps)\n",
        "\n",
        "# Split data\n",
        "test_size = 14\n",
        "X_train, X_test = X[:-test_size], X[-test_size:]\n",
        "y_train, y_test = y[:-test_size], y[-test_size:]\n",
        "test_dates = y_dates[-test_size:]  # Keep test dates for plotting\n",
        "\n",
        "## =============================================\n",
        "## 2. LSTM MODEL IMPLEMENTATION\n",
        "## =============================================\n",
        "\n",
        "# Build model with proper input layer\n",
        "model = Sequential([\n",
        "    Input(shape=(n_steps, 1)),  # Proper input specification\n",
        "    LSTM(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mae')\n",
        "\n",
        "# Train\n",
        "history = model.fit(X_train, y_train, epochs=50, verbose=0, validation_split=0.2)\n",
        "\n",
        "# Predict\n",
        "lstm_pred = model.predict(X_test)\n",
        "lstm_pred = scaler.inverse_transform(lstm_pred).flatten()\n",
        "y_test_actual = scaler.inverse_transform(y_test).flatten()\n",
        "\n",
        "## =============================================\n",
        "## 3. VISUALIZATION\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Manual styling for consistency\n",
        "plt.rcParams.update({\n",
        "    'axes.facecolor': 'white',\n",
        "    'grid.color': '#e0e0e0',\n",
        "    'axes.edgecolor': '#404040'\n",
        "})\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.plot(test_dates, y_test_actual, 'o-', color='#2c3e50',\n",
        "         label='Actual Demand', linewidth=2, markersize=8)\n",
        "plt.plot(test_dates, lstm_pred, 's--', color='#9b59b6',\n",
        "         label='LSTM Prediction', markersize=8, linewidth=2)\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.title('LSTM Model Performance: Hospital Bed Demand Prediction', pad=20)\n",
        "plt.ylabel('Patients per Million')\n",
        "plt.xlabel('Date')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend()\n",
        "\n",
        "# Annotate performance\n",
        "plt.annotate(f'MAE: {mean_absolute_error(y_test_actual, lstm_pred):.2f}',\n",
        "             xy=(0.75, 0.9), xycoords='axes fraction', color='#9b59b6',\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 4. MODEL EVALUATION\n",
        "## =============================================\n",
        "\n",
        "print(\"\\nLSTM Model Evaluation:\")\n",
        "print(f\"Test MAE: {mean_absolute_error(y_test_actual, lstm_pred):.2f} patients/million\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('LSTM Training History')\n",
        "plt.ylabel('MAE Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0iYQYPfXk2kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna pytorch-forecasting\n",
        "import optuna\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "\n",
        "## =============================================\n",
        "## 1. HYBRID MODEL ARCHITECTURES\n",
        "## =============================================\n",
        "\n",
        "### A. CNN-LSTM Hybrid\n",
        "def build_cnn_lstm(hp):\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_steps, len(features_to_scale))),\n",
        "        Conv1D(filters=hp.Int('filters', 32, 128, step=32),\n",
        "               kernel_size=hp.Int('kernel_size', 2, 5),\n",
        "               activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        LSTM(hp.Int('lstm_units', 32, 128, step=32),\n",
        "              return_sequences=True),\n",
        "        Dropout(hp.Float('dropout', 0.1, 0.5)),\n",
        "        LSTM(hp.Int('lstm_units_2', 16, 64, step=16)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
        "                loss='mae')\n",
        "    return model\n",
        "\n",
        "### B. Transformer-LSTM Hybrid\n",
        "class TransformerLSTM(Model):\n",
        "    def __init__(self, num_heads=4, key_dim=64, ff_dim=128):\n",
        "        super().__init__()\n",
        "        self.lstm = LSTM(64, return_sequences=True)\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.ffn = Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(64)\n",
        "        ])\n",
        "        self.final_dense = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.lstm(inputs)\n",
        "        attn_output = self.attention(x, x)\n",
        "        x = self.norm(x + attn_output)\n",
        "        x = self.ffn(x)\n",
        "        return self.final_dense(x[:, -1, :])\n",
        "\n",
        "### C. Bayesian-optimized GRU\n",
        "def gru_objective(trial):\n",
        "    params = {\n",
        "        'units1': trial.suggest_int('units1', 32, 128),\n",
        "        'units2': trial.suggest_int('units2', 16, 64),\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
        "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    }\n",
        "\n",
        "    model = Sequential([\n",
        "        GRU(params['units1'], return_sequences=True),\n",
        "        Dropout(params['dropout']),\n",
        "        GRU(params['units2']),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(params['lr']), loss='mae')\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=50,\n",
        "        verbose=0\n",
        "    )\n",
        "    return min(history.history['val_loss'])\n",
        "\n",
        "### D. Temporal Fusion Transformer\n",
        "def prepare_tft_data():\n",
        "    dataset = TimeSeriesDataSet(\n",
        "        us_data.reset_index(),\n",
        "        time_idx=\"time_idx\",\n",
        "        target=\"hosp_patients_per_million\",\n",
        "        group_ids=[\"group\"],\n",
        "        min_encoder_length=max_encoder_length // 2,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        time_varying_known_reals=[\"time_idx\"] + features_to_scale,\n",
        "        target_normalizer=GroupNormalizer(groups=[\"group\"]),\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "## =============================================\n",
        "## 2. TRAINING FRAMEWORK\n",
        "## =============================================\n",
        "\n",
        "# Bayesian Optimization for GRU\n",
        "gru_study = optuna.create_study(direction='minimize')\n",
        "gru_study.optimize(gru_objective, n_trials=30)\n",
        "\n",
        "# Build best GRU model\n",
        "best_gru = Sequential([\n",
        "    GRU(gru_study.best_params['units1'], return_sequences=True),\n",
        "    Dropout(gru_study.best_params['dropout']),\n",
        "    GRU(gru_study.best_params['units2']),\n",
        "    Dense(1)\n",
        "])\n",
        "best_gru.compile(optimizer=Adam(gru_study.best_params['lr']), loss='mae')\n",
        "\n",
        "# Hybrid Models\n",
        "models = {\n",
        "    'CNN-LSTM': build_cnn_lstm(optuna.FixedTrial({\n",
        "        'filters': 64,\n",
        "        'kernel_size': 3,\n",
        "        'lstm_units': 64,\n",
        "        'dropout': 0.2,\n",
        "        'lr': 0.001\n",
        "    })),\n",
        "    'Transformer-LSTM': TransformerLSTM(),\n",
        "    'Bayesian GRU': best_gru\n",
        "}\n",
        "\n",
        "# Train all models\n",
        "histories = {}\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "    histories[name] = history.history\n",
        "    predictions[name] = model.predict(X_test).flatten()\n",
        "\n",
        "## =============================================\n",
        "## 3. COMPREHENSIVE EVALUATION\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "\n",
        "# Plot actual vs predictions\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
        "plt.plot(test_dates, y_test_actual, 'o-', color='#2c3e50',\n",
        "         linewidth=3, markersize=8, label='Actual')\n",
        "\n",
        "for i, (name, preds) in enumerate(predictions.items()):\n",
        "    plt.plot(test_dates, preds, '--', color=colors[i],\n",
        "             linewidth=2.5, label=f'{name} (MAE: {mean_absolute_error(y_test_actual, preds):.2f})')\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.title('Advanced Model Comparison: Hospital Bed Demand Prediction',\n",
        "          pad=20, fontsize=16)\n",
        "plt.ylabel('Patients per Million', fontsize=14)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Performance table\n",
        "mae_scores = {name: mean_absolute_error(y_test_actual, preds)\n",
        "              for name, preds in predictions.items()}\n",
        "sorted_models = sorted(mae_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "table_data = [[name, f\"{mae:.2f}\"] for name, mae in sorted_models]\n",
        "table = plt.table(cellText=table_data,\n",
        "                 colLabels=['Model', 'MAE'],\n",
        "                 loc='lower right',\n",
        "                 cellLoc='center',\n",
        "                 bbox=[0.7, 0.15, 0.25, 0.3])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 4. MODEL ANALYSIS\n",
        "## =============================================\n",
        "\n",
        "# Training dynamics\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    plt.plot(history['loss'], '--', color=colors[i], alpha=0.7, label=f'{name} Train')\n",
        "    plt.plot(history['val_loss'], '-', color=colors[i], label=f'{name} Val')\n",
        "\n",
        "plt.title('Training Dynamics Comparison', pad=20, fontsize=16)\n",
        "plt.ylabel('MAE Loss', fontsize=14)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for TFT\n",
        "tft_dataset = prepare_tft_data()\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    tft_dataset,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=32,\n",
        "    attention_head_size=4,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=16,\n",
        "    output_size=1,\n",
        "    loss=torch.nn.MSELoss()\n",
        ")\n",
        "\n",
        "# (Note: TFT training would require PyTorch DataLoader setup)"
      ],
      "metadata": {
        "id": "7VgEeZrAmevi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Temporal Fusion Transformer (Advanced Model)"
      ],
      "metadata": {
        "id": "OcOb4UuCoGSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the dataset (replace with your actual path)\n",
        "data_path = '/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv'\n",
        "us_data = pd.read_csv(data_path, parse_dates=['date'])\n",
        "\n",
        "# Filter for United States data only\n",
        "us_data = us_data[us_data['location'] == 'United States'].set_index('date')\n",
        "\n",
        "## =============================================\n",
        "## 1. DATA PREPARATION FOR RANDOM FOREST\n",
        "## =============================================\n",
        "\n",
        "# Select features and target\n",
        "features = ['new_cases_per_million',\n",
        "            'people_fully_vaccinated_per_hundred',\n",
        "            'stringency_index',\n",
        "            'hospital_beds_per_thousand']\n",
        "target = 'hosp_patients_per_million'\n",
        "\n",
        "# Prepare data (ensure no missing values)\n",
        "model_data = us_data[features + [target]].dropna()\n",
        "\n",
        "# Create temporal features\n",
        "model_data['day_of_week'] = model_data.index.dayofweek\n",
        "model_data['month'] = model_data.index.month\n",
        "\n",
        "# Create lagged features (7-day lags)\n",
        "model_data['cases_lag7'] = model_data['new_cases_per_million'].shift(7)\n",
        "model_data['hosp_lag7'] = model_data[target].shift(7)\n",
        "\n",
        "# Remove rows with NA values from lagging\n",
        "model_data = model_data.dropna()\n",
        "\n",
        "# Final feature set\n",
        "features_extended = features + ['day_of_week', 'month', 'cases_lag7', 'hosp_lag7']\n",
        "\n",
        "## =============================================\n",
        "## 2. TRAIN-TEST SPLIT\n",
        "## =============================================\n",
        "\n",
        "# Temporal split (last 14 days for test)\n",
        "test_size = 14\n",
        "X_train = model_data[features_extended][:-test_size]\n",
        "y_train = model_data[target][:-test_size]\n",
        "X_test = model_data[features_extended][-test_size:]\n",
        "y_test = model_data[target][-test_size:]\n",
        "test_dates = model_data.index[-test_size:]\n",
        "\n",
        "## =============================================\n",
        "## 3. RANDOM FOREST IMPLEMENTATION\n",
        "## =============================================\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available cores\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "\n",
        "## =============================================\n",
        "## 4. VISUALIZATION\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot actual vs predicted\n",
        "plt.plot(test_dates, y_test, 'o-', color='#2c3e50',\n",
        "         linewidth=2, markersize=8, label='Actual Demand')\n",
        "plt.plot(test_dates, rf_pred, 's--', color='#2ecc71',\n",
        "         linewidth=2, markersize=8, label='RF Prediction')\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.title('Random Forest: Hospital Bed Demand Prediction\\nUnited States', pad=20)\n",
        "plt.ylabel('Patients per Million')\n",
        "plt.xlabel('Date')\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend()\n",
        "\n",
        "# Annotate performance\n",
        "plt.annotate(f'MAE: {mean_absolute_error(y_test, rf_pred):.2f}',\n",
        "             xy=(0.75, 0.9), xycoords='axes fraction',\n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 5. FEATURE IMPORTANCE ANALYSIS\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "importances = rf_model.feature_importances_\n",
        "sorted_idx = np.argsort(importances)[-10:]  # Top 10 features\n",
        "\n",
        "plt.barh(range(len(sorted_idx)), importances[sorted_idx], color='#27ae60')\n",
        "plt.yticks(range(len(sorted_idx)), np.array(features_extended)[sorted_idx])\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.grid(True, linestyle='--', alpha=0.4, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop Predictive Features:\")\n",
        "for feature, importance in zip(np.array(features_extended)[sorted_idx],\n",
        "                             importances[sorted_idx]):\n",
        "    print(f\"{feature}: {importance:.3f}\")"
      ],
      "metadata": {
        "id": "fToKcAq6olv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Hybrid Architectures with Bayesian Optimization"
      ],
      "metadata": {
        "id": "kq3NWO3CptgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna pytorch-forecasting\n",
        "import optuna\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, TimeDistributed\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
        "from pytorch_forecasting.data import GroupNormalizer\n",
        "\n",
        "## =============================================\n",
        "## 1. HYBRID MODEL ARCHITECTURES\n",
        "## =============================================\n",
        "\n",
        "### A. CNN-LSTM Hybrid\n",
        "def build_cnn_lstm(hp):\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_steps, len(features_to_scale))),\n",
        "        Conv1D(filters=hp.Int('filters', 32, 128, step=32),\n",
        "               kernel_size=hp.Int('kernel_size', 2, 5),\n",
        "               activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        LSTM(hp.Int('lstm_units', 32, 128, step=32),\n",
        "              return_sequences=True),\n",
        "        Dropout(hp.Float('dropout', 0.1, 0.5)),\n",
        "        LSTM(hp.Int('lstm_units_2', 16, 64, step=16)),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
        "                loss='mae')\n",
        "    return model\n",
        "\n",
        "### B. Transformer-LSTM Hybrid\n",
        "class TransformerLSTM(Model):\n",
        "    def __init__(self, num_heads=4, key_dim=64, ff_dim=128):\n",
        "        super().__init__()\n",
        "        self.lstm = LSTM(64, return_sequences=True)\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.ffn = Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(64)\n",
        "        ])\n",
        "        self.final_dense = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.lstm(inputs)\n",
        "        attn_output = self.attention(x, x)\n",
        "        x = self.norm(x + attn_output)\n",
        "        x = self.ffn(x)\n",
        "        return self.final_dense(x[:, -1, :])\n",
        "\n",
        "### C. Bayesian-optimized GRU\n",
        "def gru_objective(trial):\n",
        "    params = {\n",
        "        'units1': trial.suggest_int('units1', 32, 128),\n",
        "        'units2': trial.suggest_int('units2', 16, 64),\n",
        "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
        "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
        "    }\n",
        "\n",
        "    model = Sequential([\n",
        "        GRU(params['units1'], return_sequences=True),\n",
        "        Dropout(params['dropout']),\n",
        "        GRU(params['units2']),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(params['lr']), loss='mae')\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=50,\n",
        "        verbose=0\n",
        "    )\n",
        "    return min(history.history['val_loss'])\n",
        "\n",
        "### D. Temporal Fusion Transformer\n",
        "def prepare_tft_data():\n",
        "    dataset = TimeSeriesDataSet(\n",
        "        us_data.reset_index(),\n",
        "        time_idx=\"time_idx\",\n",
        "        target=\"hosp_patients_per_million\",\n",
        "        group_ids=[\"group\"],\n",
        "        min_encoder_length=max_encoder_length // 2,\n",
        "        max_encoder_length=max_encoder_length,\n",
        "        min_prediction_length=1,\n",
        "        max_prediction_length=max_prediction_length,\n",
        "        time_varying_known_reals=[\"time_idx\"] + features_to_scale,\n",
        "        target_normalizer=GroupNormalizer(groups=[\"group\"]),\n",
        "        add_relative_time_idx=True,\n",
        "        add_target_scales=True,\n",
        "        add_encoder_length=True,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "## =============================================\n",
        "## 2. TRAINING FRAMEWORK\n",
        "## =============================================\n",
        "\n",
        "# Bayesian Optimization for GRU\n",
        "gru_study = optuna.create_study(direction='minimize')\n",
        "gru_study.optimize(gru_objective, n_trials=30)\n",
        "\n",
        "# Build best GRU model\n",
        "best_gru = Sequential([\n",
        "    GRU(gru_study.best_params['units1'], return_sequences=True),\n",
        "    Dropout(gru_study.best_params['dropout']),\n",
        "    GRU(gru_study.best_params['units2']),\n",
        "    Dense(1)\n",
        "])\n",
        "best_gru.compile(optimizer=Adam(gru_study.best_params['lr']), loss='mae')\n",
        "\n",
        "# Hybrid Models\n",
        "models = {\n",
        "    'CNN-LSTM': build_cnn_lstm(optuna.FixedTrial({\n",
        "        'filters': 64,\n",
        "        'kernel_size': 3,\n",
        "        'lstm_units': 64,\n",
        "        'dropout': 0.2,\n",
        "        'lr': 0.001\n",
        "    })),\n",
        "    'Transformer-LSTM': TransformerLSTM(),\n",
        "    'Bayesian GRU': best_gru\n",
        "}\n",
        "\n",
        "# Train all models\n",
        "histories = {}\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "    histories[name] = history.history\n",
        "    predictions[name] = model.predict(X_test).flatten()\n",
        "\n",
        "## =============================================\n",
        "## 3. COMPREHENSIVE EVALUATION\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "\n",
        "# Plot actual vs predictions\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
        "plt.plot(test_dates, y_test_actual, 'o-', color='#2c3e50',\n",
        "         linewidth=3, markersize=8, label='Actual')\n",
        "\n",
        "for i, (name, preds) in enumerate(predictions.items()):\n",
        "    plt.plot(test_dates, preds, '--', color=colors[i],\n",
        "             linewidth=2.5, label=f'{name} (MAE: {mean_absolute_error(y_test_actual, preds):.2f})')\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.title('Advanced Model Comparison: Hospital Bed Demand Prediction',\n",
        "          pad=20, fontsize=16)\n",
        "plt.ylabel('Patients per Million', fontsize=14)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Performance table\n",
        "mae_scores = {name: mean_absolute_error(y_test_actual, preds)\n",
        "              for name, preds in predictions.items()}\n",
        "sorted_models = sorted(mae_scores.items(), key=lambda x: x[1])\n",
        "\n",
        "table_data = [[name, f\"{mae:.2f}\"] for name, mae in sorted_models]\n",
        "table = plt.table(cellText=table_data,\n",
        "                 colLabels=['Model', 'MAE'],\n",
        "                 loc='lower right',\n",
        "                 cellLoc='center',\n",
        "                 bbox=[0.7, 0.15, 0.25, 0.3])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 4. MODEL ANALYSIS\n",
        "## =============================================\n",
        "\n",
        "# Training dynamics\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i, (name, history) in enumerate(histories.items()):\n",
        "    plt.plot(history['loss'], '--', color=colors[i], alpha=0.7, label=f'{name} Train')\n",
        "    plt.plot(history['val_loss'], '-', color=colors[i], label=f'{name} Val')\n",
        "\n",
        "plt.title('Training Dynamics Comparison', pad=20, fontsize=16)\n",
        "plt.ylabel('MAE Loss', fontsize=14)\n",
        "plt.xlabel('Epoch', fontsize=14)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for TFT\n",
        "tft_dataset = prepare_tft_data()\n",
        "tft = TemporalFusionTransformer.from_dataset(\n",
        "    tft_dataset,\n",
        "    learning_rate=0.03,\n",
        "    hidden_size=32,\n",
        "    attention_head_size=4,\n",
        "    dropout=0.1,\n",
        "    hidden_continuous_size=16,\n",
        "    output_size=1,\n",
        "    loss=torch.nn.MSELoss()\n",
        ")\n",
        "\n",
        "# (Note: TFT training would require PyTorch DataLoader setup)"
      ],
      "metadata": {
        "id": "l3u9SI0apwS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import tensorflow as tf  # Added this import\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU, Input, Layer\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "## =============================================\n",
        "## 1. DATA PREPARATION (Enhanced)\n",
        "## =============================================\n",
        "\n",
        "# Prepare scaled data with more features\n",
        "scaler = MinMaxScaler()\n",
        "features_to_scale = ['hosp_patients_per_million', 'new_cases_per_million',\n",
        "                    'people_fully_vaccinated_per_hundred']\n",
        "scaled_data = scaler.fit_transform(us_data[features_to_scale])\n",
        "\n",
        "# Create sequences with multiple features\n",
        "def create_sequences(data, dates, n_steps):\n",
        "    X, y, y_dates = [], [], []\n",
        "    for i in range(len(data)-n_steps):\n",
        "        X.append(data[i:i+n_steps])\n",
        "        y.append(data[i+n_steps, 0])  # Predict only hospitalization\n",
        "        y_dates.append(dates.iloc[i+n_steps])\n",
        "    return np.array(X), np.array(y), np.array(y_dates)\n",
        "\n",
        "n_steps = 14\n",
        "X, y, y_dates = create_sequences(scaled_data, us_data['date'], n_steps)\n",
        "\n",
        "# Train-test split\n",
        "test_size = 14\n",
        "X_train, X_test = X[:-test_size], X[-test_size:]\n",
        "y_train, y_test = y[:-test_size], y[-test_size:]\n",
        "test_dates = y_dates[-test_size:]\n",
        "\n",
        "## =============================================\n",
        "## 2. MODEL ARCHITECTURES\n",
        "## =============================================\n",
        "\n",
        "### A. Stacked LSTM Model\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(n_steps, len(features_to_scale))),\n",
        "    LSTM(64, activation='relu', return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
        "\n",
        "### B. LSTM with Attention Mechanism\n",
        "class AttentionLayer(Layer):\n",
        "    def __init__(self, units=32):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        hidden_states = inputs\n",
        "        score = self.V(tf.nn.tanh(self.W1(hidden_states) + self.W2(hidden_states)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * hidden_states\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "\n",
        "# Build model\n",
        "inputs = Input(shape=(n_steps, len(features_to_scale)))\n",
        "lstm_out = LSTM(64, activation='relu', return_sequences=True)(inputs)\n",
        "attention = AttentionLayer(32)(lstm_out)\n",
        "outputs = Dense(1)(attention)\n",
        "attention_model = Model(inputs, outputs)\n",
        "attention_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
        "\n",
        "### C. GRU Model\n",
        "gru_model = Sequential([\n",
        "    Input(shape=(n_steps, len(features_to_scale))),\n",
        "    GRU(64, activation='relu', return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    GRU(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "gru_model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
        "\n",
        "## =============================================\n",
        "## 3. TRAINING AND EVALUATION\n",
        "## =============================================\n",
        "\n",
        "models = {\n",
        "    'Stacked LSTM': lstm_model,\n",
        "    'LSTM with Attention': attention_model,\n",
        "    'GRU': gru_model\n",
        "}\n",
        "\n",
        "history_dict = {}\n",
        "predictions = {}\n",
        "\n",
        "# Train and evaluate all models\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        verbose=0\n",
        "    )\n",
        "    history_dict[name] = history.history\n",
        "    predictions[name] = model.predict(X_test).flatten()\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_test_actual = scaler.inverse_transform(\n",
        "    np.concatenate([y_test.reshape(-1,1), np.zeros((len(y_test), len(features_to_scale)-1))], axis=1)\n",
        ")[:,0]\n",
        "\n",
        "for name in predictions:\n",
        "    preds = predictions[name]\n",
        "    # Create dummy array for inverse transform\n",
        "    dummy = np.zeros((len(preds), len(features_to_scale)))\n",
        "    dummy[:,0] = preds\n",
        "    predictions[name] = scaler.inverse_transform(dummy)[:,0]\n",
        "\n",
        "## =============================================\n",
        "## 4. VISUALIZATION AND COMPARISON\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "\n",
        "# Plot actual values\n",
        "plt.plot(test_dates, y_test_actual, 'o-', color='#2c3e50',\n",
        "         linewidth=3, markersize=8, label='Actual Demand')\n",
        "\n",
        "# Plot model predictions\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "for i, (name, preds) in enumerate(predictions.items()):\n",
        "    plt.plot(test_dates, preds, '--', color=colors[i],\n",
        "             linewidth=2.5, label=f'{name} (MAE: {mean_absolute_error(y_test_actual, preds):.2f})')\n",
        "\n",
        "# Formatting\n",
        "ax = plt.gca()\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.title('Sequence Model Comparison: Hospital Bed Demand Prediction', pad=20, fontsize=14)\n",
        "plt.ylabel('Patients per Million', fontsize=12)\n",
        "plt.xlabel('Date', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=12)\n",
        "\n",
        "# Add model performance table\n",
        "model_mae = {name: mean_absolute_error(y_test_actual, preds)\n",
        "             for name, preds in predictions.items()}\n",
        "sorted_models = sorted(model_mae.items(), key=lambda x: x[1])\n",
        "\n",
        "table_data = [[name, f\"{mae:.2f}\"] for name, mae in sorted_models]\n",
        "table = plt.table(cellText=table_data,\n",
        "                 colLabels=['Model', 'MAE'],\n",
        "                 loc='lower right',\n",
        "                 cellLoc='center',\n",
        "                 bbox=[0.7, 0.15, 0.25, 0.3])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## =============================================\n",
        "## 5. TRAINING HISTORY ANALYSIS\n",
        "## =============================================\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "for i, (name, history) in enumerate(history_dict.items()):\n",
        "    plt.plot(history['loss'], '--', color=colors[i], alpha=0.7, label=f'{name} Training')\n",
        "    plt.plot(history['val_loss'], '-', color=colors[i], label=f'{name} Validation')\n",
        "\n",
        "plt.title('Model Training Histories', pad=20, fontsize=14)\n",
        "plt.ylabel('MAE Loss', fontsize=12)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.grid(True, linestyle='--', alpha=0.4)\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWsRhSHZqOHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YGvTL13yo3Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Time period covered: {df['date'].min()} to {df['date'].max()}\")\n",
        "print(f\"Number of countries/regions: {df['location'].nunique()}\")\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()\n",
        "\n",
        "# Summary statistics\n",
        "df.describe()\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_data = pd.DataFrame({'Missing Values': missing_values,\n",
        "                             'Percentage': missing_percentage})\n",
        "missing_data = missing_data[missing_data['Missing Values'] > 0].sort_values('Percentage', ascending=False)\n",
        "missing_data"
      ],
      "metadata": {
        "id": "Q9Ms6hu0zolO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_2.plot(kind='scatter', x='Missing Values', y='Percentage', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "f4cYlOPI0KTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_3['Missing Values'].plot(kind='line', figsize=(8, 4), title='Missing Values')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z6SO4Tds0Gqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# @title Missing Values\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "missing_data['Missing Values'].plot(kind='line', figsize=(8, 4), title='Missing Values')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "BCV8vKSt0Dwb"
      }
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_0['Missing Values'].plot(kind='hist', bins=20, title='Missing Values')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "q2qsbX9az790"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROPHET MODEL"
      ],
      "metadata": {
        "id": "69ysok5Ln3Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "\n",
        "# Load cleaned data\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_data.csv')\n",
        "\n",
        "# Prepare Prophet format (ds = timestamp, y = target variable)\n",
        "prophet_data = data[['date', 'hospital_admissions']].rename(columns={'date': 'ds', 'hospital_admissions': 'y'})\n",
        "\n",
        "# Initialize and fit model\n",
        "model = Prophet(\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    daily_seasonality=False,\n",
        "    changepoint_prior_scale=0.05,\n",
        "    seasonality_prior_scale=10\n",
        ")\n",
        "model.add_country_holidays(country_name='US')  # Adjust based on region\n",
        "model.fit(prophet_data)\n",
        "\n",
        "# Make future dataframe and predict\n",
        "future = model.make_future_dataframe(periods=30)  # 30-day forecast\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Plot components\n",
        "fig = model.plot_components(forecast)"
      ],
      "metadata": {
        "id": "3TP2tmASn5uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Approach\n",
        "#Implementation Code:"
      ],
      "metadata": {
        "id": "Fi2sbPw5rBrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Initialize individual models\n",
        "prophet = Prophet(...)  # Configured as before\n",
        "rf = RandomForestRegressor(...)  # Configured as before\n",
        "xgb = xgb.XGBRegressor(...)  # Configured as before\n",
        "\n",
        "# Create weighted ensemble\n",
        "ensemble = VotingRegressor([\n",
        "    ('prophet', prophet),\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb)],\n",
        "    weights=[0.2, 0.3, 0.5]  # Adjust based on validation\n",
        ")\n",
        "\n",
        "# Dynamic weighting based on forecast horizon\n",
        "def get_dynamic_weights(horizon):\n",
        "    if horizon <= 7:   return [0.1, 0.2, 0.7]  # XGBoost heavy\n",
        "    elif horizon <= 14: return [0.2, 0.3, 0.5]  # Balanced\n",
        "    else:              return [0.5, 0.3, 0.2]  # Prophet heavy\n",
        "\n",
        "# Evaluate\n",
        "for horizon in [7, 14, 30]:\n",
        "    weights = get_dynamic_weights(horizon)\n",
        "    ensemble.set_params(weights=weights)\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    preds = ensemble.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    print(f\"MAE for {horizon}-day forecast: {mae:.2f}\")"
      ],
      "metadata": {
        "id": "BqQ6ckgjrBYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Networks\n",
        "# Implementation Code:"
      ],
      "metadata": {
        "id": "91e_INvWryNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Prepare sequential data\n",
        "def create_sequences(data, n_steps):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data)-n_steps):\n",
        "        X.append(data[i:i+n_steps])\n",
        "        y.append(data[i+n_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "n_steps = 14  # 2-week lookback\n",
        "X, y = create_sequences(data.values, n_steps)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    LSTM(100, activation='relu', input_shape=(n_steps, X.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train with early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
        "model.fit(\n",
        "    X, y,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stop],\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "PglymSYCr-yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, mount Google Drive (if not already mounted)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the data path\n",
        "data_path = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_data.csv'\n",
        "\n",
        "# Verify the file exists\n",
        "if os.path.exists(data_path):\n",
        "    try:\n",
        "        # Load the data\n",
        "        data = pd.read_csv(data_path)\n",
        "        print(\"Data loaded successfully!\")\n",
        "        print(f\"Data shape: {data.shape}\")\n",
        "        print(\"\\nFirst 5 rows:\")\n",
        "        print(data.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file: {e}\")\n",
        "else:\n",
        "    print(f\"File not found at: {data_path}\")\n",
        "    print(\"\\nAvailable files in /content/drive/MyDrive/:\")\n",
        "    print(os.listdir('/content/drive/MyDrive/'))\n",
        "\n",
        "    # Check if Medical_Resource_Prediction directory exists\n",
        "    if 'Medical_Resource_Prediction' in os.listdir('/content/drive/MyDrive/'):\n",
        "        print(\"\\nFiles in Medical_Resource_Prediction folder:\")\n",
        "        print(os.listdir('/content/drive/MyDrive/Medical_Resource_Prediction/'))\n",
        "    else:\n",
        "        print(\"\\nMedical_Resource_Prediction folder not found in your Google Drive\")"
      ],
      "metadata": {
        "id": "XWdQjxViuL1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesCV\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Time-series cross-validation\n",
        "tscv = TimeSeriesCV(n_splits=5)\n",
        "xgb = XGBRegressor(**xgb_params)\n",
        "xgb_scores = cross_val_score(xgb, train[features], train['future_icu_demand'],\n",
        "                            cv=tscv, scoring='neg_mean_absolute_error')"
      ],
      "metadata": {
        "id": "9zYG4s2D0CP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
        "\n",
        "# ====================== 1. LOAD OR CREATE YOUR DATA ======================\n",
        "# Example: Create synthetic time-series data (replace with your actual data)\n",
        "dates = pd.date_range(start=\"2020-01-01\", periods=100, freq=\"D\")\n",
        "train = pd.DataFrame({\n",
        "    \"date\": dates,\n",
        "    \"feature1\": np.random.rand(100) * 10,\n",
        "    \"feature2\": np.random.rand(100) * 5,\n",
        "    \"future_icu_demand\": np.random.rand(100) * 100  # Target variable\n",
        "})\n",
        "\n",
        "# Set 'date' as index (optional for time-series)\n",
        "train.set_index(\"date\", inplace=True)\n",
        "\n",
        "# ====================== 2. DEFINE FEATURES & TARGET ======================\n",
        "features = [\"feature1\", \"feature2\"]  # Predictors\n",
        "target = \"future_icu_demand\"         # Target column\n",
        "\n",
        "# ====================== 3. XGBOOST MODEL & TIME-SERIES CV ======================\n",
        "xgb_params = {\n",
        "    'n_estimators': 300,\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'gamma': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Time-series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "xgb = XGBRegressor(**xgb_params)\n",
        "\n",
        "# Run cross-validation\n",
        "xgb_scores = cross_val_score(\n",
        "    xgb,\n",
        "    train[features],          # Features (predictors)\n",
        "    train[target],            # Target variable\n",
        "    cv=tscv,\n",
        "    scoring=\"neg_mean_absolute_error\"\n",
        ")\n",
        "\n",
        "# Convert negative MAE to positive (sklearn convention)\n",
        "mae_scores = -xgb_scores\n",
        "\n",
        "print(\"Cross-validation MAE scores:\", mae_scores)\n",
        "print(f\"Mean MAE: {mae_scores.mean():.2f}\")"
      ],
      "metadata": {
        "id": "LvgKJYWOhrdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}