{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "authorship_tag": "ABX9TyNStAuDC8n0nSLyzMiI7GR4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DelMashiry-dev/DelMashiry-dev/blob/main/REALTIMEPREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount the Drive Containing the folder with the dataset\n"
      ],
      "metadata": {
        "id": "3XNhqck_8qBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn2iL5XOsX2q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/Medical_Resource_Prediction"
      ],
      "metadata": {
        "id": "dS_F3YEd8-PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Read first 100000 rows but only specific columns\n",
        "df = pd.read_csv('/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv', nrows=100000, usecols=['iso_code', 'continent', 'location', 'date','total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'reproduction_rate',\n",
        "    'icu_patients', 'hosp_patients', 'weekly_icu_admissions', 'weekly_hosp_admissions','total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated', 'new_vaccinations',\n",
        "    'total_tests', 'new_tests', 'positive_rate','population', 'population_density', 'median_age', 'aged_65_older', 'aged_70_older','cardiovasc_death_rate', 'diabetes_prevalence'])\n",
        "df.fillna(0, inplace=True)\n",
        "from IPython.display import display\n",
        "display(df.head(5000))"
      ],
      "metadata": {
        "id": "QuDpWswZ9M7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify Relevant Features\n",
        "\n"
      ],
      "metadata": {
        "id": "WPsVjR0QHI5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_columns = [\n",
        "    'iso_code', 'continent', 'location', 'date','total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'reproduction_rate',\n",
        "    'icu_patients', 'hosp_patients', 'weekly_icu_admissions', 'weekly_hosp_admissions','total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated', 'new_vaccinations',\n",
        "    'total_tests', 'new_tests', 'positive_rate', 'population', 'population_density', 'median_age', 'aged_65_older', 'aged_70_older',\n",
        "    'hospital_beds_per_thousand', 'cardiovasc_death_rate', 'diabetes_prevalence'\n",
        "]"
      ],
      "metadata": {
        "id": "sGi9QlQX-MeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning\n",
        "# Remove Irrelevant Columns\n",
        "# Filter the dataset to keep only the selected columns:\n",
        "\n"
      ],
      "metadata": {
        "id": "sZvVPoCJHlcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv')\n",
        "\n",
        "# Keep relevant columns\n",
        "df = df[relevant_columns]\n",
        "from IPython.display import display\n",
        "display(df.head(5))"
      ],
      "metadata": {
        "id": "T6xM4ZywHBCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values\n",
        "\n"
      ],
      "metadata": {
        "id": "7-s6Zoe8OtQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imputation Strategy\n",
        "\n"
      ],
      "metadata": {
        "id": "TnBflW_6PSMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert date to datetime\n",
        "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing iso_code, location, or date\n",
        "df = df.dropna(subset=['iso_code', 'location', 'date'])\n",
        "\n",
        "# Time-series imputation for disease and healthcare metrics\n",
        "time_series_cols = [\n",
        "    'total_cases', 'new_cases', 'total_deaths', 'new_deaths', 'reproduction_rate',\n",
        "    'icu_patients', 'hosp_patients', 'weekly_icu_admissions', 'weekly_hosp_admissions',\n",
        "    'total_vaccinations', 'people_vaccinated', 'people_fully_vaccinated', 'new_vaccinations',\n",
        "    'total_tests', 'new_tests', 'positive_rate'\n",
        "]\n",
        "for col in time_series_cols:\n",
        "    df[col] = df.groupby('location')[col].fillna(method='ffill').interpolate()\n",
        "\n",
        "# Zero imputation for ICU/hospital metrics if still missing (early outbreak)\n",
        "zero_impute_cols = ['icu_patients', 'hosp_patients', 'weekly_icu_admissions', 'weekly_hosp_admissions']\n",
        "df[zero_impute_cols] = df[zero_impute_cols].fillna(0)\n",
        "\n",
        "# Median imputation for demographic/healthcare capacity features\n",
        "median_impute_cols = [\n",
        "    'population_density', 'median_age', 'aged_65_older', 'aged_70_older',\n",
        "    'hospital_beds_per_thousand', 'cardiovasc_death_rate', 'diabetes_prevalence'\n",
        "]\n",
        "for col in median_impute_cols:\n",
        "    df[col] = df.groupby('continent')[col].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# Handle remaining missing values (if any) with global median\n",
        "for col in median_impute_cols:\n",
        "    df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "    display(df.head(5000))"
      ],
      "metadata": {
        "id": "5DYyaRJUPOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for Outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "dOsasCkLQ_80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in time_series_cols:\n",
        "    df[col] = df.groupby('location')[col].transform(lambda x: x.clip(upper=x.quantile(0.99)))"
      ],
      "metadata": {
        "id": "btD1MD2mQm1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "cUv15i1gSaMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lag features\n",
        "for col in ['new_cases', 'icu_patients', 'hosp_patients']:\n",
        "    df[f'{col}_lag7'] = df.groupby('location')[col].shift(7)\n",
        "    df[f'{col}_lag14'] = df.groupby('location')[col].shift(14)\n",
        "\n",
        "# Rolling averages\n",
        "df['new_cases_7d_avg'] = df.groupby('location')['new_cases'].rolling(7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "df['new_deaths_7d_avg'] = df.groupby('location')['new_deaths'].rolling(7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# Proxy for ventilator demand (e.g., 50% of ICU patients)\n",
        "df['ventilator_demand'] = df['icu_patients'] * 0.5"
      ],
      "metadata": {
        "id": "YrmyYucOSbWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Dataset\n",
        "# Ensure the dataset is sorted by location and date for time-series modeling:\n",
        "\n"
      ],
      "metadata": {
        "id": "_gOpF9z1Sk-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values(['location', 'date'])"
      ],
      "metadata": {
        "id": "JPTN4YbsSvgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rZg7_k9MUyL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0IP9Nu9KVAI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time Series Plot Example (Plotly)\n",
        "\n"
      ],
      "metadata": {
        "id": "IMNV5JJZWvv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This creates an interactive line plot where you can hover over points to see exact values, zoom in on specific time periods, and compare trends across locations. This is particularly useful for understanding temporal patterns in disease progression and resource demand\n",
        "\n"
      ],
      "metadata": {
        "id": "9cEWtVh0W3se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Plot multiple metrics over time, colored by location\n",
        "fig = px.line(df, x='date', y=['total_cases', 'new_cases', 'icu_patients'], color='location', title='Key Metrics Over Time')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hxuZD5i2VtNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scatter Plot Example (Seaborn)\n",
        "\n"
      ],
      "metadata": {
        "id": "PTXBiGPEYBAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot total_cases vs. icu_patients, colored by location\n",
        "sns.scatterplot(data=df, x='total_cases', y='icu_patients', hue='location')\n",
        "plt.title('Total Cases vs. ICU Patients')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MsvbTQNjYGG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This scatter plot helps visualize the relationship between disease spread and ICU demand, which is a proxy for ventilator and medical personnel needs. You can extend this to plot total_vaccinations vs. new_cases to assess vaccination impact.\n",
        "\n"
      ],
      "metadata": {
        "id": "DhbVDd31bdyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Heatmap (Seaborn)\n",
        "\n"
      ],
      "metadata": {
        "id": "Su5OcILlecmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numeric columns for correlation\n",
        "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Compute and plot correlation heatmap\n",
        "plt.figure(figsize=(40, 38))\n",
        "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xZVAAg6Hbe3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This heatmap shows correlations between all numeric features, helping identify which factors (e.g., population_density, median_age) are most related to resource demand\n",
        "\n"
      ],
      "metadata": {
        "id": "u5-RZ06-e8KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bar Chart(Plotly)\n",
        "\n"
      ],
      "metadata": {
        "id": "PPHRjxWyfIdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Get the latest data for each location\n",
        "latest_df = df[df['date'] == df['date'].max()]\n",
        "\n",
        "# Create bar chart for latest total_cases\n",
        "fig = px.bar(latest_df, x='location', y='total_cases', title='Latest Total Cases by Location')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2PjogA40fD8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This bar chart compares the most recent total cases across locations, which can help identify regions with high demand for real-time monitoring\n",
        "\n"
      ],
      "metadata": {
        "id": "t0YEAjdDgV62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Get the latest data for each location\n",
        "latest_df = df[df['date'] == df['date'].max()]\n",
        "\n",
        "# Sort by total cases in descending order\n",
        "latest_df = latest_df.sort_values(by='total_cases', ascending=False)\n",
        "\n",
        "# Create a horizontal bar chart with color by continent\n",
        "fig = px.bar(latest_df, x='total_cases', y='location', color='continent', orientation='h',\n",
        "             title='Latest Total Cases by Location, Colored by Continent')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "_9uC9uSsfuSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geographic Map (Plotly)\n",
        "\n"
      ],
      "metadata": {
        "id": "myJmcwMsgdlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Create choropleth map for total_cases by country, animated over time\n",
        "fig = px.choropleth(df, locations='iso_code', locationmode='ISO-3', color='total_cases', hover_name='location', animation_frame='date', title='Total Cases Over Time by Country')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "m7Bg8bA2gGjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This animated map shows the spatial distribution of total cases over time, which is useful for understanding regional disparities and can be sourced from real-time APIs like OWID\n",
        "\n"
      ],
      "metadata": {
        "id": "YRcrq8XAhq8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Assuming your cleaned DataFrame is named 'df_clean'\n",
        "# If not, replace with your actual cleaned DataFrame name\n",
        "\n",
        "def display_final_dataset(df, num_rows=5, all_columns=False):\n",
        "    \"\"\"\n",
        "    Displays the final prepared dataset in a clean table format\n",
        "\n",
        "    Parameters:\n",
        "    - df: Your cleaned DataFrame\n",
        "    - num_rows: Number of rows to display (default: 5)\n",
        "    - all_columns: Whether to show all columns (default: False for truncated view)\n",
        "    \"\"\"\n",
        "\n",
        "    # Configure display options\n",
        "    pd.set_option('display.max_columns', None if all_columns else 10)\n",
        "    pd.set_option('display.width', 1000)\n",
        "    pd.set_option('display.max_colwidth', 20)\n",
        "\n",
        "    # Create a styled table\n",
        "    styled_df = (df.head(num_rows)\n",
        "                 .style\n",
        "                 .set_properties(**{'text-align': 'center'})\n",
        "                 .set_table_styles([{\n",
        "                     'selector': 'th',\n",
        "                     'props': [('background-color', '#40466e'),\n",
        "                              ('color', 'white'),\n",
        "                              ('font-weight', 'bold')]\n",
        "                 }])\n",
        "                 .background_gradient(cmap='Blues', subset=df.select_dtypes(include='number').columns)\n",
        "                 .format(None, na_rep=\"NA\"))\n",
        "\n",
        "    # Display in notebook\n",
        "    display(styled_df)\n",
        "\n",
        "    # Show dataset info\n",
        "    print(\"\\n\\033[1mDataset Summary:\\033[0m\")\n",
        "    print(f\"Total Rows: {len(df):,}\")\n",
        "    print(f\"Total Columns: {len(df.columns)}\")\n",
        "    print(\"\\n\\033[1mColumn Types:\\033[0m\")\n",
        "    print(df.dtypes.value_counts())\n",
        "\n",
        "    # Show NA counts if any exist\n",
        "    if df.isna().sum().sum() > 0:\n",
        "        print(\"\\n\\033[1mMissing Values:\\033[0m\")\n",
        "        missing = df.isna().sum()[df.isna().sum() > 0]\n",
        "        print(missing)\n",
        "    else:\n",
        "        print(\"\\n\\033[1mNo missing values found!\\033[0m\")\n",
        "\n",
        "# Usage - call the function with your cleaned DataFrame\n",
        "display_final_dataset(df, num_rows=5, all_columns=True)"
      ],
      "metadata": {
        "id": "qSGGYLCtpVWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "def display_and_export_dataset(df, num_rows=5, export_name=\"cleaned_dataset\"):\n",
        "    \"\"\"\n",
        "    Displays the final dataset with enhanced formatting and export options\n",
        "\n",
        "    Parameters:\n",
        "    - df: Your cleaned DataFrame\n",
        "    - num_rows: Number of rows to display\n",
        "    - export_name: Base name for exported files\n",
        "    \"\"\"\n",
        "\n",
        "    # ==============================================\n",
        "    # 1. SPECIAL COLUMN FORMATTING\n",
        "    # ==============================================\n",
        "    format_rules = {\n",
        "        # Medical resource columns (integers with comma separators)\n",
        "        r'(beds|patients|ventilators|staff|ppe)': '{:,.0f}',\n",
        "\n",
        "        # Percentage columns (show as % with 1 decimal)\n",
        "        r'(rate|ratio|percent|per_hundred)': '{:.1%}',\n",
        "\n",
        "        # Date columns (standard date format)\n",
        "        'date': '{:%Y-%m-%d}',\n",
        "\n",
        "        # Small decimal numbers (3 decimal places)\n",
        "        r'(growth|factor|index)': '{:.3f}'\n",
        "    }\n",
        "\n",
        "    # ==============================================\n",
        "    # 2. CREATE STYLED TABLE\n",
        "    # ==============================================\n",
        "    styler = (df.head(num_rows)\n",
        "              .style\n",
        "              .set_properties(**{'text-align': 'center'})\n",
        "              .set_table_styles([{\n",
        "                  'selector': 'th',\n",
        "                  'props': [\n",
        "                      ('background-color', '#2a3f5f'),\n",
        "                      ('color', 'white'),\n",
        "                      ('font-weight', 'bold'),\n",
        "                      ('position', 'sticky'),\n",
        "                      ('top', '0')\n",
        "                  ]\n",
        "              }]))\n",
        "\n",
        "    # Apply special formatting\n",
        "    for regex, formatter in format_rules.items():\n",
        "        cols = df.filter(regex=regex, axis=1).columns\n",
        "        if not cols.empty:\n",
        "            styler.format(formatter, subset=cols)\n",
        "\n",
        "    # Highlight important metrics\n",
        "    medical_cols = df.filter(regex='icu|hosp|ventilator|ppe').columns\n",
        "    if not medical_cols.empty:\n",
        "        styler.background_gradient(\n",
        "            cmap='YlOrRd',\n",
        "            subset=medical_cols,\n",
        "            vmin=0, vmax=df[medical_cols].max().max()\n",
        "        )\n",
        "\n",
        "    # ==============================================\n",
        "    # 3. DISPLAY RESULTS\n",
        "    # ==============================================\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\033[1m{'CLEANED DATASET PREVIEW':^80}\\033[0m\")\n",
        "    print(\"=\"*80)\n",
        "    display(styler)\n",
        "\n",
        "    # ==============================================\n",
        "    # 4. STATISTICAL SUMMARIES\n",
        "    # ==============================================\n",
        "    print(\"\\n\\033[1mSTATISTICAL SUMMARIES\\033[0m\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Numeric columns summary\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    if not numeric_df.empty:\n",
        "        print(\"\\n\\033[4mNumeric Columns:\\033[0m\")\n",
        "        display(numeric_df.describe().style.format(\"{:.2f}\"))\n",
        "\n",
        "    # Categorical columns summary\n",
        "    categorical_df = df.select_dtypes(include='object')\n",
        "    if not categorical_df.empty:\n",
        "        print(\"\\n\\033[4mCategorical Columns:\\033[0m\")\n",
        "        for col in categorical_df.columns:\n",
        "            print(f\"\\n• {col}:\")\n",
        "            print(df[col].value_counts(dropna=False).head())\n",
        "\n",
        "    # ==============================================\n",
        "    # 5. EXPORT OPTIONS\n",
        "    # ==============================================\n",
        "    print(\"\\n\\033[1mEXPORT OPTIONS\\033[0m\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    try:\n",
        "        # Excel Export\n",
        "        excel_file = f\"{export_name}.xlsx\"\n",
        "        with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
        "            df.to_excel(writer, sheet_name='Data', index=False)\n",
        "\n",
        "            # Add summary sheets\n",
        "            numeric_df.describe().to_excel(writer, sheet_name='Numeric Summary')\n",
        "            if not categorical_df.empty:\n",
        "                pd.concat([\n",
        "                    df[col].value_counts(dropna=False).rename(col)\n",
        "                    for col in categorical_df.columns\n",
        "                ], axis=1).to_excel(writer, sheet_name='Category Counts')\n",
        "\n",
        "            # Get workbook objects\n",
        "            workbook = writer.book\n",
        "            worksheet = writer.sheets['Data']\n",
        "\n",
        "            # Add Excel formatting\n",
        "            header_format = workbook.add_format({\n",
        "                'bold': True,\n",
        "                'text_wrap': True,\n",
        "                'valign': 'top',\n",
        "                'fg_color': '#2a3f5f',\n",
        "                'font_color': 'white',\n",
        "                'border': 1\n",
        "            })\n",
        "\n",
        "            # Apply header format\n",
        "            for col_num, value in enumerate(df.columns.values):\n",
        "                worksheet.write(0, col_num, value, header_format)\n",
        "\n",
        "            # Auto-adjust column widths\n",
        "            for i, col in enumerate(df.columns):\n",
        "                max_len = max((\n",
        "                    df[col].astype(str).map(len).max(),  # Data length\n",
        "                    len(str(col))  # Header length\n",
        "                )) + 2\n",
        "                worksheet.set_column(i, i, min(max_len, 50))\n",
        "\n",
        "        print(f\"✓ Excel file saved as: {excel_file}\")\n",
        "\n",
        "        # HTML Export\n",
        "        html_file = f\"{export_name}.html\"\n",
        "        styler.to_html(html_file)\n",
        "        print(f\"✓ HTML file saved as: {html_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Export failed: {str(e)}\")\n",
        "\n",
        "# Usage example:\n",
        "# display_and_export_dataset(df_clean, num_rows=10, export_name=\"medical_resources\")"
      ],
      "metadata": {
        "id": "qvd-x0zHrkNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_and_export_dataset(df,\n",
        "                         num_rows=10,\n",
        "                         export_name=\"my_cleaned_data\")"
      ],
      "metadata": {
        "id": "TS2u6cTZv7Ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n"
      ],
      "metadata": {
        "id": "B1KJk5dTw7PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "def display_and_export_dataset(df, num_rows=5, export_name=\"cleaned_dataset\"):\n",
        "    \"\"\"\n",
        "    Displays the final dataset with enhanced formatting and export options\n",
        "\n",
        "    Parameters:\n",
        "    - df: Your cleaned DataFrame\n",
        "    - num_rows: Number of rows to display\n",
        "    - export_name: Base name for exported files\n",
        "    \"\"\"\n",
        "\n",
        "    # ==============================================\n",
        "    # 1. SPECIAL COLUMN FORMATTING\n",
        "    # ==============================================\n",
        "    format_rules = {\n",
        "        # Medical resource columns\n",
        "        r'(beds|patients|ventilators|staff|ppe)': '{:,.0f}',\n",
        "        # Percentage columns\n",
        "        r'(rate|ratio|percent|per_hundred)': '{:.1%}',\n",
        "        # Date columns\n",
        "        'date': '{:%Y-%m-%d}',\n",
        "        # Small decimal numbers\n",
        "        r'(growth|factor|index)': '{:.3f}'\n",
        "    }\n",
        "\n",
        "    # ==============================================\n",
        "    # 2. CREATE STYLED TABLE\n",
        "    # ==============================================\n",
        "    styler = (df.head(num_rows)\n",
        "              .style\n",
        "              .set_properties(**{'text-align': 'center'})\n",
        "              .set_table_styles([{\n",
        "                  'selector': 'th',\n",
        "                  'props': [\n",
        "                      ('background-color', '#2a3f5f'),\n",
        "                      ('color', 'white'),\n",
        "                      ('font-weight', 'bold')\n",
        "                  ]\n",
        "              }]))\n",
        "\n",
        "    # Apply formatting\n",
        "    for regex, formatter in format_rules.items():\n",
        "        cols = df.filter(regex=regex, axis=1).columns\n",
        "        if not cols.empty:\n",
        "            styler.format(formatter, subset=cols)\n",
        "\n",
        "    # Highlight medical metrics\n",
        "    medical_cols = df.filter(regex='icu|hosp|ventilator|ppe').columns\n",
        "    if not medical_cols.empty:\n",
        "        styler.background_gradient(\n",
        "            cmap='YlOrRd',\n",
        "            subset=medical_cols,\n",
        "            vmin=0, vmax=df[medical_cols].max().max()\n",
        "        )\n",
        "\n",
        "    # ==============================================\n",
        "    # 3. DISPLAY RESULTS\n",
        "    # ==============================================\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\033[1m{'CLEANED DATASET PREVIEW':^80}\\033[0m\")\n",
        "    print(\"=\"*80)\n",
        "    display(styler)\n",
        "\n",
        "    # ==============================================\n",
        "    # 4. STATISTICAL SUMMARIES\n",
        "    # ==============================================\n",
        "    print(\"\\n\\033[1mSTATISTICAL SUMMARIES\\033[0m\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # Numeric summary\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    if not numeric_df.empty:\n",
        "        print(\"\\n\\033[4mNumeric Columns:\\033[0m\")\n",
        "        display(numeric_df.describe().style.format(\"{:.2f}\"))\n",
        "\n",
        "    # Categorical summary\n",
        "    categorical_df = df.select_dtypes(include='object')\n",
        "    if not categorical_df.empty:\n",
        "        print(\"\\n\\033[4mCategorical Columns:\\033[0m\")\n",
        "        for col in categorical_df.columns:\n",
        "            print(f\"\\n• {col}:\")\n",
        "            print(df[col].value_counts(dropna=False).head())\n",
        "\n",
        "    # ==============================================\n",
        "    # 5. EXPORT OPTIONS\n",
        "    # ==============================================\n",
        "    print(\"\\n\\033[1mEXPORT OPTIONS\\033[0m\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    # HTML Export (always available)\n",
        "    html_file = f\"{export_name}.html\"\n",
        "    styler.to_html(html_file)\n",
        "    print(f\"✓ HTML file saved as: {html_file}\")\n",
        "\n",
        "    # Excel Export (only if xlsxwriter is available)\n",
        "    try:\n",
        "        import xlsxwriter\n",
        "        excel_file = f\"{export_name}.xlsx\"\n",
        "        with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:\n",
        "            df.to_excel(writer, sheet_name='Data', index=False)\n",
        "\n",
        "            # Add summary sheets\n",
        "            numeric_df.describe().to_excel(writer, sheet_name='Numeric Summary')\n",
        "            if not categorical_df.empty:\n",
        "                pd.concat([\n",
        "                    df[col].value_counts(dropna=False).rename(col)\n",
        "                    for col in categorical_df.columns\n",
        "                ], axis=1).to_excel(writer, sheet_name='Category Counts')\n",
        "\n",
        "            # Formatting\n",
        "            workbook = writer.book\n",
        "            worksheet = writer.sheets['Data']\n",
        "            header_format = workbook.add_format({\n",
        "                'bold': True,\n",
        "                'text_wrap': True,\n",
        "                'fg_color': '#2a3f5f',\n",
        "                'font_color': 'white',\n",
        "                'border': 1\n",
        "            })\n",
        "\n",
        "            for col_num, value in enumerate(df.columns.values):\n",
        "                worksheet.write(0, col_num, value, header_format)\n",
        "\n",
        "            # Auto-adjust columns\n",
        "            for i, col in enumerate(df.columns):\n",
        "                max_len = max((df[col].astype(str).map(len).max(), len(str(col)))) + 2\n",
        "                worksheet.set_column(i, i, min(max_len, 50))\n",
        "\n",
        "        print(f\"✓ Excel file saved as: {excel_file}\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ℹ️ Excel export requires xlsxwriter. Install with: !pip install xlsxwriter\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Excel export failed: {str(e)}\")\n",
        "\n",
        "# Usage example\n",
        "# display_and_export_dataset(df_clean, num_rows=10, export_name=\"medical_data\")"
      ],
      "metadata": {
        "id": "Q7zOaoQbyFlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_and_export_dataset(df,\n",
        "                         num_rows=10,\n",
        "                         export_name=\"my_medical_data\")"
      ],
      "metadata": {
        "id": "-NGbvQmJydAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your cleaned DataFrame is named 'df_clean'\n",
        "df.to_csv('cleaned_dataset.csv', index=False)  # index=False avoids saving row numbers"
      ],
      "metadata": {
        "id": "jw1k7NngLIKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation of Libraries"
      ],
      "metadata": {
        "id": "hVjBgdVJ3VyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data manipulation and analysis\n",
        "import pandas as pd  # For data handling, cleaning, and imputation\n",
        "import numpy as np  # For numerical operations and handling missing values\n",
        "\n",
        "# Data visualization for exploring data)\n",
        "import matplotlib.pyplot as plt  # For plotting distributions and trends\n",
        "import seaborn as sns  # For advanced visualizations (e.g., correlation heatmaps)\n",
        "\n",
        "# Feature engineering and preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # For normalization and standardization\n",
        "from sklearn.impute import SimpleImputer  # For imputation of missing values\n",
        "\n",
        "# Time-series handling\n",
        "from datetime import datetime  # For parsing and manipulating dates\n",
        "from pandas.tseries.offsets import Day, Week  # For creating lag and rolling features\n",
        "\n",
        "# Statistical analysis (optional, for feature selection)\n",
        "from scipy.stats import pearsonr  # For correlation analysis\n",
        "\n",
        "# Machine learning (optional, for later modeling)\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.ensemble import RandomForestRegressor  # Example model for feature importance\n",
        "from sklearn.metrics import mean_squared_error, r2_score  # For model evaluation\n",
        "\n",
        "# Suppress warnings (optional, for cleaner output)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "7-POhX_93Z_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING DATA"
      ],
      "metadata": {
        "id": "czG6-zpS32lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df =  pd.read_csv('/content/drive/MyDrive/Medical_Resource_Prediction/owid-covid-data.csv')\n",
        "data_df.sample(10)"
      ],
      "metadata": {
        "id": "WkbEmSX25XvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ANALYSIS\n",
        "#Checking for missing values & categorical variables"
      ],
      "metadata": {
        "id": "cvl_bKFX59cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values and categorical variables in the dataset\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "-32TdPsC6Gm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note: ¶\n",
        "\n",
        "# Datatype of features 'tests_units' is \"object\" which needs to be converted into numerical variable (will be done in data preprocessing) before we feed the data to algorithms."
      ],
      "metadata": {
        "id": "E5LTQ7vY6REt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gVvPgkOE7S2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive Statistics"
      ],
      "metadata": {
        "id": "Lwk7gqY-9BQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Doing Univariate Analysis for statistical description and understanding of dispersion of data\n",
        "data_df.describe().T"
      ],
      "metadata": {
        "id": "GGnrCwdP9CaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the feature \"Unnamed\"\n",
        "#data_df = data_df.drop([\"Unnamed: 0\"], axis=1)\n",
        "data_df.shape"
      ],
      "metadata": {
        "id": "2aOW1tp295CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL BUILDING\n",
        "\n"
      ],
      "metadata": {
        "id": "KNSX_m3cFB2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Independent and Dependent Variables\n",
        "\n",
        "# Based on the cleaned dataset, define:\n",
        "# 1. Independent Variables (X): Features that influence medical resource demand.\n",
        "\n",
        "# 2. Dependent Variables (y): Target variables representing resource demand (e.g., icu_patients, total_vaccinations, hosp_patients).\n",
        "\n"
      ],
      "metadata": {
        "id": "ufG9lN9gFNuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['new_cases', 'total_cases', 'new_deaths', 'total_deaths', 'positive_rate', 'tests_per_case',\n",
        "           'new_tests', 'hosp_patients', 'weekly_hosp_admissions', 'total_vaccinations', 'new_vaccinations',\n",
        "           'people_vaccinated', 'people_fully_vaccinated', 'total_boosters', 'new_people_vaccinated_smoothed',\n",
        "           'stringency_index', 'population', 'population_density', 'median_age', 'aged_65_older',\n",
        "           'aged_70_older', 'gdp_per_capita', 'hospital_beds_per_thousand', 'life_expectancy',\n",
        "           'human_development_index', 'new_cases_lag1', 'icu_patients_roll7', 'day_of_week', 'month',\n",
        "           'is_weekend', 'continent_Africa', 'continent_Asia', 'continent_Australia', 'continent_Europe',\n",
        "           'continent_North America', 'continent_South America']\n",
        "\n"
      ],
      "metadata": {
        "id": "u31-L0FbFALM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  List of Pipelines and Dictionary\n",
        "\n"
      ],
      "metadata": {
        "id": "P-7Wvl6aIfbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of pipelines and a dictionary for easy reference"
      ],
      "metadata": {
        "id": "ri6Va7BJIzfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n"
      ],
      "metadata": {
        "id": "zVfqlMLdIgnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline Creation\n",
        "\n"
      ],
      "metadata": {
        "id": "34vNE_MmJDoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define pipelines\n",
        "pipelines = [\n",
        "    Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', LinearRegression())\n",
        "    ]),\n",
        "    Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
        "    ]),\n",
        "    Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', GradientBoostingRegressor(random_state=42))\n",
        "    ]),\n",
        "    Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', XGBRegressor(random_state=42, n_jobs=-1))\n",
        "    ]),\n",
        "    Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', SVR())\n",
        "    ])\n",
        "]\n",
        "\n",
        "# Dictionary for reference\n",
        "pipeline_dict = {\n",
        "    'LinearRegression': pipelines[0],\n",
        "    'RandomForest': pipelines[1],\n",
        "    'GradientBoosting': pipelines[2],\n",
        "    'XGBoost': pipelines[3],\n",
        "    'SVR': pipelines[4]\n",
        "}"
      ],
      "metadata": {
        "id": "iposeo38JF7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit the Pipelines\n",
        "\n",
        "# Load the cleaned dataset.\n",
        "\n",
        "# Split the data into training and test sets.\n",
        "\n",
        "# Fit each pipeline on the training data.\n",
        "\n",
        "# Evaluate performance using RMSE on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "ipke_NECJvBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List all files in /content\n",
        "print(\"Files in /content:\")\n",
        "print(os.listdir('/content'))\n",
        "\n",
        "# Alternative for Colab:\n",
        "!ls /content"
      ],
      "metadata": {
        "id": "r_k0RyLLLr8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try these common variations\n",
        "possible_paths = [\n",
        "    '/content/cleaned_dataset.csv',  # Default name from earlier\n",
        "    '/content/drive/MyDrive/cleaned_data.csv',  # Common Google Drive path\n",
        "    'cleaned_data.csv',  # Current working directory\n",
        "    './cleaned_data.csv'  # Explicit current directory\n",
        "]\n",
        "\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        df_clean = pd.read_csv(path)\n",
        "        print(f\"Successfully loaded from {path}\")\n",
        "        break\n",
        "else:\n",
        "    print(\"File not found in common locations\")"
      ],
      "metadata": {
        "id": "KVg1Na8QNDKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Try loading from Drive\n",
        "drive_path = '/content/drive/MyDrive/cleaned_data.csv'  # Adjust folder if needed\n",
        "if os.path.exists(drive_path):\n",
        "    df_clean = pd.read_csv(drive_path)\n",
        "else:\n",
        "    print(f\"File not found at {drive_path}\")\n",
        "    print(\"Saving current data to Drive...\")\n",
        "    df_clean.to_csv(drive_path, index=False)"
      ],
      "metadata": {
        "id": "I_93-kfPNr5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your cleaned DataFrame first\n",
        "df_clean.to_csv('/content/cleaned_data.csv', index=False)\n",
        "print(\"File saved successfully\")\n",
        "\n",
        "# Now load it\n",
        "df_clean= pd.read_csv('/content/cleaned_data.csv')"
      ],
      "metadata": {
        "id": "A3T6IaShNMpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 1. Create sample cleaned data (if you don't have df_clean yet)\n",
        "data = {'patient_id': [1, 2, 3], 'cases': [100, 150, 200]}\n",
        "df_clean = pd.DataFrame(data)\n",
        "\n",
        "# 2. Save to CSV\n",
        "save_path = '/content/cleaned_data.csv'\n",
        "df_clean.to_csv(save_path, index=False)\n",
        "print(f\"Saved to {save_path}\")\n",
        "\n",
        "# 3. Verify file exists\n",
        "if os.path.exists(save_path):\n",
        "    print(\"File verification:\")\n",
        "    print(pd.read_csv(save_path).head())\n",
        "else:\n",
        "    print(\"❗ File not found - check permissions\")"
      ],
      "metadata": {
        "id": "t7Gf3H0uON-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load cleaned dataset\n",
        "df_clean = pd.read_csv('/content/cleaned_data.csv')\n",
        "\n",
        "# Print available columns to diagnose missing features\n",
        "print(\"Available columns in df_clean:\\n\", df_clean.columns.tolist())\n",
        "\n",
        "# Define pipelines (as before)\n",
        "pipelines = [\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=42, n_jobs=-1))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', GradientBoostingRegressor(random_state=42))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', XGBRegressor(random_state=42, n_jobs=-1))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', SVR())])\n",
        "]\n",
        "pipeline_dict = {\n",
        "    'LinearRegression': pipelines[0],\n",
        "    'RandomForest': pipelines[1],\n",
        "    'GradientBoosting': pipelines[2],\n",
        "    'XGBoost': pipelines[3],\n",
        "    'SVR': pipelines[4]\n",
        "}\n",
        "\n",
        "# Feature engineering (to ensure missing features are created)\n",
        "# Convert date to datetime (if not already done)\n",
        "if 'date' in df_clean.columns:\n",
        "    df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
        "    df_clean['day_of_week'] = df_clean['date'].dt.dayofweek\n",
        "    df_clean['month'] = df_clean['date'].dt.month\n",
        "    df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "# Create lag and rolling features\n",
        "if 'new_cases' in df_clean.columns and 'location' in df_clean.columns:\n",
        "    df_clean['new_cases_lag1'] = df_clean.groupby('location')['new_cases'].shift(1)\n",
        "if 'icu_patients' in df_clean.columns and 'location' in df_clean.columns:\n",
        "    df_clean['icu_patients_roll7'] = df_clean.groupby('location')['icu_patients'].rolling(window=7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "\n",
        "# One-hot encode continent (if not already done)\n",
        "if 'continent' in df_clean.columns:\n",
        "    df_clean = pd.get_dummies(df_clean, columns=['continent'], prefix='continent')\n",
        "\n",
        "# Define features (adjusted to available columns)\n",
        "available_features = [col for col in [\n",
        "    'new_cases', 'total_cases', 'new_deaths', 'total_deaths', 'positive_rate', 'tests_per_case',\n",
        "    'new_tests', 'hosp_patients', 'weekly_hosp_admissions', 'total_vaccinations', 'new_vaccinations',\n",
        "    'people_vaccinated', 'people_fully_vaccinated', 'total_boosters', 'new_people_vaccinated_smoothed',\n",
        "    'stringency_index', 'population', 'population_density', 'median_age', 'aged_65_older',\n",
        "    'aged_70_older', 'gdp_per_capita', 'hospital_beds_per_thousand', 'life_expectancy',\n",
        "    'human_development_index', 'new_cases_lag1', 'icu_patients_roll7', 'day_of_week', 'month',\n",
        "    'is_weekend', 'continent_Africa', 'continent_Asia', 'continent_Australia', 'continent_Europe',\n",
        "    'continent_North America', 'continent_South America'\n",
        "] if col in df_clean.columns]\n",
        "\n",
        "target = 'icu_patients'\n",
        "\n",
        "# Check if target exists\n",
        "if target not in df_clean.columns:\n",
        "    raise ValueError(f\"Target column '{target}' not found in the dataset.\")\n",
        "\n",
        "# Split data\n",
        "X = df_clean[available_features]\n",
        "y = df_clean[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle any remaining missing values in X\n",
        "X_train = X_train.fillna(X_train.median())\n",
        "X_test = X_test.fillna(X_train.median())  # Use train median to avoid leakage\n",
        "y_train = y_train.fillna(y_train.median())  # Impute target if needed\n",
        "y_test = y_test.fillna(y_train.median())\n",
        "\n",
        "# Fit pipelines and evaluate\n",
        "rmse_scores = {}\n",
        "for name, pipeline in pipeline_dict.items():\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Calculate RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    rmse_scores[name] = rmse\n",
        "    print(f'{name} RMSE: {rmse:.4f}')\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = min(rmse_scores, key=rmse_scores.get)\n",
        "best_rmse = rmse_scores[best_model_name]\n",
        "print(f'Best model: {best_model_name} with RMSE: {best_rmse:.4f}')\n",
        "\n",
        "# Use the best pipeline for predictions\n",
        "best_pipeline = pipeline_dict[best_model_name]\n",
        "y_pred_best = best_pipeline.predict(X_test)\n",
        "\n",
        "# Save predictions\n",
        "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best})\n",
        "predictions.to_csv('icu_patients_predictions.csv', index=False)\n",
        "\n",
        "# Feature importance (for tree-based models)\n",
        "if best_model_name in ['RandomForest', 'GradientBoosting', 'XGBoost']:\n",
        "    model = best_pipeline.named_steps['model']\n",
        "    importances = pd.DataFrame({'Feature': available_features, 'Importance': model.feature_importances_})\n",
        "    importances = importances.sort_values(by='Importance', ascending=False)\n",
        "    print('Feature Importances:\\n', importances)\n",
        "\n",
        "# Save the best model\n",
        "import joblib\n",
        "joblib.dump(best_pipeline, 'best_icu_patients_model.pkl')"
      ],
      "metadata": {
        "id": "yeQWWdDmJ8I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Try loading the correct dataset\n",
        "try:\n",
        "    df_clean = pd.read_csv('/content/cleaned_dataset.csv')  # Adjust path if needed\n",
        "    print(\"Available columns in df_clean:\\n\", df_clean.columns.tolist())\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file path or name.\")"
      ],
      "metadata": {
        "id": "q_UjBXp-PzWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (only those present in df_clean)\n",
        "available_features = [col for col in [\n",
        "    'new_cases', 'total_cases', 'new_deaths', 'total_deaths', 'positive_rate', 'tests_per_case',\n",
        "    'new_tests', 'hosp_patients', 'weekly_hosp_admissions', 'total_vaccinations', 'new_vaccinations',\n",
        "    'people_vaccinated', 'people_fully_vaccinated', 'total_boosters', 'new_people_vaccinated_smoothed',\n",
        "    'stringency_index', 'population', 'population_density', 'median_age', 'aged_65_older',\n",
        "    'aged_70_older', 'gdp_per_capita', 'hospital_beds_per_thousand', 'life_expectancy',\n",
        "    'human_development_index', 'new_cases_lag1', 'icu_patients_roll7', 'day_of_week', 'month',\n",
        "    'is_weekend', 'continent_Africa', 'continent_Asia', 'continent_Australia', 'continent_Europe',\n",
        "    'continent_North America', 'continent_South America'\n",
        "] if col in df_clean.columns]\n",
        "\n",
        "target = 'icu_patients'\n",
        "\n",
        "# Proceed with model building (as in previous response)"
      ],
      "metadata": {
        "id": "Ta4l4h45QZl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = pd.read_csv('/content/cleaned_dataset.csv')\n",
        "print(\"Available columns in df_clean:\\n\", df_clean.columns.tolist())"
      ],
      "metadata": {
        "id": "pYG7ItudQz-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.info()"
      ],
      "metadata": {
        "id": "Xi49T1IQRL40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n",
        "\n"
      ],
      "metadata": {
        "id": "Xh3CGNCaqJFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "import joblib\n",
        "\n",
        "# Load cleaned dataset\n",
        "#df_clean = pd.read_csv('/content/cleaned_medical_resource_dataset.csv')\n",
        "df_clean = pd.read_csv('/content/cleaned_dataset.csv')\n",
        "# Define features and target\n",
        "available_features = [col for col in [\n",
        "    'new_cases', 'total_cases', 'new_deaths', 'total_deaths', 'positive_rate', 'tests_per_case',\n",
        "    'new_tests', 'hosp_patients', 'weekly_hosp_admissions', 'total_vaccinations', 'new_vacciliations',\n",
        "    'people_vaccinated', 'people_fully_vaccinated', 'total_boosters', 'new_people_vaccinated_smoothed',\n",
        "    'stringency_index', 'population', 'population_density', 'median_age', 'aged_65_older',\n",
        "    'aged_70_older', 'gdp_per_capita', 'hospital_beds_per_thousand', 'life_expectancy',\n",
        "    'human_development_index', 'new_cases_lag1', 'icu_patients_roll7', 'day_of_week', 'month',\n",
        "    'is_weekend', 'continent_Africa', 'continent_Asia', 'continent_Australia', 'continent_Europe',\n",
        "    'continent_North America', 'continent_South America'\n",
        "] if col in df_clean.columns]\n",
        "\n",
        "target = 'icu_patients'\n",
        "\n",
        "# Check if target exists\n",
        "if target not in df_clean.columns:\n",
        "    raise ValueError(f\"Target column '{target}' not found in the dataset.\")\n",
        "\n",
        "# Split data\n",
        "X = df_clean[available_features]\n",
        "y = df_clean[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Handle missing values\n",
        "X_train = X_train.fillna(X_train.median())\n",
        "X_test = X_test.fillna(X_train.median())\n",
        "y_train = y_train.fillna(y_train.median())\n",
        "y_test = y_test.fillna(y_train.median())\n",
        "\n",
        "# Define pipelines\n",
        "pipelines = [\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor(random_state=42, n_jobs=-1))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', GradientBoostingRegressor(random_state=42))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', XGBRegressor(random_state=42, n_jobs=-1))]),\n",
        "    Pipeline([('scaler', StandardScaler()), ('model', SVR())])\n",
        "]\n",
        "pipeline_dict = {\n",
        "    'LinearRegression': pipelines[0],\n",
        "    'RandomForest': pipelines[1],\n",
        "    'GradientBoosting': pipelines[2],\n",
        "    'XGBoost': pipelines[3],\n",
        "    'SVR': pipelines[4]\n",
        "}\n",
        "\n",
        "# Fit pipelines and evaluate\n",
        "rmse_scores = {}\n",
        "for name, pipeline in pipeline_dict.items():\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    rmse_scores[name] = rmse\n",
        "    print(f'{name} RMSE: {rmse:.4f}')\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = min(rmse_scores, key=rmse_scores.get)\n",
        "best_rmse = rmse_scores[best_model_name]\n",
        "print(f'Best model: {best_model_name} with RMSE: {best_rmse:.4f}')\n",
        "\n",
        "# Use the best pipeline for predictions\n",
        "best_pipeline = pipeline_dict[best_model_name]\n",
        "y_pred_best = best_pipeline.predict(X_test)\n",
        "\n",
        "# Save predictions\n",
        "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_best})\n",
        "predictions.to_csv('icu_patients_predictions.csv', index=False)\n",
        "\n",
        "# Feature importance (for tree-based models)\n",
        "if best_model_name in ['RandomForest', 'GradientBoosting', 'XGBoost']:\n",
        "    model = best_pipeline.named_steps['model']\n",
        "    importances = pd.DataFrame({'Feature': available_features, 'Importance': model.feature_importances_})\n",
        "    importances = importances.sort_values(by='Importance', ascending=False)\n",
        "    print('Feature Importances:\\n', importances)\n",
        "\n",
        "# Save the best model\n",
        "joblib.dump(best_pipeline, 'best_icu_patients_model.pkl')"
      ],
      "metadata": {
        "id": "JE2D9ajaRjFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VHfJGTPo2vtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv\")\n",
        "\n",
        "# Load Excel\n",
        "#df = pd.read_excel(\"/content/drive/MyDrive/your_folder/data.xlsx\")"
      ],
      "metadata": {
        "id": "KD1Peb0G2_5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Implemetation"
      ],
      "metadata": {
        "id": "Gcvy_-r81IaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Implementation for Medical Resource Demand Prediction\n",
        "# This code assumes pre-processed data is available in the format specified\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. Load the pre-processed data\n",
        "# -------------------------------------------------------------------------\n",
        "def load_preprocessed_data(file_path='/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'):\n",
        "    \"\"\"\n",
        "    Load the pre-processed dataset from a CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "  df_clean = pd.read_csv(\"/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv\")\n",
        "\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "\n",
        "        # Convert date column to datetime if it exists\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. Prepare data for time series modeling\n",
        "# -------------------------------------------------------------------------\n",
        "def prepare_time_series_data(df, target_cols, feature_cols=None, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Prepare data for time series modeling by creating appropriate train/test splits\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the pre-processed data\n",
        "    - target_cols: List of target columns (resource demands to predict)\n",
        "    - feature_cols: List of feature columns to use (if None, all non-target columns except date)\n",
        "    - test_size: Proportion of data to use for testing\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "    \"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'date' column for time series splitting\")\n",
        "\n",
        "    # If feature columns not specified, use all columns except targets and date\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [col for col in df.columns if col not in target_cols and col != 'date']\n",
        "\n",
        "    # Get the cutoff point for time-based split\n",
        "    split_idx = int(len(df) * (1 - test_size))\n",
        "\n",
        "    # Create train/test splits preserving time order\n",
        "    train_df = df.iloc[:split_idx]\n",
        "    test_df = df.iloc[split_idx:]\n",
        "\n",
        "    # Extract features and targets\n",
        "    X_train = train_df[feature_cols]\n",
        "    X_test = test_df[feature_cols]\n",
        "\n",
        "    # For multiple targets, create a dictionary of y values\n",
        "    y_train = {target: train_df[target] for target in target_cols}\n",
        "    y_test = {target: test_df[target] for target in target_cols}\n",
        "\n",
        "    # Save dates for plotting\n",
        "    dates_train = train_df['date'] if 'date' in train_df.columns else None\n",
        "    dates_test = test_df['date'] if 'date' in test_df.columns else None\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test, feature_cols\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. Implement Random Forest model\n",
        "# -------------------------------------------------------------------------\n",
        "def build_random_forest_model(X_train, y_train, target_name, optimize=True):\n",
        "    \"\"\"\n",
        "    Build and train a Random Forest model with optional hyperparameter optimization\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: Training features\n",
        "    - y_train: Training target values (single target)\n",
        "    - target_name: Name of the target variable (for logging)\n",
        "    - optimize: Whether to perform hyperparameter optimization\n",
        "\n",
        "    Returns:\n",
        "    - Trained model\n",
        "    \"\"\"\n",
        "    print(f\"\\nBuilding Random Forest model for {target_name}...\")\n",
        "\n",
        "    if optimize:\n",
        "        # Define parameter grid for hyperparameter tuning\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [None, 10, 20, 30],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['auto', 'sqrt']\n",
        "        }\n",
        "\n",
        "        # Use TimeSeriesSplit for cross-validation to respect temporal order\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "        # Initialize Random Forest model\n",
        "        rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        # Perform grid search with time series cross-validation\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=rf,\n",
        "            param_grid=param_grid,\n",
        "            cv=tscv,\n",
        "            scoring='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Fit the grid search to find optimal parameters\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Get the best model\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        print(f\"Best parameters for {target_name}: {grid_search.best_params_}\")\n",
        "        print(f\"Best CV score: {-grid_search.best_score_:.4f} MSE\")\n",
        "\n",
        "        return best_model\n",
        "    else:\n",
        "        # Use default parameters\n",
        "        rf = RandomForestRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=None,\n",
        "            min_samples_split=2,\n",
        "            min_samples_leaf=1,\n",
        "            max_features='auto',\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        rf.fit(X_train, y_train)\n",
        "        return rf\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4. Evaluate model performance\n",
        "# -------------------------------------------------------------------------\n",
        "def evaluate_model(model, X_test, y_test, target_name):\n",
        "    \"\"\"\n",
        "    Evaluate the model using multiple metrics\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model\n",
        "    - X_test: Test features\n",
        "    - y_test: Test target values\n",
        "    - target_name: Name of the target variable\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(np.abs(y_test), 1))) * 100\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nPerformance metrics for {target_name}:\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "    print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "    metrics = {\n",
        "        'target': target_name,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "    return metrics, y_pred\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5. Feature importance analysis\n",
        "# -------------------------------------------------------------------------\n",
        "def analyze_feature_importance(model, feature_names, target_name):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained Random Forest model\n",
        "    - feature_names: List of feature names\n",
        "    - target_name: Name of the target variable\n",
        "    \"\"\"\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Sort features by importance\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Create DataFrame for better visualization\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': [feature_names[i] for i in indices],\n",
        "        'Importance': [importances[i] for i in indices]\n",
        "    })\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
        "    plt.title(f'Top 20 Feature Importances for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'feature_importance_{target_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nTop 10 important features for {target_name}:\")\n",
        "    for i, feature in enumerate(importance_df['Feature'].head(10)):\n",
        "        print(f\"{i+1}. {feature} ({importance_df['Importance'].iloc[i]:.4f})\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6. Visualize predictions vs actual values\n",
        "# -------------------------------------------------------------------------\n",
        "def visualize_predictions(y_test, y_pred, dates_test, target_name):\n",
        "    \"\"\"\n",
        "    Visualize predictions vs actual values\n",
        "\n",
        "    Parameters:\n",
        "    - y_test: Actual values\n",
        "    - y_pred: Predicted values\n",
        "    - dates_test: Dates corresponding to test data\n",
        "    - target_name: Name of the target variable\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    if dates_test is not None:\n",
        "        plt.plot(dates_test, y_test, label='Actual', marker='o', linestyle='-', alpha=0.7)\n",
        "        plt.plot(dates_test, y_pred, label='Predicted', marker='x', linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Date')\n",
        "    else:\n",
        "        plt.plot(y_test.index, y_test, label='Actual', marker='o', linestyle='-', alpha=0.7)\n",
        "        plt.plot(y_test.index, y_pred, label='Predicted', marker='x', linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Index')\n",
        "\n",
        "    plt.ylabel(target_name)\n",
        "    plt.title(f'Random Forest Predictions vs Actual Values: {target_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'predictions_vs_actual_{target_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot residuals\n",
        "    residuals = y_test - y_pred\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    if dates_test is not None:\n",
        "        plt.scatter(dates_test, residuals, alpha=0.7)\n",
        "        plt.axhline(y=0, color='r', linestyle='-')\n",
        "        plt.xlabel('Date')\n",
        "    else:\n",
        "        plt.scatter(range(len(residuals)), residuals, alpha=0.7)\n",
        "        plt.axhline(y=0, color='r', linestyle='-')\n",
        "        plt.xlabel('Index')\n",
        "\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title(f'Residual Analysis: {target_name}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'residuals_{target_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 7. Save the trained model\n",
        "# -------------------------------------------------------------------------\n",
        "def save_model(model, target_name):\n",
        "    \"\"\"\n",
        "    Save the trained model to disk\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model\n",
        "    - target_name: Name of the target variable\n",
        "    \"\"\"\n",
        "    model_filename = f\"random_forest_model_{target_name.replace(' ', '_').lower()}.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"\\nModel saved as {model_filename}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 8. Create forecast for future periods\n",
        "# -------------------------------------------------------------------------\n",
        "def create_forecast(model, X_test, feature_cols, dates_test, days_to_forecast=30):\n",
        "    \"\"\"\n",
        "    Create a forecast for future periods\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model\n",
        "    - X_test: Latest features\n",
        "    - feature_cols: List of feature names\n",
        "    - dates_test: Latest dates\n",
        "    - days_to_forecast: Number of days to forecast\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with forecasted values\n",
        "    \"\"\"\n",
        "    # Get the last date in the test set\n",
        "    last_date = dates_test.iloc[-1]\n",
        "\n",
        "    # Initialize a list to store forecasted values\n",
        "    forecast_dates = [last_date + timedelta(days=i+1) for i in range(days_to_forecast)]\n",
        "    forecasted_values = []\n",
        "\n",
        "    # Get the last row of actual data to start forecasting from\n",
        "    latest_data = X_test.iloc[-1:].copy()\n",
        "\n",
        "    # Create forecast for each day\n",
        "    for i in range(days_to_forecast):\n",
        "        # Predict for current data point\n",
        "        pred = model.predict(latest_data)[0]\n",
        "        forecasted_values.append(pred)\n",
        "\n",
        "        # Update features for next prediction (this is simplified and would need\n",
        "        # to be adapted based on your actual feature engineering process)\n",
        "        latest_data = latest_data.copy()\n",
        "\n",
        "        # Example: Update lag features if they exist\n",
        "        for col in feature_cols:\n",
        "            if 'lag_1_' in col:\n",
        "                original_col = col.replace('lag_1_', '')\n",
        "                lag_2_col = f'lag_2_{original_col}'\n",
        "\n",
        "                if lag_2_col in feature_cols:\n",
        "                    latest_data[lag_2_col] = latest_data[col]\n",
        "                latest_data[col] = pred\n",
        "\n",
        "            # Update other time-based features as needed\n",
        "            # This is highly dependent on your feature engineering process\n",
        "\n",
        "    # Create DataFrame with forecasted values\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'date': forecast_dates,\n",
        "        'forecasted_value': forecasted_values\n",
        "    })\n",
        "\n",
        "    # Plot forecast\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot historical data\n",
        "    plt.plot(dates_test, y_test, label='Historical', marker='o', linestyle='-', alpha=0.7)\n",
        "\n",
        "    # Plot forecast\n",
        "    plt.plot(forecast_df['date'], forecast_df['forecasted_value'],\n",
        "             label='Forecast', marker='x', linestyle='--', color='red', alpha=0.7)\n",
        "\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(target_name)\n",
        "    plt.title(f'Random Forest Forecast: {target_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'forecast_{target_name}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return forecast_df\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 9. Main function to run the full pipeline\n",
        "# -------------------------------------------------------------------------\n",
        "def run_random_forest_pipeline(data_path, target_cols, optimize=True):\n",
        "    \"\"\"\n",
        "    Run the complete Random Forest pipeline\n",
        "\n",
        "    Parameters:\n",
        "    - data_path: Path to pre-processed data\n",
        "    - target_cols: List of target columns (resource demands to predict)\n",
        "    - optimize: Whether to perform hyperparameter optimization\n",
        "\n",
        "    Returns:\n",
        "    - Dictionary of trained models and evaluation metrics\n",
        "    \"\"\"\n",
        "    # Load pre-processed data\n",
        "    df = load_preprocessed_data(data_path)\n",
        "    if df is None:\n",
        "        return None\n",
        "\n",
        "    # Prepare data for time series modeling\n",
        "    X_train, X_test, y_train, y_test, dates_train, dates_test, feature_cols = prepare_time_series_data(\n",
        "        df, target_cols=target_cols\n",
        "    )\n",
        "\n",
        "    # Dictionary to store models and metrics\n",
        "    results = {}\n",
        "\n",
        "    # Train and evaluate models for each target\n",
        "    for target in target_cols:\n",
        "        # Build and train model\n",
        "        model = build_random_forest_model(\n",
        "            X_train, y_train[target], target, optimize=optimize\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        metrics, y_pred = evaluate_model(model, X_test, y_test[target], target)\n",
        "\n",
        "        # Analyze feature importance\n",
        "        importance_df = analyze_feature_importance(model, feature_cols, target)\n",
        "\n",
        "        # Visualize predictions\n",
        "        visualize_predictions(y_test[target], y_pred, dates_test, target)\n",
        "\n",
        "        # Save model\n",
        "        save_model(model, target)\n",
        "\n",
        "        # Create forecast (optional)\n",
        "        # forecast_df = create_forecast(model, X_test, feature_cols, dates_test)\n",
        "\n",
        "        # Store results\n",
        "        results[target] = {\n",
        "            'model': model,\n",
        "            'metrics': metrics,\n",
        "            'importance': importance_df,\n",
        "            'predictions': y_pred\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# Example usage\n",
        "# -------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Define target variables (medical resources to predict)\n",
        "    target_cols = [\n",
        "        'hospital_beds_required',\n",
        "        'icu_beds_required',\n",
        "        'ventilators_required',\n",
        "        'ppe_daily_consumption',\n",
        "        'staff_hours_required'\n",
        "    ]\n",
        "\n",
        "    # Run the pipeline\n",
        "    results = run_random_forest_pipeline(\n",
        "        data_path='preprocessed_medical_data.csv',\n",
        "        target_cols=target_cols,\n",
        "        optimize=True  # Set to False for faster execution with default parameters\n",
        "    )\n",
        "\n",
        "    # Summary of results\n",
        "    if results:\n",
        "        print(\"\\n=== SUMMARY OF RANDOM FOREST MODEL PERFORMANCE ===\")\n",
        "        summary_metrics = []\n",
        "\n",
        "        for target, result in results.items():\n",
        "            metrics = result['metrics']\n",
        "            summary_metrics.append(metrics)\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        summary_df = pd.DataFrame(summary_metrics)\n",
        "        print(summary_df)\n",
        "\n",
        "        # Save summary to CSV\n",
        "        summary_df.to_csv('random_forest_performance_summary.csv', index=False)\n",
        "        print(\"Summary saved to random_forest_performance_summary.csv\")"
      ],
      "metadata": {
        "id": "DqiSgxiC1HYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load the pre-processed data\n",
        "df_clean = pd.read_csv(\"/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset shape:\", df_clean.shape)\n",
        "print(\"\\nDataset columns:\", df_clean.columns.tolist())\n",
        "print(\"\\nData types:\")\n",
        "print(df_clean.dtypes)\n",
        "print(\"\\nSummary statistics:\")\n",
        "print(df_clean.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# Convert date column to datetime if it exists\n",
        "if 'date' in df_clean.columns:\n",
        "    df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
        "    df_clean = df_clean.sort_values('date')\n",
        "    print(\"\\nDate range:\", df_clean['date'].min(), \"to\", df_clean['date'].max())\n",
        "\n",
        "# Define target variables (adjust based on your specific dataset)\n",
        "target_cols = [\n",
        "    'hospital_beds_required',\n",
        "    'icu_beds_required',\n",
        "    'ventilators_required',\n",
        "    'ppe_daily_consumption',\n",
        "    'medical_staff_required'\n",
        "]\n",
        "\n",
        "# Check if target columns exist in the dataset\n",
        "available_targets = [col for col in target_cols if col in df_clean.columns]\n",
        "if not available_targets:\n",
        "    print(\"\\nWARNING: None of the specified target columns found in the dataset!\")\n",
        "    print(\"Available columns:\", df_clean.columns.tolist())\n",
        "    # You might want to manually define your target column here\n",
        "    # For example: target_cols = ['actual_target_column_name']\n",
        "else:\n",
        "    print(\"\\nTarget variables found:\", available_targets)\n",
        "    target_cols = available_targets\n",
        "\n",
        "# Define feature columns (all columns except targets and non-feature columns)\n",
        "exclude_cols = target_cols + ['date'] if 'date' in df_clean.columns else target_cols\n",
        "feature_cols = [col for col in df_clean.columns if col not in exclude_cols]\n",
        "\n",
        "print(\"\\nFeature columns:\", len(feature_cols))\n",
        "print(\"Target columns:\", len(target_cols))\n",
        "\n",
        "# Function to prepare data for time series modeling\n",
        "def prepare_time_series_data(df, target_cols, feature_cols, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Prepare data for time series modeling by creating appropriate train/test splits\n",
        "    \"\"\"\n",
        "    # Get the cutoff point for time-based split\n",
        "    if 'date' in df.columns:\n",
        "        # Sort by date to ensure chronological order\n",
        "        df = df.sort_values('date')\n",
        "        split_idx = int(len(df) * (1 - test_size))\n",
        "        train_df = df.iloc[:split_idx]\n",
        "        test_df = df.iloc[split_idx:]\n",
        "        dates_train = train_df['date'] if 'date' in train_df.columns else None\n",
        "        dates_test = test_df['date'] if 'date' in test_df.columns else None\n",
        "    else:\n",
        "        # If no date column, use simple indices\n",
        "        split_idx = int(len(df) * (1 - test_size))\n",
        "        train_df = df.iloc[:split_idx]\n",
        "        test_df = df.iloc[split_idx:]\n",
        "        dates_train = None\n",
        "        dates_test = None\n",
        "\n",
        "    # Extract features and targets\n",
        "    X_train = train_df[feature_cols]\n",
        "    X_test = test_df[feature_cols]\n",
        "\n",
        "    # For multiple targets, create a dictionary of y values\n",
        "    y_train = {target: train_df[target] for target in target_cols}\n",
        "    y_test = {target: test_df[target] for target in target_cols}\n",
        "\n",
        "    print(f\"Training data shape: {X_train.shape}\")\n",
        "    print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "\n",
        "# Prepare the data\n",
        "X_train, X_test, y_train, y_test, dates_train, dates_test = prepare_time_series_data(\n",
        "    df_clean, target_cols, feature_cols\n",
        ")\n",
        "\n",
        "# Function to build and train a Random Forest model\n",
        "def build_random_forest_model(X_train, y_train, target_name, n_estimators=200, max_depth=None):\n",
        "    \"\"\"\n",
        "    Build and train a Random Forest model\n",
        "    \"\"\"\n",
        "    print(f\"\\nBuilding Random Forest model for {target_name}...\")\n",
        "\n",
        "    # Initialize Random Forest model with specified parameters\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        max_features='auto',\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(f\"Model for {target_name} trained successfully.\")\n",
        "\n",
        "    return rf\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, X_test, y_test, target_name):\n",
        "    \"\"\"\n",
        "    Evaluate the model using multiple metrics\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Calculate MAPE with handling for zero values\n",
        "    epsilon = 1e-10  # Small value to prevent division by zero\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / (np.abs(y_test) + epsilon))) * 100\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"\\nPerformance metrics for {target_name}:\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "    print(f\"MAPE: {mape:.2f}%\")\n",
        "\n",
        "    metrics = {\n",
        "        'target': target_name,\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'mape': mape\n",
        "    }\n",
        "\n",
        "    return metrics, y_pred\n",
        "\n",
        "# Function to analyze feature importance\n",
        "def analyze_feature_importance(model, feature_cols, target_name):\n",
        "    \"\"\"\n",
        "    Analyze and visualize feature importance\n",
        "    \"\"\"\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Sort features by importance\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Create DataFrame for better visualization\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': [feature_cols[i] for i in indices],\n",
        "        'Importance': [importances[i] for i in indices]\n",
        "    })\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
        "    plt.title(f'Top 20 Feature Importances for {target_name}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'feature_importance_{target_name}.png')\n",
        "\n",
        "    print(f\"\\nTop 10 important features for {target_name}:\")\n",
        "    for i, feature in enumerate(importance_df['Feature'].head(10)):\n",
        "        print(f\"{i+1}. {feature} ({importance_df['Importance'].iloc[i]:.4f})\")\n",
        "\n",
        "    return importance_df\n",
        "\n",
        "# Function to visualize predictions vs actual values\n",
        "def visualize_predictions(y_test, y_pred, dates_test, target_name):\n",
        "    \"\"\"\n",
        "    Visualize predictions vs actual values\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Plot actual vs predicted\n",
        "    if dates_test is not None:\n",
        "        plt.plot(dates_test, y_test.values, label='Actual', marker='o', linestyle='-', alpha=0.7)\n",
        "        plt.plot(dates_test, y_pred, label='Predicted', marker='x', linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Date')\n",
        "    else:\n",
        "        plt.plot(y_test.index, y_test.values, label='Actual', marker='o', linestyle='-', alpha=0.7)\n",
        "        plt.plot(y_test.index, y_pred, label='Predicted', marker='x', linestyle='--', alpha=0.7)\n",
        "        plt.xlabel('Index')\n",
        "\n",
        "    plt.ylabel(target_name)\n",
        "    plt.title(f'Random Forest Predictions vs Actual Values: {target_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'predictions_vs_actual_{target_name}.png')\n",
        "\n",
        "    # Plot residuals\n",
        "    residuals = y_test.values - y_pred\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    if dates_test is not None:\n",
        "        plt.scatter(dates_test, residuals, alpha=0.7)\n",
        "        plt.axhline(y=0, color='r', linestyle='-')\n",
        "        plt.xlabel('Date')\n",
        "    else:\n",
        "        plt.scatter(range(len(residuals)), residuals, alpha=0.7)\n",
        "        plt.axhline(y=0, color='r', linestyle='-')\n",
        "        plt.xlabel('Index')\n",
        "\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title(f'Residual Analysis: {target_name}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'residuals_{target_name}.png')\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Train and evaluate models for each target variable\n",
        "for target in target_cols:\n",
        "    # Build and train model\n",
        "    model = build_random_forest_model(\n",
        "        X_train,\n",
        "        y_train[target],\n",
        "        target,\n",
        "        n_estimators=200,  # You can adjust these parameters\n",
        "        max_depth=None\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics, y_pred = evaluate_model(model, X_test, y_test[target], target)\n",
        "\n",
        "    # Analyze feature importance\n",
        "    importance_df = analyze_feature_importance(model, feature_cols, target)\n",
        "\n",
        "    # Visualize predictions\n",
        "    visualize_predictions(y_test[target], y_pred, dates_test, target)\n",
        "\n",
        "    # Save model\n",
        "    model_filename = f\"random_forest_model_{target.replace(' ', '_').lower()}.pkl\"\n",
        "    joblib.dump(model, model_filename)\n",
        "    print(f\"\\nModel saved as {model_filename}\")\n",
        "\n",
        "    # Store results\n",
        "    results[target] = {\n",
        "        'model': model,\n",
        "        'metrics': metrics,\n",
        "        'importance': importance_df,\n",
        "        'predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Create summary of results\n",
        "print(\"\\n=== SUMMARY OF RANDOM FOREST MODEL PERFORMANCE ===\")\n",
        "summary_metrics = []\n",
        "\n",
        "for target, result in results.items():\n",
        "    metrics = result['metrics']\n",
        "    summary_metrics.append(metrics)\n",
        "\n",
        "# Create summary DataFrame\n",
        "summary_df = pd.DataFrame(summary_metrics)\n",
        "print(summary_df)\n",
        "\n",
        "# Save summary to CSV\n",
        "summary_df.to_csv('random_forest_performance_summary.csv', index=False)\n",
        "print(\"Summary saved to random_forest_performance_summary.csv\")"
      ],
      "metadata": {
        "id": "ZJmF6PjQ5clW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Random Forest Implementation for Medical Resource Demand Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Load the pre-processed data\n",
        "def load_preprocessed_data(file_path='/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'):\n",
        "    \"\"\"Load the pre-processed dataset from a CSV file\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "        # Convert date column to datetime if it exists\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# 2. Prepare data for time series modeling\n",
        "def prepare_time_series_data(df, target_cols, feature_cols=None, test_size=0.2):\n",
        "    \"\"\"Prepare data for time series modeling\"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'date' column for time series splitting\")\n",
        "    # If feature columns not specified, use all except target and date\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [col for col in df.columns if col not in target_cols + ['date']]\n",
        "    # Split data into train and test sets\n",
        "    n_samples = df.shape[0]\n",
        "    n_test = int(test_size * n_samples)\n",
        "    df_train = df[:-n_test]\n",
        "    df_test = df[-n_test:]\n",
        "    X_train = df_train[feature_cols].values\n",
        "    X_test = df_test[feature_cols].values\n",
        "    y_train = df_train[target_cols].values\n",
        "    y_test = df_test[target_cols].values\n",
        "    dates_train = df_train['date'].values\n",
        "    dates_test = df_test['date'].values\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "\n",
        "# 3. Train and evaluate the Random Forest model\n",
        "def train_and_evaluate_rf_model(X_train, y_train, X_test, y_test, target_cols, param_grid=None):\n",
        "    \"\"\"Train and evaluate a Random Forest model using time series cross-validation\"\"\"\n",
        "    # Create a Random Forest Regressor\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    # Use TimeSeriesSplit for cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    # Perform GridSearchCV if param_grid is provided\n",
        "    if param_grid:\n",
        "        grid_search = GridSearchCV(rf_model, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        rf_model = grid_search.best_estimator_\n",
        "        print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        rf_model.fit(X_train, y_train)\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    # Evaluate the model\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"R-squared"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "B_idsOJH8nE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Random Forest Implementation for Medical Resource Demand Prediction\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Load the pre-processed data\n",
        "def load_preprocessed_data(file_path='/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'):\n",
        "    \"\"\"Load the pre-processed dataset from a CSV file\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "        # Convert date column to datetime if it exists\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# 2. Prepare data for time series modeling\n",
        "def prepare_time_series_data(df, target_cols, feature_cols=None, test_size=0.2):\n",
        "    \"\"\"Prepare data for time series modeling\"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'date' column for time series splitting\")\n",
        "    # If feature columns not specified, use all except target and date\n",
        "    if feature_cols is None:\n",
        "        feature_cols = [col for col in df.columns if col not in target_cols + ['date']]\n",
        "    # Split data into train and test sets\n",
        "    n_samples = df.shape[0]\n",
        "    n_test = int(test_size * n_samples)\n",
        "    df_train = df[:-n_test]\n",
        "    df_test = df[-n_test:]\n",
        "    X_train = df_train[feature_cols].values\n",
        "    X_test = df_test[feature_cols].values\n",
        "    y_train = df_train[target_cols].values\n",
        "    y_test = df_test[target_cols].values\n",
        "    dates_train = df_train['date'].values\n",
        "    dates_test = df_test['date'].values\n",
        "    return X_train, X_test, y_train, y_test, dates_train, dates_test\n",
        "\n",
        "# 3. Train and evaluate the Random Forest model\n",
        "def train_and_evaluate_rf_model(X_train, y_train, X_test, y_test, target_cols, param_grid=None):\n",
        "    \"\"\"Train and evaluate a Random Forest model using time series cross-validation\"\"\"\n",
        "    # Create a Random Forest Regressor\n",
        "    rf_model = RandomForestRegressor(random_state=42)\n",
        "    # Use TimeSeriesSplit for cross-validation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    # Perform GridSearchCV if param_grid is provided\n",
        "    if param_grid:\n",
        "        grid_search = GridSearchCV(rf_model, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        rf_model = grid_search.best_estimator_\n",
        "        print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
        "    else:\n",
        "        rf_model.fit(X_train, y_train)\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf_model.predict(X_test)\n",
        "    # Evaluate the model\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"R-squared (R2): {r2:.4f}\")\n",
        "    return rf_model\n",
        "\n",
        "# 4. Save the trained model\n",
        "def save_trained_model(model, file_path='trained_rf_model.pkl'):\n",
        "    \"\"\"Save the trained Random Forest model to a file\"\"\"\n",
        "    try:\n",
        "        joblib.dump(model, file_path)\n",
        "        print(f\"Model saved to: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load pre-processed data\n",
        "    data = load_preprocessed_data()\n",
        "    if data is not None:\n",
        "        # Define target and feature columns\n",
        "        target_cols = ['icu_patients']  # Replace with your desired target columns\n",
        "        # Prepare data for time series modeling\n",
        "        X_train, X_test, y_train, y_test, dates_train, dates_test = prepare_time_series_data(\n",
        "            data, target_cols, test_size=0.2\n",
        "        )\n",
        "        # Train and evaluate the Random Forest model\n",
        "        rf_model = train_and_evaluate_rf_model(X_train, y_train, X_test, y_test, target_cols)\n",
        "        # Save the trained model\n",
        "        save_trained_model(rf_model)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oFfnTTZn86UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import joblib\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Enhanced Data Loading with Categorical Handling\n",
        "def load_preprocessed_data(file_path='/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'):\n",
        "    \"\"\"Load and preprocess data with categorical handling\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "\n",
        "        # Convert date column to datetime\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# 2. Data Preparation with Feature Engineering\n",
        "def prepare_time_series_data(df, target_cols, categorical_cols=None, test_size=0.2):\n",
        "    \"\"\"Prepare data with categorical encoding\"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'date' column\")\n",
        "\n",
        "    # Identify numerical features (exclude target and date)\n",
        "    numerical_cols = [col for col in df.columns\n",
        "                     if col not in target_cols + ['date'] + (categorical_cols or [])]\n",
        "\n",
        "    # Preprocess categorical variables\n",
        "    if categorical_cols:\n",
        "        # Label Encoding for ordinal categories\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col])\n",
        "            label_encoders[col] = le\n",
        "\n",
        "    # Split data\n",
        "    n_test = int(test_size * len(df))\n",
        "    df_train = df[:-n_test]\n",
        "    df_test = df[-n_test:]\n",
        "\n",
        "    # Separate features and target\n",
        "    X_train = df_train[numerical_cols + (categorical_cols or [])].values\n",
        "    X_test = df_test[numerical_cols + (categorical_cols or [])].values\n",
        "    y_train = df_train[target_cols].values\n",
        "    y_test = df_test[target_cols].values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# 3. Model Training with Enhanced Configuration\n",
        "def train_and_evaluate_rf_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train Random Forest with robust settings\"\"\"\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=tscv,\n",
        "                             scoring='neg_mean_squared_error',\n",
        "                             verbose=2)\n",
        "    grid_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Enhanced evaluation\n",
        "    metrics = {\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R2': r2_score(y_test, y_pred),\n",
        "        'Best Params': grid_search.best_params_\n",
        "    }\n",
        "\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# 4. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    data = load_preprocessed_data()\n",
        "\n",
        "    if data is not None:\n",
        "        # Configure based on your data\n",
        "        target_cols = ['icu_patients']\n",
        "        categorical_cols = ['country']  # Add all categorical columns here\n",
        "\n",
        "        # Prepare data\n",
        "        X_train, X_test, y_train, y_test = prepare_time_series_data(\n",
        "            data, target_cols, categorical_cols\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        model = train_and_evaluate_rf_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Save model\n",
        "        joblib.dump(model, 'rf_medical_demand_model.pkl')\n",
        "        print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "id": "n4W_g1p4-ggk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# 1. Enhanced Data Loading with Automatic Categorical Detection\n",
        "def load_preprocessed_data(file_path):\n",
        "    \"\"\"Load and preprocess data with automatic categorical detection\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "        print(\"Columns in dataset:\", df.columns.tolist())\n",
        "\n",
        "        # Convert date column to datetime if exists\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "            df = df.sort_values('date')\n",
        "\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "# 2. Smart Data Preparation\n",
        "def prepare_time_series_data(df, target_cols, test_size=0.2):\n",
        "    \"\"\"Prepare data with automatic feature handling\"\"\"\n",
        "    if 'date' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must contain a 'date' column\")\n",
        "\n",
        "    # Auto-detect categorical columns (string type columns that aren't dates/targets)\n",
        "    categorical_cols = [col for col in df.columns\n",
        "                       if df[col].dtype == 'object'\n",
        "                       and col not in target_cols + ['date']]\n",
        "\n",
        "    # Auto-detect numerical columns\n",
        "    numerical_cols = [col for col in df.columns\n",
        "                     if col not in target_cols + ['date'] + categorical_cols\n",
        "                     and pd.api.types.is_numeric_dtype(df[col])]\n",
        "\n",
        "    print(f\"Detected categorical columns: {categorical_cols}\")\n",
        "    print(f\"Using numerical columns: {numerical_cols}\")\n",
        "\n",
        "    # Encode categorical variables\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "    # Split data\n",
        "    n_test = int(test_size * len(df))\n",
        "    df_train = df[:-n_test]\n",
        "    df_test = df[-n_test:]\n",
        "\n",
        "    # Prepare features and targets\n",
        "    feature_cols = numerical_cols + categorical_cols\n",
        "    X_train = df_train[feature_cols].values\n",
        "    X_test = df_test[feature_cols].values\n",
        "    y_train = df_train[target_cols].values\n",
        "    y_test = df_test[target_cols].values\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, label_encoders\n",
        "\n",
        "# 3. Model Training with Feature Importance\n",
        "def train_and_evaluate_rf_model(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train and evaluate Random Forest with feature analysis\"\"\"\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200],\n",
        "        'max_depth': [10, 20, None],\n",
        "        'min_samples_split': [2, 5]\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=tscv,\n",
        "                             scoring='neg_mean_squared_error',\n",
        "                             verbose=1)\n",
        "    grid_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    # Evaluation metrics\n",
        "    metrics = {\n",
        "        'MAE': mean_absolute_error(y_test, y_pred),\n",
        "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
        "        'R2': r2_score(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "    print(\"\\nModel Evaluation:\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
        "\n",
        "    # Feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': numerical_cols + categorical_cols,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "# 4. Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Update this path to your dataset\n",
        "    DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "    # Load data\n",
        "    data = load_preprocessed_data(DATA_PATH)\n",
        "\n",
        "    if data is not None:\n",
        "        # Configure target column (update this to your actual target)\n",
        "        target_cols = ['icu_patients']  # Change this to your target column name\n",
        "\n",
        "        # Prepare data\n",
        "        X_train, X_test, y_train, y_test, label_encoders = prepare_time_series_data(\n",
        "            data, target_cols\n",
        "        )\n",
        "\n",
        "        # Train model\n",
        "        model = train_and_evaluate_rf_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Save model and encoders\n",
        "        joblib.dump({\n",
        "            'model': model,\n",
        "            'label_encoders': label_encoders\n",
        "        }, 'medical_demand_rf_model.pkl')\n",
        "        print(\"Model and encoders saved successfully.\")"
      ],
      "metadata": {
        "id": "8MO3bL2j_Jns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model\n",
        "artifacts = joblib.load('xgboost_medical_demand_model.pkl')\n",
        "model = artifacts['model']\n",
        "label_encoders = artifacts['label_encoders']\n",
        "\n",
        "# Prepare new data (example)\n",
        "new_data = pd.DataFrame({\n",
        "    'date': ['2023-12-01'],\n",
        "    'region': ['North'],\n",
        "    'cases': [1250],\n",
        "    ...\n",
        "})\n",
        "\n",
        "# Apply same preprocessing\n",
        "for col, le in label_encoders.items():\n",
        "    new_data[col] = le.transform(new_data[col].astype(str))\n",
        "\n",
        "# Generate time features\n",
        "new_data['date'] = pd.to_datetime(new_data['date'])\n",
        "new_data['day_of_week'] = new_data['date'].dt.dayofweek\n",
        "# ... add other features like in training\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_data[features])  # features saved in artifacts\n",
        "print(f\"Predicted ICU patients: {prediction[0]:.0f}\")"
      ],
      "metadata": {
        "id": "pUPnpvuhAhgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load saved model\n",
        "artifacts = joblib.load('xgboost_medical_demand_model.pkl')\n",
        "model = artifacts['model']\n",
        "label_encoders = artifacts['label_encoders']\n",
        "features = artifacts['features']  # This line is missing in your code\n",
        "\n",
        "# Prepare new data (example)\n",
        "new_data = pd.DataFrame({\n",
        "    'date': ['2023-12-01'],\n",
        "    'region': ['North'],\n",
        "    'cases': [1250],\n",
        "    # ... other features\n",
        "})\n",
        "\n",
        "# Apply same preprocessing\n",
        "for col, le in label_encoders.items():\n",
        "    if col in new_data.columns:\n",
        "        new_data[col] = le.transform(new_data[col].astype(str))\n",
        "\n",
        "# Generate time features\n",
        "new_data['date'] = pd.to_datetime(new_data['date'])\n",
        "new_data['day_of_week'] = new_data['date'].dt.dayofweek\n",
        "# ... add other features like in training\n",
        "\n",
        "# Predict\n",
        "prediction = model.predict(new_data[features])\n",
        "print(f\"Predicted ICU patients: {prediction[0]:.0f}\")"
      ],
      "metadata": {
        "id": "5EA54ZW0SZ_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Convert 'date' column to DatetimeIndex and set as index\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df = df.set_index('date')\n",
        "\n",
        "# 2. Resample to weekly frequency and sum cases and deaths\n",
        "weekly_trends = df[['new_cases', 'new_deaths']].resample('W').sum()\n",
        "\n",
        "# 3. Display or further process the 'weekly_trends' DataFrame\n",
        "print(weekly_trends.head())  # Example: Print the first few rows"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bomQGNwTD9CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Splitting"
      ],
      "metadata": {
        "id": "cGRqqYIQHjLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load pre-processed dataset (assumed to be cleaned and encoded)\n",
        " DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop(columns=['target'])  # Replace 'target' with your target column (e.g., hospital_beds)\n",
        "y = data['target']\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Scale numerical features (important for LSTM and other models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaled data for models requiring DataFrame format (e.g., Prophet)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Print shapes to verify splits\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "NIYTKOCVHnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset path (no indentation)\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load pre-processed dataset\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# Adjust column names based on your dataset\n",
        "X = data.drop(columns=['hospital_beds_needed', 'date'])  # Example target and date columns\n",
        "y = data['hospital_beds_needed']\n",
        "\n",
        "# Verify data loading\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "print(\"Features:\", X.columns.tolist())"
      ],
      "metadata": {
        "id": "_Bwz--W_IZIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Inspect columns and data\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "print(\"\\nFirst few rows:\\n\", data.head())"
      ],
      "metadata": {
        "id": "oxwfKxxuIwZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD THE CLEANED DATASET FOR SPLITTING"
      ],
      "metadata": {
        "id": "UD_4Ym98I6pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "# print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'target' and 'date' with actual column names from your dataset\n",
        "# Example: if target is 'hospital_beds' and date is 'date', use those\n",
        "X = data.drop(columns=['target', 'date'])  # Update 'target' to your target column (e.g., 'hospital_beds')\n",
        "y = data['target']  # Update 'target' to your target column\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "# shuffle=False to preserve temporal order for time-series\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
        "\n",
        "# Scale numerical features (important for LSTM and other models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaled data as DataFrames for models requiring DataFrame format (e.g., Prophet)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Print shapes to verify splits\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "kbLIp49bI5mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Inspect columns and data\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "print(\"\\nFirst few rows:\\n\", data.head())"
      ],
      "metadata": {
        "id": "e4QkathSJPdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "try:\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {DATA_PATH}. Please check the path and try again.\")\n",
        "    raise\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "# print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'hospital_beds' and 'date_column' with actual column names from your dataset\n",
        "# Run the inspection code above to find the correct names (e.g., 'beds_needed', 'date')\n",
        "try:\n",
        "    X = data.drop(columns=['hospital_beds', 'date_column'])  # Update to your target and date columns\n",
        "    y = data['hospital_beds']  # Update to your target column\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}. Please check column names using data.columns.tolist() and update the code.\")\n",
        "    raise\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "# shuffle=False to preserve temporal order for time-series\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
        "\n",
        "# Scale numerical features (important for LSTM and other models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaled data as DataFrames for models requiring DataFrame format (e.g., Prophet)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Print shapes to verify splits\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "RYEk5jdnJWYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "try:\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {DATA_PATH}. Please check the path and try again.\")\n",
        "    raise\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'hosp_patients' and 'date' with actual column names from your dataset\n",
        "# Run the inspection code above to find the correct names (e.g., 'beds_needed', 'date')\n",
        "try:\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "try:\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {DATA_PATH}. Please check the path and try again.\")\n",
        "    raise\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'hosp_patients' and 'date' with actual column names from your dataset\n",
        "# Run the inspection code above to find the correct names (e.g., 'beds_needed', 'date')\n",
        "try:\n",
        "    # Update with actual column names\n",
        "    X = data.drop(columns=['hosp_patients', 'date'])  # Example: hosp_patients might be the target for hospital bed demand\n",
        "    y = data['hosp_patients']  #\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Column {e} not found in the dataset. Please check the column names and try again.\")\n",
        "    raise"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5-Nj5gUGJgHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "try:\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {DATA_PATH}. Please check the path and try again.\")\n",
        "    raise\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'hosp_patients' and 'date' with actual column names from your dataset\n",
        "# Run the inspection code above to find the correct names (e.g., 'beds_needed', 'date')\n",
        "try:\n",
        "    # Update with actual column names\n",
        "    X = data.drop(columns=['hosp_patients', 'date'])  # Example: hosp_patients might be the target for hospital bed demand\n",
        "    y = data['hosp_patients']  # Example: hosp_patients could represent hospital bed demand\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}. Please check column names using data.columns.tolist() and update the code.\")\n",
        "    raise\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "# shuffle=False to preserve temporal order for time-series\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
        "\n",
        "# Scale numerical features (important for LSTM and other models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaled data as DataFrames for models requiring DataFrame format (e.g., Prophet)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Print shapes to verify splits\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zzYxv78YJj4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define dataset path\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# Load pre-processed dataset\n",
        "try:\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {DATA_PATH}. Please check the path and try again.\")\n",
        "    raise\n",
        "\n",
        "# Inspect columns (uncomment to verify)\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "# print(\"\\nFirst few rows:\\n\", data.head())\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "# REPLACE 'hosp_patients' and 'date' with actual column names from your dataset\n",
        "# Run the inspection code above to find the correct names (e.g., 'beds_needed', 'date')\n",
        "try:\n",
        "    # Update with actual column names\n",
        "    X = data.drop(columns=['hosp_patients', 'date'])  # Example: hosp_patients might be the target for hospital bed demand\n",
        "    # Select only numeric features for scaling\n",
        "    numeric_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    X = X[numeric_features]\n",
        "    y = data['hosp_patients']  # Example: hosp_patients could represent hospital bed demand\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}. Please check column names using data.columns.tolist() and update the code.\")\n",
        "    raise\n",
        "\n",
        "# Split data into training (70%), validation (15%), and test (15%) sets\n",
        "# shuffle=False to preserve temporal order for time-series\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=False)\n",
        "\n",
        "# Scale numerical features (important for LSTM and other models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save scaled data as DataFrames for models requiring DataFrame format (e.g., Prophet)\n",
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns)\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# Print shapes to verify splits\n",
        "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Gugdz4ysJuV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RANDOM FOREST MODEL"
      ],
      "metadata": {
        "id": "7Pey543KKaqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Train model on scaled training data\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_rf = rf_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "rf_rmse = mean_squared_error(y_val, y_val_pred_rf, squared=False)\n",
        "rf_mae = mean_absolute_error(y_val, y_val_pred_rf)\n",
        "rf_r2 = r2_score(y_val, y_val_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(f\"Random Forest - RMSE: {rf_rmse:.4f}, MAE: {rf_mae:.4f}, R²: {rf_r2:.4f}\")\n",
        "\n",
        "# Feature importance (optional)\n",
        "importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
        "print(\"Feature Importances:\\n\", importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "MESsdRUwKZrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Model\n",
        "\n"
      ],
      "metadata": {
        "id": "kYXEDOqCKptd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "\n",
        "# Train model\n",
        "xgb_model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], early_stopping_rounds=10, verbose=False)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_xgb = xgb_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "xgb_rmse = mean_squared_error(y_val, y_val_pred_xgb, squared=False)\n",
        "xgb_mae = mean_absolute_error(y_val, y_val_pred_xgb)\n",
        "xgb_r2 = r2_score(y_val, y_val_pred_xgb)\n",
        "\n",
        "# Print results\n",
        "print(f\"XGBoost - RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}, R²: {xgb_r2:.4f}\")"
      ],
      "metadata": {
        "id": "baOLd1mJKx-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting"
      ],
      "metadata": {
        "id": "U756fan2K4G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Gradient Boosting model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train model\n",
        "gb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_gb = gb_model.predict(X_val_scaled)\n",
        "\n",
        "# Evaluate model\n",
        "gb_rmse = mean_squared_error(y_val, y_val_pred_gb, squared=False)\n",
        "gb_mae = mean_absolute_error(y_val, y_val_pred_gb)\n",
        "gb_r2 = r2_score(y_val, y_val_pred_gb)\n",
        "\n",
        "# Print results\n",
        "print(f\"Gradient Boosting - RMSE: {gb_rmse:.4f}, MAE: {gb_mae:.4f}, R²: {gb_r2:.4f}\")"
      ],
      "metadata": {
        "id": "2jIehtmGK89g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Short-Term Memory (LSTM)\n",
        "\n"
      ],
      "metadata": {
        "id": "0JgQjmXfLGdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for LSTM: [samples, timesteps, features]\n",
        "timesteps = 7  # Example: Use 7-day sequences\n",
        "def create_sequences(X, y, timesteps):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(X) - timesteps):\n",
        "        X_seq.append(X[i:i+timesteps])\n",
        "        y_seq.append(y[i+timesteps])\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Create sequences for training, validation, and test sets\n",
        "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, timesteps)\n",
        "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val.values, timesteps)\n",
        "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, timesteps)\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(timesteps, X_train_scaled.shape[1]), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(50, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Train model\n",
        "history = lstm_model.fit(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq),\n",
        "                         epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Predict on validation set\n",
        "y_val_pred_lstm = lstm_model.predict(X_val_seq)\n",
        "\n",
        "# Evaluate model\n",
        "lstm_rmse = mean_squared_error(y_val_seq, y_val_pred_lstm, squared=False)\n",
        "lstm_mae = mean_absolute_error(y_val_seq, y_val_pred_lstm)\n",
        "lstm_r2 = r2_score(y_val_seq, y_val_pred_lstm)\n",
        "\n",
        "# Print results\n",
        "print(f\"LSTM - RMSE: {lstm_rmse:.4f}, MAE: {lstm_mae:.4f}, R²: {lstm_r2:.4f}\")"
      ],
      "metadata": {
        "id": "kUTwUvdnLKrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prophet"
      ],
      "metadata": {
        "id": "uNX3kzhvLPMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for Prophet (requires 'ds' for date and 'y' for target)\n",
        "# REPLACE 'date_column' and 'hospital_beds' with actual column names from your dataset\n",
        "try:\n",
        "    prophet_data = data[['date_column', 'hospital_beds']].copy()\n",
        "    prophet_data.rename(columns={'date_column': 'ds', 'hospital_beds': 'y'}, inplace=True)\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}. Please check column names using data.columns.tolist() and update the code.\")\n",
        "    raise\n",
        "\n",
        "# Convert date column to datetime\n",
        "prophet_data['ds'] = pd.to_datetime(prophet_data['ds'])\n",
        "\n",
        "# Add regressors (all features except date and target)\n",
        "for col in X_train_scaled_df.columns:\n",
        "    prophet_data[col] = data[col]  # Align with original data\n",
        "\n",
        "# Split Prophet data\n",
        "train_size = int(len(prophet_data) * 0.7)\n",
        "val_size = int(len(prophet_data) * 0.15)\n",
        "prophet_train = prophet_data[:train_size]\n",
        "prophet_val = prophet_data[train_size:train_size + val_size]\n",
        "\n",
        "# Initialize Prophet model\n",
        "prophet_model = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=True)\n",
        "\n",
        "# Add regressors\n",
        "for col in X_train_scaled_df.columns:\n",
        "    prophet_model.add_regressor(col)\n",
        "\n",
        "# Train model\n",
        "prophet_model.fit(prophet_train)\n",
        "\n",
        "# Predict on validation set\n",
        "future = prophet_model.make_future_dataframe(periods=len(prophet_val), freq='D')\n",
        "future = future.merge(prophet_val[['ds'] + X_train_scaled_df.columns.tolist()], on='ds', how='left').fillna(0)\n",
        "forecast = prophet_model.predict(future)\n",
        "y_val_pred_prophet = forecast['yhat'].iloc[train_size:train_size + len(prophet_val)]\n",
        "\n",
        "# Evaluate model\n",
        "prophet_rmse = mean_squared_error(prophet_val['y'], y_val_pred_prophet, squared=False)\n",
        "prophet_mae = mean_absolute_error(prophet_val['y'], y_val_pred_prophet)\n",
        "prophet_r2 = r2_score(prophet_val['y'], y_val_pred_prophet)\n",
        "\n",
        "# Print results\n",
        "print(f\"Prophet - RMSE: {prophet_rmse:.4f}, MAE: {prophet_mae:.4f}, R²: {prophet_r2:.4f}\")\n",
        "\n",
        "# Visualize predictions (optional)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_val.values, label='Actual', color='black')\n",
        "plt.plot(y_val_pred_rf, label='Random Forest', alpha=0.7)\n",
        "plt.plot(y_val_pred_xgb, label='XGBoost', alpha=0.7)\n",
        "plt.plot(y_val_pred_gb, label='Gradient Boosting', alpha=0.7)\n",
        "plt.plot(y_val_pred_lstm.flatten(), label='LSTM', alpha=0.7)  # Flatten LSTM predictions\n",
        "plt.plot(y_val_pred_prophet, label='Prophet', alpha=0.7)\n",
        "plt.legend()\n",
        "plt.title('Model Predictions vs Actual (Validation Set)')\n",
        "plt.xlabel('Validation Set Index')\n",
        "plt.ylabel('Hospital Beds Needed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xO5g8DD-LTLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Real-Time Machine Learning Model for Medical Resource Demand Prediction\n",
        "# Author: Claude\n",
        "# Date: May 8, 2025\n",
        "\n",
        "This notebook implements a working prototype for real-time medical resource demand prediction:\n",
        "1. Load and analyze preprocessed dataset\n",
        "2. Build and train predictive models\n",
        "3. Implement a dashboard for visualization\n",
        "4. Set up framework for real-time data ingestion\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the dataset\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# 1. Data Loading and Exploration\n",
        "print(\"Loading dataset from:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"\\n==== Dataset Overview ====\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "display(df.dtypes)\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Check if there's a datetime column, if not, create one\n",
        "if not any(df.columns.str.contains('date|time', case=False)):\n",
        "    print(\"\\nNo explicit date/time column found. Creating a synthetic one for time-series analysis...\")\n",
        "    df['date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='D')\n",
        "else:\n",
        "    date_col = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()][0]\n",
        "    print(f\"\\nUsing existing date column: {date_col}\")\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    date_col = 'date'\n",
        "\n",
        "# Identify target columns (resources to predict)\n",
        "# For now, let's assume columns with 'demand', 'usage', 'need', 'resource' in their name are targets\n",
        "target_cols = [col for col in df.columns if any(x in col.lower() for x in ['demand', 'usage', 'need', 'resource'])]\n",
        "\n",
        "if not target_cols:\n",
        "    print(\"\\nNo clear target columns identified. Please specify which columns represent resource demand.\")\n",
        "    # For demonstration, let's assume the last column might be the target\n",
        "    target_cols = [df.columns[-2]]\n",
        "\n",
        "print(f\"\\nIdentified potential target columns: {target_cols}\")\n",
        "\n",
        "# Let's visualize the time series of the first identified target\n",
        "plt.figure(figsize=(12, 6))\n",
        "for target in target_cols[:3]:  # Show up to 3 targets\n",
        "    plt.plot(df[date_col], df[target], label=target)\n",
        "plt.title('Medical Resource Demand Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Feature Engineering and Preprocessing\n",
        "def preprocess_data(dataframe, target_column):\n",
        "    \"\"\"Preprocess the data for modeling\"\"\"\n",
        "    # Make a copy to avoid modifying the original dataframe\n",
        "    df_copy = dataframe.copy()\n",
        "\n",
        "    # Extract date features if date column exists\n",
        "    if 'date' in df_copy.columns:\n",
        "        df_copy['day_of_week'] = df_copy['date'].dt.dayofweek\n",
        "        df_copy['month'] = df_copy['date'].dt.month\n",
        "        df_copy['day'] = df_copy['date'].dt.day\n",
        "        df_copy['is_weekend'] = df_copy['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # Drop non-numeric columns except derived features\n",
        "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cols_to_keep = numeric_cols + ['day_of_week', 'month', 'day', 'is_weekend']\n",
        "    df_copy = df_copy[cols_to_keep]\n",
        "\n",
        "    # Remove the target from features\n",
        "    features = df_copy.drop(target_column, axis=1)\n",
        "    target = df_copy[target_column]\n",
        "\n",
        "    # Create lagged features (important for time series)\n",
        "    for lag in [1, 3, 7]:\n",
        "        if len(df_copy) > lag:\n",
        "            features[f'{target_column}_lag_{lag}'] = target.shift(lag)\n",
        "\n",
        "    # Drop rows with NaN due to lag creation\n",
        "    features = features.dropna()\n",
        "    target = target.iloc[features.index]\n",
        "\n",
        "    return features, target\n",
        "\n",
        "# 3. Model Building and Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n==== {model_name} Performance ====\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "def build_and_evaluate_models(features, target):\n",
        "    \"\"\"Build and evaluate multiple models\"\"\"\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        features, target, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'Linear Regression': LinearRegression(),\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "        'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.05, random_state=42),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    best_model_name = None\n",
        "    best_model_obj = None\n",
        "    best_score = float('inf')  # Lower is better for RMSE\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Evaluate\n",
        "        eval_results = evaluate_model(y_test, y_pred, name)\n",
        "        results[name] = eval_results\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if eval_results['rmse'] < best_score:\n",
        "            best_score = eval_results['rmse']\n",
        "            best_model_name = name\n",
        "            best_model_obj = model\n",
        "\n",
        "    print(f\"\\nBest model: {best_model_name} with RMSE: {best_score:.4f}\")\n",
        "\n",
        "    # Plot actual vs predicted for the best model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    y_pred = best_model_obj.predict(X_test_scaled)\n",
        "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "    plt.xlabel('Actual')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.title(f'Actual vs Predicted - {best_model_name}')\n",
        "    plt.show()\n",
        "\n",
        "    return best_model_obj, scaler, results, best_model_name\n",
        "\n",
        "# Apply preprocessing and model building for the first target\n",
        "selected_target = target_cols[0]\n",
        "print(f\"\\nBuilding model for target: {selected_target}\")\n",
        "features, target = preprocess_data(df, selected_target)\n",
        "best_model, scaler, model_results, best_model_name = build_and_evaluate_models(features, target)\n",
        "\n",
        "# 4. Real-time Prediction Framework\n",
        "class RealTimePredictor:\n",
        "    def __init__(self, model, scaler, feature_columns, target_column, historical_data=None):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.feature_columns = feature_columns\n",
        "        self.target_column = target_column\n",
        "\n",
        "        # Store historical data for creating lag features\n",
        "        self.historical_data = historical_data.copy() if historical_data is not None else pd.DataFrame()\n",
        "\n",
        "        # Cache for API responses\n",
        "        self.api_cache = {}\n",
        "\n",
        "        # Mock data generator for demonstration\n",
        "        self.last_update = datetime.now()\n",
        "\n",
        "    def prepare_features(self, new_data):\n",
        "        \"\"\"Prepare features for prediction including lags\"\"\"\n",
        "        # Combine with historical to generate lag features\n",
        "        combined_data = pd.concat([self.historical_data, new_data]).reset_index(drop=True)\n",
        "\n",
        "        # Create date features\n",
        "        if 'date' in combined_data.columns:\n",
        "            combined_data['day_of_week'] = combined_data['date'].dt.dayofweek\n",
        "            combined_data['month'] = combined_data['date'].dt.month\n",
        "            combined_data['day'] = combined_data['date'].dt.day\n",
        "            combined_data['is_weekend'] = combined_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "        # Create lag features\n",
        "        for lag in [1, 3, 7]:\n",
        "            if len(combined_data) > lag:\n",
        "                combined_data[f'{self.target_column}_lag_{lag}'] = combined_data[self.target_column].shift(lag)\n",
        "\n",
        "        # Get the latest row with all features\n",
        "        latest_data = combined_data.iloc[-1:].dropna(axis=1, how='any')\n",
        "\n",
        "        # Select only the feature columns the model was trained on\n",
        "        available_features = [col for col in self.feature_columns if col in latest_data.columns]\n",
        "        X = latest_data[available_features]\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    def fetch_real_time_data(self, source=\"mock\"):\n",
        "        \"\"\"Fetch real-time data from specified source\"\"\"\n",
        "        current_time = datetime.now()\n",
        "\n",
        "        if source == \"mock\":\n",
        "            # Generate mock data for demonstration\n",
        "            # In a real scenario, you would replace this with API calls\n",
        "\n",
        "            # Only generate new data if enough time has passed\n",
        "            if (current_time - self.last_update).seconds < 5:\n",
        "                return None\n",
        "\n",
        "            self.last_update = current_time\n",
        "\n",
        "            # Create synthetic data with some randomness\n",
        "            new_row = {\n",
        "                'date': pd.Timestamp(current_time),\n",
        "                # Add random fluctuation to the last known value\n",
        "                self.target_column: self.historical_data[self.target_column].iloc[-1] * (1 + np.random.normal(0, 0.05))\n",
        "            }\n",
        "\n",
        "            # Add other features that might be needed\n",
        "            for col in self.historical_data.columns:\n",
        "                if col not in new_row and col != 'date' and col != self.target_column:\n",
        "                    if col.endswith(('_lag_1', '_lag_3', '_lag_7')):\n",
        "                        continue  # Skip lag columns as they'll be computed\n",
        "                    new_row[col] = self.historical_data[col].iloc[-1] * (1 + np.random.normal(0, 0.03))\n",
        "\n",
        "            new_data = pd.DataFrame([new_row])\n",
        "            return new_data\n",
        "\n",
        "        elif source == \"owid\":\n",
        "            # Our World in Data API integration\n",
        "            # Example: Fetch latest COVID-19 data\n",
        "            if \"owid\" not in self.api_cache or (current_time - self.api_cache.get(\"last_update\", datetime.min)).seconds > 3600:\n",
        "                try:\n",
        "                    url = \"https://covid.ourworldindata.org/data/latest/owid-covid-latest.csv\"\n",
        "                    response = requests.get(url)\n",
        "                    data = pd.read_csv(io.StringIO(response.text))\n",
        "                    self.api_cache[\"owid\"] = data\n",
        "                    self.api_cache[\"last_update\"] = current_time\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching OWID data: {e}\")\n",
        "                    return None\n",
        "\n",
        "            # Process the data to match our format\n",
        "            # This would need to be customized based on your specific requirements\n",
        "            return None  # Return processed data\n",
        "\n",
        "        elif source == \"cdc\":\n",
        "            # CDC API integration would go here\n",
        "            # This is a placeholder for demonstration\n",
        "            return None\n",
        "\n",
        "        elif source == \"healthmap\":\n",
        "            # HealthMap API integration would go here\n",
        "            return None\n",
        "\n",
        "        elif source == \"ehr\":\n",
        "            # Electronic Health Records integration would go here\n",
        "            return None\n",
        "\n",
        "        return None\n",
        "\n",
        "    def update_historical_data(self, new_data):\n",
        "        \"\"\"Add new data to historical dataset\"\"\"\n",
        "        if new_data is not None and not new_data.empty:\n",
        "            self.historical_data = pd.concat([self.historical_data, new_data]).reset_index(drop=True)\n",
        "            # Keep only the most recent 1000 rows to prevent memory issues\n",
        "            if len(self.historical_data) > 1000:\n",
        "                self.historical_data = self.historical_data.iloc[-1000:]\n",
        "\n",
        "    def predict_demand(self):\n",
        "        \"\"\"Make a prediction based on the latest data\"\"\"\n",
        "        # Fetch new data\n",
        "        new_data = self.fetch_real_time_data()\n",
        "\n",
        "        if new_data is None or new_data.empty:\n",
        "            return None, None\n",
        "\n",
        "        # Update historical data with the new data\n",
        "        self.update_historical_data(new_data)\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        X = self.prepare_features(new_data)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = self.model.predict(X)[0]\n",
        "\n",
        "        return prediction, new_data\n",
        "\n",
        "# 5. Create Interactive Dashboard\n",
        "def create_dashboard(predictor, target_column):\n",
        "    \"\"\"Create an interactive dashboard for visualizing predictions\"\"\"\n",
        "    # Create a figure for the dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        specs=[[{\"colspan\": 2}, None],\n",
        "               [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]],\n",
        "        subplot_titles=(\"Historical and Predicted Demand\",\n",
        "                         \"Current Demand\", \"Predicted Demand\")\n",
        "    )\n",
        "\n",
        "    # Get historical data for plotting\n",
        "    historical_data = predictor.historical_data.copy()\n",
        "\n",
        "    # Initial plot of historical data\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=historical_data['date'],\n",
        "            y=historical_data[target_column],\n",
        "            mode='lines',\n",
        "            name='Historical Demand',\n",
        "            line=dict(color='blue')\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Add trace for predictions (initially empty)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[],\n",
        "            y=[],\n",
        "            mode='lines+markers',\n",
        "            name='Predicted Demand',\n",
        "            line=dict(color='red')\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Add indicator for current demand\n",
        "    fig.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"number+delta\",\n",
        "            value=historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "            title={\"text\": \"Current Demand\"},\n",
        "            delta={'reference': historical_data[target_column].iloc[-2] if len(historical_data) > 1 else 0,\n",
        "                   'valueformat': '.2f'},\n",
        "            number={'valueformat': '.2f'}\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Add indicator for predicted demand (initially same as current)\n",
        "    fig.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"number+delta\",\n",
        "            value=historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "            title={\"text\": \"Predicted Demand\"},\n",
        "            delta={'reference': historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "                   'valueformat': '.2f'},\n",
        "            number={'valueformat': '.2f'}\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=600,\n",
        "        title_text=f\"Real-Time Medical Resource Demand Dashboard: {target_column}\",\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Create function to update dashboard\n",
        "    prediction_history = []\n",
        "    timestamp_history = []\n",
        "\n",
        "    def update_dashboard():\n",
        "        nonlocal prediction_history, timestamp_history\n",
        "\n",
        "        # Make prediction\n",
        "        prediction, new_data = predictor.predict_demand()\n",
        "\n",
        "        if prediction is not None and new_data is not None:\n",
        "            # Store prediction and timestamp\n",
        "            prediction_history.append(prediction)\n",
        "            timestamp_history.append(new_data['date'].iloc[0])\n",
        "\n",
        "            # Keep only the last 50 predictions\n",
        "            if len(prediction_history) > 50:\n",
        "                prediction_history = prediction_history[-50:]\n",
        "                timestamp_history = timestamp_history[-50:]\n",
        "\n",
        "            # Get the most recent historical data\n",
        "            historical_data = predictor.historical_data.copy()\n",
        "\n",
        "            # Update historical demand line\n",
        "            fig.data[0].x = historical_data['date']\n",
        "            fig.data[0].y = historical_data[target_column]\n",
        "\n",
        "            # Update prediction line\n",
        "            fig.data[1].x = timestamp_history\n",
        "            fig.data[1].y = prediction_history\n",
        "\n",
        "            # Update current demand indicator\n",
        "            current_value = historical_data[target_column].iloc[-1]\n",
        "            reference_value = historical_data[target_column].iloc[-2] if len(historical_data) > 1 else current_value\n",
        "            fig.data[2].value = current_value\n",
        "            fig.data[2].delta.reference = reference_value\n",
        "\n",
        "            # Update predicted demand indicator\n",
        "            fig.data[3].value = prediction\n",
        "            fig.data[3].delta.reference = current_value\n",
        "\n",
        "            # Update layout with current time\n",
        "            fig.update_layout(\n",
        "                title_text=f\"Real-Time Medical Resource Demand Dashboard: {target_column}<br>Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "            )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    return fig, update_dashboard\n",
        "\n",
        "# 6. Run Real-Time Prediction System\n",
        "def run_real_time_system(model, scaler, df, target_column):\n",
        "    \"\"\"Run the real-time prediction system\"\"\"\n",
        "    # Initialize predictor with historical data\n",
        "    feature_columns = [col for col in df.columns if col != target_column and col != 'date']\n",
        "    predictor = RealTimePredictor(model, scaler, feature_columns, target_column, historical_data=df)\n",
        "\n",
        "    # Set up dashboard\n",
        "    fig, update_dashboard = create_dashboard(predictor, target_column)\n",
        "\n",
        "    # Create buttons for dashboard control\n",
        "    start_button = widgets.Button(description='Start Real-Time Prediction')\n",
        "    stop_button = widgets.Button(description='Stop')\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Define button behaviors\n",
        "    running = False\n",
        "    def start_clicked(b):\n",
        "        nonlocal running\n",
        "        running = True\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Starting real-time prediction for {target_column}...\")\n",
        "            display(fig)\n",
        "\n",
        "            while running:\n",
        "                fig = update_dashboard()\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Real-time prediction for {target_column} (Press 'Stop' to end)\")\n",
        "                print(f\"Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                display(fig)\n",
        "                time.sleep(2)  # Update every 2 seconds\n",
        "\n",
        "    def stop_clicked(b):\n",
        "        nonlocal running\n",
        "        running = False\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Real-time prediction stopped.\")\n",
        "\n",
        "    start_button.on_click(start_clicked)\n",
        "    stop_button.on_click(stop_clicked)\n",
        "\n",
        "    # Display buttons and output\n",
        "    display(widgets.HBox([start_button, stop_button]))\n",
        "    display(output)\n",
        "\n",
        "# Execute the main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n==== Starting Medical Resource Demand Prediction System ====\")\n",
        "\n",
        "    # Let the user select which target to predict\n",
        "    if len(target_cols) > 1:\n",
        "        target_selector = widgets.Dropdown(\n",
        "            options=target_cols,\n",
        "            value=target_cols[0],\n",
        "            description='Select Resource:',\n",
        "        )\n",
        "\n",
        "        def on_target_change(change):\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Selected target: {change.new}\")\n",
        "            # Re-run the model building for the new target\n",
        "            features, target = preprocess_data(df, change.new)\n",
        "            best_model, scaler, model_results, best_model_name = build_and_evaluate_models(features, target)\n",
        "            # Start the real-time system\n",
        "            run_real_time_system(best_model, scaler, df, change.new)\n",
        "\n",
        "        target_selector.observe(on_target_change, names='value')\n",
        "        display(target_selector)\n",
        "    else:\n",
        "        # If only one target, start immediately\n",
        "        run_real_time_system(best_model, scaler, df, selected_target)"
      ],
      "metadata": {
        "id": "SnMahZlCJaY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Real-Time Machine Learning Model for Medical Resource Demand Prediction\n",
        "# Author: Claude\n",
        "# Date: May 8, 2025\n",
        "\n",
        "This notebook implements a working prototype for real-time medical resource demand prediction:\n",
        "1. Load and analyze preprocessed dataset\n",
        "2. Build and train predictive models\n",
        "3. Implement a dashboard for visualization\n",
        "4. Set up framework for real-time data ingestion\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from prophet import Prophet\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "import joblib\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the dataset\n",
        "DATA_PATH = '/content/drive/MyDrive/Medical_Resource_Prediction/cleaned_dataset.csv'\n",
        "\n",
        "# 1. Data Loading and Exploration\n",
        "print(\"Loading dataset from:\", DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(\"\\n==== Dataset Overview ====\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "display(df.dtypes)\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Check if there's a datetime column, if not, create one\n",
        "if not any(df.columns.str.contains('date|time', case=False)):\n",
        "    print(\"\\nNo explicit date/time column found. Creating a synthetic one for time-series analysis...\")\n",
        "    df['date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='D')\n",
        "else:\n",
        "    date_col = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()][0]\n",
        "    print(f\"\\nUsing existing date column: {date_col}\")\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    date_col = 'date'\n",
        "\n",
        "# Identify target columns (resources to predict)\n",
        "# For now, let's assume columns with 'demand', 'usage', 'need', 'resource' in their name are targets\n",
        "target_cols = [col for col in df.columns if any(x in col.lower() for x in ['demand', 'usage', 'need', 'resource'])]\n",
        "\n",
        "if not target_cols:\n",
        "    print(\"\\nNo clear target columns identified. Please specify which columns represent resource demand.\")\n",
        "    # For demonstration, let's assume the last column might be the target\n",
        "    target_cols = [df.columns[-2]]\n",
        "\n",
        "print(f\"\\nIdentified potential target columns: {target_cols}\")\n",
        "\n",
        "# Let's visualize the time series of the first identified target\n",
        "plt.figure(figsize=(12, 6))\n",
        "for target in target_cols[:3]:  # Show up to 3 targets\n",
        "    plt.plot(df[date_col], df[target], label=target)\n",
        "plt.title('Medical Resource Demand Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Demand')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Feature Engineering and Preprocessing\n",
        "def preprocess_data(dataframe, target_column):\n",
        "    \"\"\"Preprocess the data for modeling\"\"\"\n",
        "    # Make a copy to avoid modifying the original dataframe\n",
        "    df_copy = dataframe.copy()\n",
        "\n",
        "    # Extract date features if date column exists\n",
        "    if 'date' in df_copy.columns:\n",
        "        df_copy['day_of_week'] = df_copy['date'].dt.dayofweek\n",
        "        df_copy['month'] = df_copy['date'].dt.month\n",
        "        df_copy['day'] = df_copy['date'].dt.day\n",
        "        df_copy['is_weekend'] = df_copy['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # Drop non-numeric columns except derived features\n",
        "    numeric_cols = df_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cols_to_keep = numeric_cols + ['day_of_week', 'month', 'day', 'is_weekend']\n",
        "    df_copy = df_copy[cols_to_keep]\n",
        "\n",
        "    # Remove the target from features\n",
        "    features = df_copy.drop(target_column, axis=1)\n",
        "    target = df_copy[target_column]\n",
        "\n",
        "    # Create lagged features (important for time series)\n",
        "    for lag in [1, 3, 7]:\n",
        "        if len(df_copy) > lag:\n",
        "            features[f'{target_column}_lag_{lag}'] = target.shift(lag)\n",
        "\n",
        "    # Drop rows with NaN due to lag creation\n",
        "    features = features.dropna()\n",
        "    target = target.iloc[features.index]\n",
        "\n",
        "    return features, target\n",
        "\n",
        "# 3. Advanced Model Building and Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"\\n==== {model_name} Performance ====\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R²: {r2:.4f}\")\n",
        "\n",
        "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "def prepare_lstm_data(X, y, time_steps=10):\n",
        "    \"\"\"Prepare data for LSTM model with time steps\"\"\"\n",
        "    X_lstm, y_lstm = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        X_lstm.append(X[i:i + time_steps])\n",
        "        y_lstm.append(y[i + time_steps])\n",
        "    return np.array(X_lstm), np.array(y_lstm)\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    \"\"\"Build an LSTM model for time series forecasting\"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def prepare_prophet_data(df, date_col, target_col):\n",
        "    \"\"\"Prepare data for Prophet model\"\"\"\n",
        "    # Prophet requires a specific dataframe format with 'ds' and 'y' columns\n",
        "    prophet_df = df[[date_col, target_col]].copy()\n",
        "    prophet_df.columns = ['ds', 'y']\n",
        "    return prophet_df\n",
        "\n",
        "def build_and_evaluate_models(features, target, df, date_col, target_col, time_steps=10):\n",
        "    \"\"\"Build and evaluate multiple advanced models\"\"\"\n",
        "    # Split data into training and testing sets - use time series split for better evaluation\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "    # For regular models\n",
        "    train_index, test_index = list(tscv.split(features))[-1]  # Use the last fold\n",
        "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
        "    y_train, y_test = target.iloc[train_index], target.iloc[test_index]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # For time series data normalization\n",
        "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "    y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "    # Prepare data for LSTM\n",
        "    X_train_lstm, y_train_lstm = prepare_lstm_data(X_train_scaled, y_train_scaled, time_steps)\n",
        "    X_test_lstm, y_test_lstm = prepare_lstm_data(X_test_scaled, y_test_scaled, time_steps)\n",
        "\n",
        "    # Prepare data for Prophet\n",
        "    prophet_df = prepare_prophet_data(df, date_col, target_col)\n",
        "    prophet_train = prophet_df.iloc[train_index]\n",
        "    prophet_test = prophet_df.iloc[test_index]\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "        'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.05, random_state=42, n_jobs=-1),\n",
        "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    best_model_name = None\n",
        "    best_model_obj = None\n",
        "    best_scaler = None\n",
        "    best_target_scaler = None\n",
        "    best_score = float('inf')  # Lower is better for RMSE\n",
        "\n",
        "    # Train and evaluate traditional models\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "        # Evaluate\n",
        "        eval_results = evaluate_model(y_test, y_pred, name)\n",
        "        results[name] = eval_results\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if eval_results['rmse'] < best_score:\n",
        "            best_score = eval_results['rmse']\n",
        "            best_model_name = name\n",
        "            best_model_obj = model\n",
        "            best_scaler = scaler\n",
        "            best_target_scaler = None  # Not used for traditional models\n",
        "\n",
        "    # Train and evaluate LSTM model\n",
        "    print(\"\\nTraining LSTM...\")\n",
        "    lstm_model = build_lstm_model((X_train_lstm.shape[1], X_train_lstm.shape[2]))\n",
        "\n",
        "    # Use early stopping to prevent overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Fit the LSTM model\n",
        "    lstm_history = lstm_model.fit(\n",
        "        X_train_lstm, y_train_lstm,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    lstm_pred_scaled = lstm_model.predict(X_test_lstm)\n",
        "    lstm_pred = target_scaler.inverse_transform(lstm_pred_scaled)\n",
        "\n",
        "    # Need to align predictions with test data (LSTM requires time steps)\n",
        "    y_test_lstm_actual = y_test.iloc[time_steps:]\n",
        "\n",
        "    # Evaluate LSTM\n",
        "    lstm_results = evaluate_model(y_test_lstm_actual, lstm_pred, 'LSTM')\n",
        "    results['LSTM'] = lstm_results\n",
        "\n",
        "    # Check if LSTM is the best model\n",
        "    if lstm_results['rmse'] < best_score:\n",
        "        best_score = lstm_results['rmse']\n",
        "        best_model_name = 'LSTM'\n",
        "        best_model_obj = lstm_model\n",
        "        best_scaler = scaler\n",
        "        best_target_scaler = target_scaler\n",
        "\n",
        "    # Train and evaluate Prophet model\n",
        "    print(\"\\nTraining Prophet...\")\n",
        "    prophet_model = Prophet(\n",
        "        interval_width=0.95,\n",
        "        yearly_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        daily_seasonality=True\n",
        "    )\n",
        "\n",
        "    # Add holiday effects if available\n",
        "    try:\n",
        "        prophet_model.add_country_holidays(country_name='US')\n",
        "    except:\n",
        "        print(\"Could not add US holidays to Prophet model.\")\n",
        "\n",
        "    # Fit Prophet model\n",
        "    prophet_model.fit(prophet_train)\n",
        "\n",
        "    # Create future dataframe for prediction\n",
        "    future = prophet_model.make_future_dataframe(periods=len(prophet_test))\n",
        "    forecast = prophet_model.predict(future)\n",
        "\n",
        "    # Extract predictions for test period\n",
        "    prophet_pred = forecast.iloc[-len(prophet_test):]['yhat'].values\n",
        "\n",
        "    # Evaluate Prophet\n",
        "    prophet_results = evaluate_model(prophet_test['y'].values, prophet_pred, 'Prophet')\n",
        "    results['Prophet'] = prophet_results\n",
        "\n",
        "    # Check if Prophet is the best model\n",
        "    if prophet_results['rmse'] < best_score:\n",
        "        best_score = prophet_results['rmse']\n",
        "        best_model_name = 'Prophet'\n",
        "        best_model_obj = prophet_model\n",
        "        best_scaler = None  # Not used for Prophet\n",
        "        best_target_scaler = None  # Not used for Prophet\n",
        "\n",
        "    print(f\"\\nBest model: {best_model_name} with RMSE: {best_score:.4f}\")\n",
        "\n",
        "    # Plot actual vs predicted for the best model\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    if best_model_name == 'LSTM':\n",
        "        plt.plot(y_test_lstm_actual.values, label='Actual')\n",
        "        plt.plot(lstm_pred, label='Predicted')\n",
        "    elif best_model_name == 'Prophet':\n",
        "        plt.plot(prophet_test['y'].values, label='Actual')\n",
        "        plt.plot(prophet_pred, label='Predicted')\n",
        "    else:\n",
        "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "\n",
        "    plt.xlabel('Actual' if best_model_name not in ['LSTM', 'Prophet'] else 'Time')\n",
        "    plt.ylabel('Value')\n",
        "    plt.title(f'Actual vs Predicted - {best_model_name}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance for tree-based models\n",
        "    if best_model_name in ['Random Forest', 'XGBoost', 'Gradient Boosting']:\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        feature_importance = best_model_obj.feature_importances_\n",
        "        sorted_idx = np.argsort(feature_importance)\n",
        "        plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
        "        plt.yticks(range(len(sorted_idx)), np.array(X_train.columns)[sorted_idx])\n",
        "        plt.title(f'Feature Importance - {best_model_name}')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Save the best model\n",
        "    if best_model_name not in ['LSTM', 'Prophet']:\n",
        "        joblib.dump(best_model_obj, f'best_model_{best_model_name}.pkl')\n",
        "        joblib.dump(best_scaler, f'scaler_{best_model_name}.pkl')\n",
        "        print(f\"\\nModel saved as 'best_model_{best_model_name}.pkl'\")\n",
        "    elif best_model_name == 'LSTM':\n",
        "        best_model_obj.save('best_model_LSTM.h5')\n",
        "        joblib.dump(best_scaler, 'scaler_LSTM.pkl')\n",
        "        joblib.dump(best_target_scaler, 'target_scaler_LSTM.pkl')\n",
        "        print(\"\\nModel saved as 'best_model_LSTM.h5'\")\n",
        "    elif best_model_name == 'Prophet':\n",
        "        with open('best_model_Prophet.pkl', 'wb') as f:\n",
        "            joblib.dump(prophet_model, f)\n",
        "        print(\"\\nModel saved as 'best_model_Prophet.pkl'\")\n",
        "\n",
        "    return best_model_obj, best_scaler, results, best_model_name, best_target_scaler\n",
        "\n",
        "# Apply preprocessing and model building for the first target\n",
        "selected_target = target_cols[0]\n",
        "print(f\"\\nBuilding model for target: {selected_target}\")\n",
        "features, target = preprocess_data(df, selected_target)\n",
        "best_model, scaler, model_results, best_model_name = build_and_evaluate_models(features, target)\n",
        "\n",
        "# 4. Advanced Real-time Prediction Framework\n",
        "class RealTimePredictor:\n",
        "    def __init__(self, model, scaler, feature_columns, target_column, model_type, historical_data=None, target_scaler=None, time_steps=10):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.target_scaler = target_scaler  # For LSTM models\n",
        "        self.feature_columns = feature_columns\n",
        "        self.target_column = target_column\n",
        "        self.model_type = model_type  # Type of model: 'Random Forest', 'XGBoost', 'Gradient Boosting', 'LSTM', 'Prophet'\n",
        "        self.time_steps = time_steps  # For LSTM models\n",
        "\n",
        "        # Store historical data for creating lag features\n",
        "        self.historical_data = historical_data.copy() if historical_data is not None else pd.DataFrame()\n",
        "\n",
        "        # Maintain a buffer for LSTM predictions\n",
        "        if self.model_type == 'LSTM':\n",
        "            self.prediction_buffer = np.zeros((1, self.time_steps, len(feature_columns)))\n",
        "            self.initialized_buffer = False\n",
        "\n",
        "        # Cache for API responses\n",
        "        self.api_cache = {}\n",
        "\n",
        "        # Track model performance metrics in real-time\n",
        "        self.performance_metrics = {'predictions': [], 'actuals': [], 'rmse': [], 'mae': [], 'timestamp': []}\n",
        "\n",
        "        # Mock data generator for demonstration\n",
        "        self.last_update = datetime.now()\n",
        "\n",
        "        # For Prophet predictions\n",
        "        if self.model_type == 'Prophet':\n",
        "            self.last_forecast = None\n",
        "            self.forecast_dates = None\n",
        "\n",
        "    def prepare_features(self, new_data):\n",
        "        \"\"\"Prepare features for prediction including lags\"\"\"\n",
        "        # Combine with historical to generate lag features\n",
        "        combined_data = pd.concat([self.historical_data, new_data]).reset_index(drop=True)\n",
        "\n",
        "        # Create date features\n",
        "        if 'date' in combined_data.columns:\n",
        "            combined_data['day_of_week'] = combined_data['date'].dt.dayofweek\n",
        "            combined_data['month'] = combined_data['date'].dt.month\n",
        "            combined_data['day'] = combined_data['date'].dt.day\n",
        "            combined_data['is_weekend'] = combined_data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "            combined_data['hour'] = combined_data['date'].dt.hour\n",
        "            combined_data['quarter'] = combined_data['date'].dt.quarter\n",
        "\n",
        "            # Add seasonality features\n",
        "            combined_data['sin_day'] = np.sin(2 * np.pi * combined_data['day'] / 31)\n",
        "            combined_data['cos_day'] = np.cos(2 * np.pi * combined_data['day'] / 31)\n",
        "            combined_data['sin_month'] = np.sin(2 * np.pi * combined_data['month'] / 12)\n",
        "            combined_data['cos_month'] = np.cos(2 * np.pi * combined_data['month'] / 12)\n",
        "            combined_data['sin_hour'] = np.sin(2 * np.pi * combined_data['hour'] / 24)\n",
        "            combined_data['cos_hour'] = np.cos(2 * np.pi * combined_data['hour'] / 24)\n",
        "\n",
        "        # Create lag features\n",
        "        for lag in [1, 3, 7, 14, 30]:  # Extended lag features\n",
        "            if len(combined_data) > lag:\n",
        "                combined_data[f'{self.target_column}_lag_{lag}'] = combined_data[self.target_column].shift(lag)\n",
        "\n",
        "        # Add rolling statistics\n",
        "        for window in [7, 14, 30]:\n",
        "            if len(combined_data) > window:\n",
        "                combined_data[f'{self.target_column}_rolling_mean_{window}'] = combined_data[self.target_column].rolling(window=window).mean()\n",
        "                combined_data[f'{self.target_column}_rolling_std_{window}'] = combined_data[self.target_column].rolling(window=window).std()\n",
        "                combined_data[f'{self.target_column}_rolling_min_{window}'] = combined_data[self.target_column].rolling(window=window).min()\n",
        "                combined_data[f'{self.target_column}_rolling_max_{window}'] = combined_data[self.target_column].rolling(window=window).max()\n",
        "\n",
        "        # Get the latest data\n",
        "        if self.model_type == 'LSTM':\n",
        "            # For LSTM, we need the last time_steps rows\n",
        "            latest_data = combined_data.iloc[-self.time_steps:].dropna(axis=1, how='any')\n",
        "\n",
        "            # Select only the feature columns the model was trained on\n",
        "            available_features = [col for col in self.feature_columns if col in latest_data.columns]\n",
        "            X = latest_data[available_features]\n",
        "\n",
        "            # Scale features\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "            # Reshape for LSTM [samples, time_steps, features]\n",
        "            X_lstm = np.reshape(X_scaled, (1, X_scaled.shape[0], X_scaled.shape[1]))\n",
        "\n",
        "            # Update the prediction buffer\n",
        "            self.prediction_buffer = X_lstm\n",
        "            self.initialized_buffer = True\n",
        "\n",
        "            return X_lstm\n",
        "        elif self.model_type == 'Prophet':\n",
        "            # For Prophet, prepare a future dataframe\n",
        "            latest_date = combined_data['date'].max()\n",
        "            future = pd.DataFrame({'ds': [latest_date]})\n",
        "            return future\n",
        "        else:\n",
        "            # For traditional models\n",
        "            latest_data = combined_data.iloc[-1:].dropna(axis=1, how='any')\n",
        "\n",
        "            # Select only the feature columns the model was trained on\n",
        "            available_features = [col for col in self.feature_columns if col in latest_data.columns]\n",
        "            X = latest_data[available_features]\n",
        "\n",
        "            # Scale features\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "            return X_scaled\n",
        "\n",
        "    def fetch_real_time_data(self, source=\"mock\"):\n",
        "        \"\"\"Fetch real-time data from specified source\"\"\"\n",
        "        current_time = datetime.now()\n",
        "\n",
        "        if source == \"mock\":\n",
        "            # Generate mock data for demonstration\n",
        "            # In a real scenario, you would replace this with API calls\n",
        "\n",
        "            # Only generate new data if enough time has passed\n",
        "            if (current_time - self.last_update).seconds < 5:\n",
        "                return None\n",
        "\n",
        "            self.last_update = current_time\n",
        "\n",
        "            # Create synthetic data with some randomness and seasonality patterns\n",
        "            hour_of_day = current_time.hour\n",
        "            day_of_week = current_time.weekday()\n",
        "            month = current_time.month\n",
        "\n",
        "            # Base value with seasonal patterns\n",
        "            base_value = self.historical_data[self.target_column].iloc[-1]\n",
        "\n",
        "            # Add time-based patterns\n",
        "            # Higher demand during business hours\n",
        "            hour_factor = 1.2 if 8 <= hour_of_day <= 17 else 0.8\n",
        "            # Lower demand on weekends\n",
        "            day_factor = 0.7 if day_of_week >= 5 else 1.1\n",
        "            # Seasonal variations\n",
        "            month_factor = 1.1 if month in [1, 2, 12] else (1.2 if month in [6, 7, 8] else 1.0)\n",
        "\n",
        "            # Add trend component (slight upward trend)\n",
        "            trend_factor = 1.001\n",
        "\n",
        "            # Add random noise\n",
        "            noise = np.random.normal(0, 0.03)\n",
        "\n",
        "            # Calculate new value\n",
        "            new_value = base_value * hour_factor * day_factor * month_factor * trend_factor * (1 + noise)\n",
        "\n",
        "            new_row = {\n",
        "                'date': pd.Timestamp(current_time),\n",
        "                self.target_column: new_value\n",
        "            }\n",
        "\n",
        "            # Add other features that might be needed\n",
        "            for col in self.historical_data.columns:\n",
        "                if col not in new_row and col != 'date' and col != self.target_column:\n",
        "                    if col.endswith(('_lag_', '_rolling_')):\n",
        "                        continue  # Skip lag and rolling columns as they'll be computed\n",
        "                    new_row[col] = self.historical_data[col].iloc[-1] * (1 + np.random.normal(0, 0.02))\n",
        "\n",
        "            new\n",
        "\n",
        "# 5. Create Interactive Dashboard\n",
        "def create_dashboard(predictor, target_column):\n",
        "    \"\"\"Create an interactive dashboard for visualizing predictions\"\"\"\n",
        "    # Create a figure for the dashboard\n",
        "    fig = make_subplots(\n",
        "        rows=2, cols=2,\n",
        "        specs=[[{\"colspan\": 2}, None],\n",
        "               [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]],\n",
        "        subplot_titles=(\"Historical and Predicted Demand\",\n",
        "                         \"Current Demand\", \"Predicted Demand\")\n",
        "    )\n",
        "\n",
        "    # Get historical data for plotting\n",
        "    historical_data = predictor.historical_data.copy()\n",
        "\n",
        "    # Initial plot of historical data\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=historical_data['date'],\n",
        "            y=historical_data[target_column],\n",
        "            mode='lines',\n",
        "            name='Historical Demand',\n",
        "            line=dict(color='blue')\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Add trace for predictions (initially empty)\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[],\n",
        "            y=[],\n",
        "            mode='lines+markers',\n",
        "            name='Predicted Demand',\n",
        "            line=dict(color='red')\n",
        "        ),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    # Add indicator for current demand\n",
        "    fig.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"number+delta\",\n",
        "            value=historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "            title={\"text\": \"Current Demand\"},\n",
        "            delta={'reference': historical_data[target_column].iloc[-2] if len(historical_data) > 1 else 0,\n",
        "                   'valueformat': '.2f'},\n",
        "            number={'valueformat': '.2f'}\n",
        "        ),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "    # Add indicator for predicted demand (initially same as current)\n",
        "    fig.add_trace(\n",
        "        go.Indicator(\n",
        "            mode=\"number+delta\",\n",
        "            value=historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "            title={\"text\": \"Predicted Demand\"},\n",
        "            delta={'reference': historical_data[target_column].iloc[-1] if not historical_data.empty else 0,\n",
        "                   'valueformat': '.2f'},\n",
        "            number={'valueformat': '.2f'}\n",
        "        ),\n",
        "        row=2, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    fig.update_layout(\n",
        "        height=600,\n",
        "        title_text=f\"Real-Time Medical Resource Demand Dashboard: {target_column}\",\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Create function to update dashboard\n",
        "    prediction_history = []\n",
        "    timestamp_history = []\n",
        "\n",
        "    def update_dashboard():\n",
        "        nonlocal prediction_history, timestamp_history\n",
        "\n",
        "        # Make prediction\n",
        "        prediction, new_data = predictor.predict_demand()\n",
        "\n",
        "        if prediction is not None and new_data is not None:\n",
        "            # Store prediction and timestamp\n",
        "            prediction_history.append(prediction)\n",
        "            timestamp_history.append(new_data['date'].iloc[0])\n",
        "\n",
        "            # Keep only the last 50 predictions\n",
        "            if len(prediction_history) > 50:\n",
        "                prediction_history = prediction_history[-50:]\n",
        "                timestamp_history = timestamp_history[-50:]\n",
        "\n",
        "            # Get the most recent historical data\n",
        "            historical_data = predictor.historical_data.copy()\n",
        "\n",
        "            # Update historical demand line\n",
        "            fig.data[0].x = historical_data['date']\n",
        "            fig.data[0].y = historical_data[target_column]\n",
        "\n",
        "            # Update prediction line\n",
        "            fig.data[1].x = timestamp_history\n",
        "            fig.data[1].y = prediction_history\n",
        "\n",
        "            # Update current demand indicator\n",
        "            current_value = historical_data[target_column].iloc[-1]\n",
        "            reference_value = historical_data[target_column].iloc[-2] if len(historical_data) > 1 else current_value\n",
        "            fig.data[2].value = current_value\n",
        "            fig.data[2].delta.reference = reference_value\n",
        "\n",
        "            # Update predicted demand indicator\n",
        "            fig.data[3].value = prediction\n",
        "            fig.data[3].delta.reference = current_value\n",
        "\n",
        "            # Update layout with current time\n",
        "            fig.update_layout(\n",
        "                title_text=f\"Real-Time Medical Resource Demand Dashboard: {target_column}<br>Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "            )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    return fig, update_dashboard\n",
        "\n",
        "# 6. Run Real-Time Prediction System\n",
        "def run_real_time_system(model, scaler, df, target_column):\n",
        "    \"\"\"Run the real-time prediction system\"\"\"\n",
        "    # Initialize predictor with historical data\n",
        "    feature_columns = [col for col in df.columns if col != target_column and col != 'date']\n",
        "    predictor = RealTimePredictor(model, scaler, feature_columns, target_column, historical_data=df)\n",
        "\n",
        "    # Set up dashboard\n",
        "    fig, update_dashboard = create_dashboard(predictor, target_column)\n",
        "\n",
        "    # Create buttons for dashboard control\n",
        "    start_button = widgets.Button(description='Start Real-Time Prediction')\n",
        "    stop_button = widgets.Button(description='Stop')\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Define button behaviors\n",
        "    running = False\n",
        "    def start_clicked(b):\n",
        "        nonlocal running\n",
        "        running = True\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Starting real-time prediction for {target_column}...\")\n",
        "            display(fig)\n",
        "\n",
        "            while running:\n",
        "                fig = update_dashboard()\n",
        "                clear_output(wait=True)\n",
        "                print(f\"Real-time prediction for {target_column} (Press 'Stop' to end)\")\n",
        "                print(f\"Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                display(fig)\n",
        "                time.sleep(2)  # Update every 2 seconds\n",
        "\n",
        "    def stop_clicked(b):\n",
        "        nonlocal running\n",
        "        running = False\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Real-time prediction stopped.\")\n",
        "\n",
        "    start_button.on_click(start_clicked)\n",
        "    stop_button.on_click(stop_clicked)\n",
        "\n",
        "    # Display buttons and output\n",
        "    display(widgets.HBox([start_button, stop_button]))\n",
        "    display(output)\n",
        "\n",
        "# Execute the main workflow\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n==== Starting Medical Resource Demand Prediction System ====\")\n",
        "\n",
        "    # Let the user select which target to predict\n",
        "    if len(target_cols) > 1:\n",
        "        target_selector = widgets.Dropdown(\n",
        "            options=target_cols,\n",
        "            value=target_cols[0],\n",
        "            description='Select Resource:',\n",
        "        )\n",
        "\n",
        "        def on_target_change(change):\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Selected target: {change.new}\")\n",
        "            # Re-run the model building for the new target\n",
        "            features, target = preprocess_data(df, change.new)\n",
        "            best_model, scaler, model_results, best_model_name = build_and_evaluate_models(features, target)\n",
        "            # Start the real-time system\n",
        "            run_real_time_system(best_model, scaler, df, change.new)\n",
        "\n",
        "        target_selector.observe(on_target_change, names='value')\n",
        "        display(target_selector)\n",
        "    else:\n",
        "        # If only one target, start immediately\n",
        "        run_real_time_system(best_model, scaler, df, selected_target)"
      ],
      "metadata": {
        "id": "w542spjzK9aa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}